[{"authors":["hajime-nagahara"],"categories":null,"content":"Contact  email: nagahara@ids. webpage: https://www.ids.osaka-u.ac.jp/nagahara/  Please add osaka-u.ac.jp to complete email address\n","date":1593565986,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1593565986,"objectID":"26a8a9f7a8274796c0674bb5a221f080","permalink":"/author/hajime-nagahara/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/hajime-nagahara/","section":"authors","summary":"Contact  email: nagahara@ids. webpage: https://www.ids.osaka-u.ac.jp/nagahara/  Please add osaka-u.ac.jp to complete email address","tags":null,"title":"Hajime Nagahara","type":"authors"},{"authors":["kenya-uomori"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"d276cad43ff16e4f21f923f805314c86","permalink":"/author/kenya-uomori/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/kenya-uomori/","section":"authors","summary":"","tags":null,"title":"Kenya Uomori","type":"authors"},{"authors":["yoshio-kitaoka"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"23aee09cc782c6b93ad84ff6da5a6363","permalink":"/author/yoshio-kitaoka/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/yoshio-kitaoka/","section":"authors","summary":"","tags":null,"title":"Yoshio Kitaoka","type":"authors"},{"authors":["kaname-harumoto"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"0fc84699e13ed58a932dbd6d1a58790b","permalink":"/author/kaname-harumoto/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/kaname-harumoto/","section":"authors","summary":"","tags":null,"title":"Kaname Harumoto","type":"authors"},{"authors":["atsuo-kishimoto"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"945bdbb019a6f812c7e7107d2250cfd7","permalink":"/author/atsuo-kishimoto/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/atsuo-kishimoto/","section":"authors","summary":"","tags":null,"title":"Atsuo Kishimoto","type":"authors"},{"authors":["chenhui-chu"],"categories":null,"content":"Contact  email: chu@ids. webpage: https://researchmap.jp/chu/  Please add osaka-u.ac.jp to complete email address\n","date":1593566008,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1593566008,"objectID":"6dfe8658a821152c04a5fdb8665f8822","permalink":"/author/chenhui-chu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/chenhui-chu/","section":"authors","summary":"Contact  email: chu@ids. webpage: https://researchmap.jp/chu/  Please add osaka-u.ac.jp to complete email address","tags":null,"title":"Chenhui Chu","type":"authors"},{"authors":["yuta-nakashima"],"categories":null,"content":"Contact  email: n-yuta@ids. webpage: https://www.n-yuta.jp/  Please add osaka-u.ac.jp to complete email address\n","date":1593566008,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1593566008,"objectID":"8e71f981ba25e13262028fc4b5035638","permalink":"/author/yuta-nakashima/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/yuta-nakashima/","section":"authors","summary":"Contact  email: n-yuta@ids. webpage: https://www.n-yuta.jp/  Please add osaka-u.ac.jp to complete email address","tags":null,"title":"Yuta Nakashima","type":"authors"},{"authors":["noriko-takemura"],"categories":null,"content":"Contact  email: takemura@ids. webpage: https://sites.google.com/site/norikotakemurashomepage/  Please add osaka-u.ac.jp to complete email address\n","date":1593565986,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1593565986,"objectID":"3b47240a64a671541bde59c59d40f449","permalink":"/author/noriko-takemura/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/noriko-takemura/","section":"authors","summary":"Contact  email: takemura@ids. webpage: https://sites.google.com/site/norikotakemurashomepage/  Please add osaka-u.ac.jp to complete email address","tags":null,"title":"Noriko Takemura","type":"authors"},{"authors":["benjamin-renoust"],"categories":null,"content":"Contact  email: renoust@ids. webpage: http://renoust.com  Please add osaka-u.ac.jp to complete email address\n","date":1592402552,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1592402552,"objectID":"96e4b16c6e5ac7272a6c6799e5313d0d","permalink":"/author/benjamin-renoust/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/benjamin-renoust/","section":"authors","summary":"Contact  email: renoust@ids. webpage: http://renoust.com  Please add osaka-u.ac.jp to complete email address","tags":null,"title":"Benjamin Renoust","type":"authors"},{"authors":["tetsuya-fukushima"],"categories":null,"content":"Contact  email: niioka@ids. webpage: https://researchmap.jp/Hirohiko_Niioka/  Please add osaka-u.ac.jp to complete email address\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"a045a45b224b7275cc7d5ed113ca788d","permalink":"/author/tetsuya-fukushima/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/tetsuya-fukushima/","section":"authors","summary":"Contact  email: niioka@ids. webpage: https://researchmap.jp/Hirohiko_Niioka/  Please add osaka-u.ac.jp to complete email address","tags":null,"title":"Tetsuya Fukushima","type":"authors"},{"authors":["zekun-yang"],"categories":null,"content":"Contact  email: yang.zekun@lab.ime.cmc.  Please add osaka-u.ac.jp to complete email address\n","date":1593566008,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1593566008,"objectID":"8515554d3928ddb8cd923a8e336a69b2","permalink":"/author/zekun-yang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/zekun-yang/","section":"authors","summary":"Contact  email: yang.zekun@lab.ime.cmc.  Please add osaka-u.ac.jp to complete email address","tags":null,"title":"Zekun Yang","type":"authors"},{"authors":["hirohiko-niioka"],"categories":null,"content":"Contact  email: niioka@ids. webpage: https://researchmap.jp/Hirohiko_Niioka/  Please add osaka-u.ac.jp to complete email address\n","date":1585699200,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1585699200,"objectID":"01f8e3e309bbaa90a7a7c061d31bedc3","permalink":"/author/hirohiko-niioka/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/hirohiko-niioka/","section":"authors","summary":"Contact  email: niioka@ids. webpage: https://researchmap.jp/Hirohiko_Niioka/  Please add osaka-u.ac.jp to complete email address","tags":null,"title":"Hirohiko Niioka","type":"authors"},{"authors":["michitaka-yoshida"],"categories":null,"content":"Contact  email: yoshida@am.sanken. webpage: https://michitakayoshida.github.io/  Please add osaka-u.ac.jp to complete email address\n","date":1514764800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1514764800,"objectID":"770c333de126ce2dbe7f1a7f9fffb4a5","permalink":"/author/michitaka-yoshida/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/michitaka-yoshida/","section":"authors","summary":"Contact  email: yoshida@am.sanken. webpage: https://michitakayoshida.github.io/  Please add osaka-u.ac.jp to complete email address","tags":null,"title":"Michitaka Yoshida","type":"authors"},{"authors":["hiromi-takahata"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"a16e50690da265549b2c7f7f19d98939","permalink":"/author/hiromi-takahata/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/hiromi-takahata/","section":"authors","summary":"","tags":null,"title":"Hiromi Takahata","type":"authors"},{"authors":["natsuko-yamamoto"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"deaf64e7cbe2ed239e65269b9dbaa570","permalink":"/author/natsuko-yamamoto/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/natsuko-yamamoto/","section":"authors","summary":"","tags":null,"title":"Natsuko Yamamoto","type":"authors"},{"authors":["tadashi-nakano"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"b2ecea59580792eaa952edb85a59f15c","permalink":"/author/tadashi-nakano/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/tadashi-nakano/","section":"authors","summary":"","tags":null,"title":"Tadashi Nakano","type":"authors"},{"authors":["tomoyuki-kajiwara"],"categories":null,"content":"Contact  email: kajiwara@ids. webpage: https://sites.google.com/site/moguranosenshi/  Please add osaka-u.ac.jp to complete email address\n","date":1593565986,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1593565986,"objectID":"3bbbdd0e32714bb8242dd84c11f36db8","permalink":"/author/tomoyuki-kajiwara/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/tomoyuki-kajiwara/","section":"authors","summary":"Contact  email: kajiwara@ids. webpage: https://sites.google.com/site/moguranosenshi/  Please add osaka-u.ac.jp to complete email address","tags":null,"title":"Tomoyuki Kajiwara","type":"authors"},{"authors":["bowen-wang"],"categories":null,"content":"Contact  email: bowen.wang@lab.ime.cmc.  Please add osaka-u.ac.jp to complete email address\n","date":1593565512,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1593565512,"objectID":"60654d8602382c75bf60987e094b1f0e","permalink":"/author/bowen-wang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/bowen-wang/","section":"authors","summary":"Contact  email: bowen.wang@lab.ime.cmc.  Please add osaka-u.ac.jp to complete email address","tags":null,"title":"Bowen Wang","type":"authors"},{"authors":["noriko-ohashi"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"974f36df0a3d3e0e857922b0cca65597","permalink":"/author/noriko-ohashi/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/noriko-ohashi/","section":"authors","summary":"","tags":null,"title":"Noriko Ohashi","type":"authors"},{"authors":["trung-thanh-ngo"],"categories":null,"content":"Contact  email: trung@am.sanken. webpage: http://www.am.sanken.osaka-u.ac.jp/~trung/  Please add osaka-u.ac.jp to complete email address\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"17703c44f4445e63cb49467d897cb781","permalink":"/author/trung-thanh-ngo/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/trung-thanh-ngo/","section":"authors","summary":"Contact  email: trung@am.sanken. webpage: http://www.am.sanken.osaka-u.ac.jp/~trung/  Please add osaka-u.ac.jp to complete email address","tags":null,"title":"Trung Thanh Ngo","type":"authors"},{"authors":["manisha-verma"],"categories":null,"content":"Contact  email: mverma@ids. webpage: https://sites.google.com/site/manishaverma89/  Please add osaka-u.ac.jp to complete email address\n","date":1593565635,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1593565635,"objectID":"624a3dddf6bc1c56e92f53c4ae3786f2","permalink":"/author/manisha-verma/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/manisha-verma/","section":"authors","summary":"Contact  email: mverma@ids. webpage: https://sites.google.com/site/manishaverma89/  Please add osaka-u.ac.jp to complete email address","tags":null,"title":"Manisha Verma","type":"authors"},{"authors":["haruya-sakashita"],"categories":null,"content":"Contact  email: sakashita.haruya@ist.  Please add osaka-u.ac.jp to complete email address\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"f539c77631773f15dcb9aa658f19cb82","permalink":"/author/haruya-sakashita/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/haruya-sakashita/","section":"authors","summary":"Contact  email: sakashita.haruya@ist.  Please add osaka-u.ac.jp to complete email address","tags":null,"title":"Haruya Sakashita","type":"authors"},{"authors":["kosuke-fukui"],"categories":null,"content":"Contact  email: fukui@am.sanken.  Please add osaka-u.ac.jp to complete email address\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"689a0537caf6fb1fa062e9413599b5cd","permalink":"/author/kosuke-fukui/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/kosuke-fukui/","section":"authors","summary":"Contact  email: fukui@am.sanken.  Please add osaka-u.ac.jp to complete email address","tags":null,"title":"Kosuke Fukui","type":"authors"},{"authors":["mawataka-niwa"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"cabf857d0c8700cf522e44876ddf7b54","permalink":"/author/masataka-niwa/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/masataka-niwa/","section":"authors","summary":"","tags":null,"title":"Masataka Niwa","type":"authors"},{"authors":["devanshi-sakhija"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"5870ecd2e1f4d22982ec15c982846038","permalink":"/author/devanshi-sakhija/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/devanshi-sakhija/","section":"authors","summary":"","tags":null,"title":"Devanshi Sakhija","type":"authors"},{"authors":["jules-samaran"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"1b2e5f24fb6f6800acf049c2108cc670","permalink":"/author/jules-samaran/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/jules-samaran/","section":"authors","summary":"","tags":null,"title":"Jules Samaran","type":"authors"},{"authors":["rika-umeyama"],"categories":null,"content":"Contact  email: umeyama@am.sanken.  Please add osaka-u.ac.jp to complete email address\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"ea209578497f229ea934f87be0d83e39","permalink":"/author/rika-umeyama/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/rika-umeyama/","section":"authors","summary":"Contact  email: umeyama@am.sanken.  Please add osaka-u.ac.jp to complete email address","tags":null,"title":"Rika Umeyama","type":"authors"},{"authors":["noa-garcia"],"categories":null,"content":"Contact  email: noagarcia@ids. webpage: http://noagarciad.com/index.html  Please add osaka-u.ac.jp to complete email address\n","date":1593566008,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1593566008,"objectID":"a90bc29d0d60216b4801bfaf08e55792","permalink":"/author/noa-garcia/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/noa-garcia/","section":"authors","summary":"Contact  email: noagarcia@ids. webpage: http://noagarciad.com/index.html  Please add osaka-u.ac.jp to complete email address","tags":null,"title":"Noa Garcia","type":"authors"},{"authors":["liangzhi-li"],"categories":null,"content":"Contact  email: li@ids. webpage: https://www.liangzhili.com  Please add osaka-u.ac.jp to complete email address\n","date":1593565512,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1593565512,"objectID":"d816164de845acbbe4d19dca5c089443","permalink":"/author/liangzhi-li/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/liangzhi-li/","section":"authors","summary":"Contact  email: li@ids. webpage: https://www.liangzhili.com  Please add osaka-u.ac.jp to complete email address","tags":null,"title":"Liangzhi Li","type":"authors"},{"authors":["admin"],"categories":null,"content":"","date":1591714800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1591714800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/author/web-administrator/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/web-administrator/","section":"authors","summary":"","tags":null,"title":"Web Administrator","type":"authors"},{"authors":["Noa Garcia","Zekun Yang","Chenhui Chu","Yuta Nakashima"],"categories":[],"content":"Visual question answering (VQA) with knowledge is a task that requires knowledge to answer questions on images/video. This additional requirement of knowledge poses an interesting challenge on top of the classic VQA tasks. Specifically, a system needs to explore external knowledge sources to answer the questions correctly, as well as understanding the visual content.\nWe created a dedicated dataset for our knowledge VQA task and made it open to the public so that everyone can enjoy our new task. We have also published several papers on this task.\nPublications  Noa Garcia, Mayu Otani, Chenhui Chu, and Yuta Nakashima (2019). KnowIT VQA: Answering knowledge-based questions about videos. Proc. AAAI Conference on Artificial Intelligence, Feb. 2020. Zekun Yang, Noa Garcia, Chenhui Chu, Mayu Otani, Yuta Nakashima, and Haruo Takemura (2020). BERT representations for video question answering. Proc. IEEE Winter Conference on Applications of Computer Vision. Noa Garcia, Chenhui Chu, Mayu Otani, and Yuta Nakashima (2019). Video meets knowledge in visual question answering. MIRU. Zekun Yang, Noa Garcia, Chenhui Chu, Mayu Otani, Yuta Nakashima, and Haruo Takemura (2019). Video question answering with BERT. MIRU.  ","date":1593566008,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593566008,"objectID":"35c2cf0200b210ae26c8e2fde9635b1d","permalink":"/project/kiban_b-kvqa/","publishdate":"2020-07-01T10:13:28+09:00","relpermalink":"/project/kiban_b-kvqa/","section":"project","summary":"Visual question answering (VQA) with knowledge is a task that requires knowledge to answer questions on images/video. This additional requirement of knowledge poses an interesting challenge on top of the classic VQA tasks.","tags":["kvqa"],"title":"Knowledge VQA","type":"project"},{"authors":["Chenhui Chu","Tomoyuki Kajiwara","Yuta Nakashima","Noriko Takemura","Hajime Nagahara"],"categories":[],"content":"In collaboration with Prof. Fujikawa at Graduate School of Letters, Osaka University, we are working on exploring Australian history through public meetings, of which call for participation appears in newspapers from back then.\nWe explore ways to analyze such newspapers with state-of-the-art technologies in NLP to make OCR output better and to automatically detect/structure call for participation.\n","date":1593565986,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593565986,"objectID":"83e967c7f6d05cd746814f014627dbc8","permalink":"/project/australian-history/","publishdate":"2020-07-01T10:13:06+09:00","relpermalink":"/project/australian-history/","section":"project","summary":"In collaboration with Prof. Fujikawa at Graduate School of Letters, Osaka University, we are working on exploring Australian history through public meetings, of which call for participation appears in newspapers from back then.","tags":[],"title":"Australian History in Newspaper and AI","type":"project"},{"authors":["Hajime Nagahara"],"categories":[],"content":"Coming soon.\n","date":1593565957,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593565957,"objectID":"642be3359efdcc518ee8d20f9965aa01","permalink":"/project/kiban_s-plenoptic/","publishdate":"2020-07-01T10:12:37+09:00","relpermalink":"/project/kiban_s-plenoptic/","section":"project","summary":"Coming soon.","tags":[],"title":"Kiban_s Plenoptic","type":"project"},{"authors":["Hajime Nagahara"],"categories":[],"content":"Coming soon.\n","date":1593565728,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593565728,"objectID":"6ae9418c84de6a44587227213e7ea1cb","permalink":"/project/crest-3d-cancer/","publishdate":"2020-07-01T10:08:48+09:00","relpermalink":"/project/crest-3d-cancer/","section":"project","summary":"Coming soon.","tags":[],"title":"Crest 3d Cancer","type":"project"},{"authors":["Manisha Verma","Tomoyuki Kajiwara","Noriko Takemura","Yuta Nakashima","Hajime Nagahara"],"categories":[],"content":"Institute for Datability Science, Osaka University is now working on Society 5.0 using information science and technology.\n In the world of Society 5.0, innovations in IoT, Big Data, robotics, and AI will be part of everyday life, helping people lead active and high-quality lives, creating a super-smart society. This project encourages collaboration across projects and university organizations, thus promoting faster adoption of research results in real-world society.\n Under this big project, we are working two sub-projects:\nSocial sensing for Society 5.0 Sensing technologies that aggregate various types of information from publicly and ubiquitously available social probes including social networking services are essential for providing various services in the world of Society 5.0. We are working towards establishing a social sensing technology that can infer emotional states of people for timely services.\nFuture school technology for Society 5.0 Education everywhere is one of SDGs, and e-learning is one solution toward this. We collectively working on a broad range of technologies related to e-learning.\n","date":1593565635,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593565635,"objectID":"17b776a1a3de9846e5051a4d47e01406","permalink":"/project/society5_0/","publishdate":"2020-07-01T10:07:15+09:00","relpermalink":"/project/society5_0/","section":"project","summary":"Institute for Datability Science, Osaka University is now working on Society 5.0 using information science and technology.\n In the world of Society 5.0, innovations in IoT, Big Data, robotics, and AI will be part of everyday life, helping people lead active and high-quality lives, creating a super-smart society.","tags":["society5.0"],"title":"Society 5.0 Projects","type":"project"},{"authors":["Hajime Nagahara"],"categories":[],"content":"There have been ever-increasing demands for esthetics in the oral cavity after tooth restorations. Especially in the anterior, functional restoration is not enough; restored teeth should have similar color tones and light transmissions to natural teeth because the appearance of restored teeth when exposed to light varies greatly depending on the material used for the crown restoration device and the abutment structure.\nTherefore, by analyzing the optical properties of various dental restoration materials and dental tissues via optical simulation, the behavior of light in natural teeth and dental restorations is visualized and analyzed for esthetic tooth restorations.\n  ","date":1593565618,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593565618,"objectID":"4da61ca738ce05bc7470ff6a879aec12","permalink":"/project/esthetic-dentistry/","publishdate":"2020-07-01T10:06:58+09:00","relpermalink":"/project/esthetic-dentistry/","section":"project","summary":"By analyzing the optical properties of various dental restoration materials and dental tissues via optical simulation, the behavior of light in natural teeth and dental restorations is visualized and analyzed for esthetic tooth restorations.","tags":[],"title":"Esthetic Dentistry and Optical Analysis","type":"project"},{"authors":["Hajime Nagahara"],"categories":[],"content":"Coming soon\n","date":1593565571,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593565571,"objectID":"e4379780660e54035b5dc6093de3100f","permalink":"/project/brain-pharmaceutics/","publishdate":"2020-07-01T10:06:11+09:00","relpermalink":"/project/brain-pharmaceutics/","section":"project","summary":"Coming soon","tags":[],"title":"Brain Pharmaceutics","type":"project"},{"authors":["Liangzhi Li","Bowen Wang","Manisha Verma","Yuta Nakashima","Hajime Nagahara"],"categories":[],"content":"Osaka University Medical Hospital has launched Artificical Intelligence Center for Medical Research and Application (AIM), which supports physicians, nurses, and all the medical staff collaborating with medical information specialists and data scientists to boost the medical application of AI in daily practices of the hospital.\nWe are collaborating with AIM to provide cutting-edge technologies.\nOphthalmology and AI In ophthalmology or any other departments, vessels in retinal fundus images provide rich information on the cardiovascular system of human bodies. We proposed a state-of-the-art method, coined IterNet for extracting vessels from retinal fundus images as in the (e) in the figure below.\n  On top of this technology, we also invented a new method for classifying vessels into artery/vein, which takes a two-step approach: We firstly segment vessels in input images using an IterNet-based method and then classify them into artery/vein with some post-processing.\n  ","date":1593565512,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593565512,"objectID":"699a8acba39e6eefcaaff8aa767d5cd0","permalink":"/project/ai-hospital/","publishdate":"2020-07-01T10:05:12+09:00","relpermalink":"/project/ai-hospital/","section":"project","summary":"Osaka University Medical Hospital has launched Artificical Intelligence Center for Medical Research and Application (AIM), which supports physicians, nurses, and all the medical staff collaborating with medical information specialists and data scientists to boost the medical application of AI in daily practices of the hospital.","tags":[],"title":"AI Hospital","type":"project"},{"authors":["Benjamin Renoust","Chenhui Chu","Yuta Nakashima","Noriko Takemura","Hajime Nagahara"],"categories":[],"content":"In collaboration with Prof. Noriko Okubo at Graduate School of Law and Politics, Osaka University, we are studying to automatically evaluate how green laws are enforced in different countries.\nGreen laws\u0026rsquo; participation principle consists of 1) the information access right, 2) participation in the policy decision process, 3) the judicial access; however, actual implementation varies country to country, and legal methodologies have been explored for evaluating their effectiveness. This work investigates legal evaluation criteria on the green laws\u0026rsquo; participation principle, analyzes Japanese participation system\u0026rsquo;s pros and cons in a comparative perspective, and propose some recommendations to establish the environmental democracy.\nThe difficulty lies in how to automatically find out related legislations, cases, statutes, etc. in different languages. As the first attempt, we proposed a method for identifying the topic of such legal documents through analyzing citation networks in addition to classic topic modeling. The figure below shows citation networks among different types of legal documents (e.g., cases-prior cases).\n  ","date":1592402552,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1592402552,"objectID":"0c600c4037c2f79b81042424f8d963ab","permalink":"/project/green_law/","publishdate":"2020-06-17T23:02:32+09:00","relpermalink":"/project/green_law/","section":"project","summary":"This work investigates legal evaluation criteria on the green laws' participation principle, analyzes Japanese participation system's pros and cons in a comparative perspective, and propose some recommendations to establish the environmental democracy.","tags":[],"title":"Law and AI","type":"project"},{"authors":["Benjamin Renoust","Noa Garcia","Yuta Nakashima","Noriko Takemura","Hajime Nagahara"],"categories":[],"content":"In collaboration with Prof. Fujioka with Graduate School of Letters/School of Letters, Osaka University, we are attempting to create an AI for analyzing various aspects of Buddha faces in images.\nFocusing on the face of the Buddha image, i.e., \u0026ldquo;Buddha face\u0026rdquo;, we analyze the characteristics of the style of each region, era, and author using statistical and machine learning approaches based on images and 3D geometric data, building a genealogy of Buddha faces. This is to realize style judgment based on the knowledge obtained from data, not based on the experience of art historians, which promotes the globalization of the Buddha statue research and also helps to identify the genealogy of Buddha faces propagated through the Silk Road, giving a new perspective on the spread of culture in Asia.\nWe have built several interfaces to browse through a large corpus of precious Buddha faces for facilitating annotations on the basic meta-data on the statues, which will then serve as a source to train more sophisticated models for analyzing them.\n  For example, we built a model that can embed various information on target entities (i.e., Buddha status), such as authors, eras, places, etc., into a vector representation of images and use them for other tasks like classification, through the model below.\n  ","date":1592401961,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1592401961,"objectID":"fc6780ed8b6f6fab04c94b520f5cc154","permalink":"/project/buddha-face/","publishdate":"2020-06-17T22:52:41+09:00","relpermalink":"/project/buddha-face/","section":"project","summary":"In collaboration with Prof. Fujioka with Graduate School of Letters/School of Letters, Osaka University, we are attempting to create an AI for analyzing various aspects of Buddha faces in images.","tags":["buddha"],"title":"Buddha Face and AI","type":"project"},{"authors":["Web Administrator"],"categories":[],"content":"","date":1591714800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1591714800,"objectID":"fe4adb6f5c1529af511657f14929d01e","permalink":"/post/comprehensive_partnership_with_u-yamanashi_and_tachibana-u/","publishdate":"2020-06-10T00:00:00+09:00","relpermalink":"/post/comprehensive_partnership_with_u-yamanashi_and_tachibana-u/","section":"post","summary":"","tags":[],"title":"IDS Concluded comprehensive partnership with University of Yamanashi and Kyoto Tachibana University","type":"post"},{"authors":["Kiminori Yanagisawa","Masayasu Toratani","Ayumu Asai","Masamitsu Konno","Hirohiko Niioka","Tsunekazu Mizushima","Taroh Satoh","Jun Miyake","Kazuhiko Ogawa","Andrea Vecchione","Yuichiro Doki","Hidetoshi Eguchi","Hideshi Ishii"],"categories":null,"content":"","date":1585699200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585699200,"objectID":"9cb95097bb5522425465287c80766770","permalink":"/publication/yanagisawa-2020/","publishdate":"2020-08-03T06:16:28.668379Z","relpermalink":"/publication/yanagisawa-2020/","section":"publication","summary":"textlessptextgreaterIt is known that single or isolated tumor cells enter cancer patients' circulatory systems. These circulating tumor cells (CTCs) are thought to be an effective tool for diagnosing cancer malignancy. However, handling CTC samples and evaluating CTC sequence analysis results are challenging. Recently, the convolutional neural network (CNN) model, a type of deep learning model, has been increasingly adopted for medical image analyses. However, it is controversial whether cell characteristics can be identified at the single-cell level by using machine learning methods. This study intends to verify whether an AI system could classify the sensitivity of anticancer drugs, based on cell morphology during culture. We constructed a CNN based on the VGG16 model that could predict the efficiency of antitumor drugs at the single-cell level. The machine learning revealed that our model could identify the effects of antitumor drugs with ̃0.80 accuracies. Our results show that, in the future, realizing precision medicine to identify effective antitumor drugs for individual patients may be possible by extracting CTCs from blood and performing classification by using an AI system.textless/ptextgreater","tags":null,"title":"Convolutional Neural Network Can Recognize Drug Resistance of Single Cancer Cells","type":"publication"},{"authors":["Manisha Verma","Sudhakar Kumawat","Yuta Nakashima","Shanmuganathan Raman"],"categories":null,"content":"","date":1585699200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585699200,"objectID":"d112eb00805143e765af51dd5ad6f1b9","permalink":"/publication/verma-2020/","publishdate":"2020-08-03T06:16:28.751208Z","relpermalink":"/publication/verma-2020/","section":"publication","summary":"Human pose estimation is a well-known problem in computer vision to locate joint positions. Existing datasets for the learning of poses are observed to be not challenging enough in terms of pose diversity, object occlusion, and viewpoints. This makes the pose annotation process relatively simple and restricts the application of the models that have been trained on them. To handle more variety in human poses, we propose the concept of fine-grained hierarchical pose classification, in which we formulate the pose estimation as a classification task, and propose a dataset, Yoga-82, for large-scale yoga pose recognition with 82 classes. Yoga-82 consists of complex poses where fine annotations may not be possible. To resolve this, we provide hierarchical labels for yoga poses based on the body configuration of the pose. The dataset contains a three-level hierarchy including body positions, variations in body positions, and the actual pose names. We present the classification accuracy of the state-of-the-art convolutional neural network architectures on Yoga-82. We also present several hierarchical variants of DenseNet in order to utilize the hierarchical labels.","tags":null,"title":"Yoga-82: a new dataset for fine-grained classification of human poses","type":"publication"},{"authors":["Shogo Terai","Shizuka Shirai","Mehrasa Alizadeh","Ryosuke Kawamura","Noriko Takemura","Yuki Uranishi","Haruo Takemura","Hajime Nagahara"],"categories":null,"content":"","date":1583020800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583020800,"objectID":"a212aa5587763fd90848f0ac09106b5f","permalink":"/publication/terai-2020/","publishdate":"2020-08-03T06:16:28.735536Z","relpermalink":"/publication/terai-2020/","section":"publication","summary":"Drowsiness is a major factor that hinders learning. To improve learning efficiency, it is important to understand students' physical status such as wakefulness during online coursework. In this study, we have proposed a drowsiness estimation method based on learners' head and facial movements while viewing video lectures. To examine the effectiveness of head and facial movements in drowsiness estimation, we collected learner video data recorded during e-learning and applied a deep learning approach under the following conditions: (a) using only facial movement data, (b) using only head movement data, and (c) using both facial and head movement data.We achieved an average F1-macro score of 0.74 in personalized models for detecting learner drowsiness using both facial and head movement data.","tags":["drowsiness estimation","e-learning","facial movements","head movements"],"title":"Detecting learner drowsiness based on facial expressions and head movements in online courses","type":"publication"},{"authors":["Web Administrator"],"categories":[],"content":"","date":1581001200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1581001200,"objectID":"a8bf0346e29f530d73b2a9b2ea7de151","permalink":"/post/ids_symposium_2020-02/","publishdate":"2020-02-07T00:00:00+09:00","relpermalink":"/post/ids_symposium_2020-02/","section":"post","summary":"","tags":[],"title":"Hosted IDS Symposium (in Japanese)","type":"post"},{"authors":["Noa Garcia","Mayu Otani","Chenhui Chu","Yuta Nakashima"],"categories":null,"content":"","date":1580515200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1580515200,"objectID":"375e830cecf035f28bff9f1a2ec02b17","permalink":"/publication/garcia-2020-a/","publishdate":"2020-08-03T06:16:28.696525Z","relpermalink":"/publication/garcia-2020-a/","section":"publication","summary":"We propose a novel video understanding task by fusing knowledge-based and video question answering. First, we introduce KnowIT VQA, a video dataset with 24,282 human-generated question-answer pairs about a popular sitcom. The dataset combines visual, textual and temporal coherence reasoning together with knowledge-based questions, which need of the experience obtained from the viewing of the series to be answered. Second, we propose a video understanding model by combining the visual and textual video content with specific knowledge about the show. Our main findings are: (i) the incorporation of knowledge produces outstanding improvements for VQA in video, and (ii) the performance on KnowIT VQA still lags well behind human accuracy, indicating its usefulness for studying current video modelling limitations.","tags":["kvqa"],"title":"KnowIT VQA: Answering knowledge-based questions about videos","type":"publication"},{"authors":["Takahiro Yamaguchi","Hajime Nagahara","Ken'ichi Morooka","Yuta Nakashima","Yuki Uranishi","Shoko Miyauchi","Ryo Kurazume"],"categories":null,"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577836800,"objectID":"864f1353406978fe35b11f5194909e6d","permalink":"/publication/yamaguchi-2020/","publishdate":"2020-08-03T06:16:28.639346Z","relpermalink":"/publication/yamaguchi-2020/","section":"publication","summary":"This paper presents a method for reconstructing 3D image from multi-focus microscopic images captured with different focuses. We model the multi-focus imaging by a microscopy and produce the 3D image of a target object based on the model. The 3D image reconstruction is done by minimizing the difference between the observed images and the simulated images generated by the imaging model. Simulation and experimental result shows that the proposed method can generate the 3D image of a transparent object efficiently and reliably.","tags":["3d imaging","microscopy","multi-focus images","transparent object"],"title":"3D Image Reconstruction from Multi-focus Microscopic Images","type":"publication"},{"authors":["Zekun Yang","Noa Garcia","Chenhui Chu","Mayu Otani","Yuta Nakashima","Haruo Takemura"],"categories":null,"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577836800,"objectID":"43dde4cd53f4c67bbe45798593d7b0ce","permalink":"/publication/yang-2020/","publishdate":"2020-08-03T06:16:28.858534Z","relpermalink":"/publication/yang-2020/","section":"publication","summary":"Visual question answering (VQA) aims at answering questions about the visual content of an image or a video. Currently, most work on VQA is focused on image-based question answering, and less attention has been paid into answering questions about videos. However, VQA in video presents some unique challenges that are worth studying: it not only requires to model a sequence of visual features over time, but often it also needs to reason about associated subtitles. In this work, we propose to use BERT, a sequential modelling technique based on Transformers, to encode the complex semantics from video clips. Our proposed model jointly captures the visual and language information of a video scene by encoding not only the subtitles but also a sequence of visual concepts with a pretrained language-based Transformer. In our experiments, we exhaustively study the performance of our model by taking different input arrangements, showing outstanding improvements when compared against previous work on two well-known video VQA datasets: TVQA and Pororo.","tags":["kvqa"],"title":"BERT representations for video question answering","type":"publication"},{"authors":["Noa Garcia","Benjamin Renoust","Yuta Nakashima"],"categories":null,"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577836800,"objectID":"59a4022388c2136510187dd95d09c49a","permalink":"/publication/garcia-2020/","publishdate":"2020-08-03T06:16:28.863834Z","relpermalink":"/publication/garcia-2020/","section":"publication","summary":"© 2019, The Author(s). In automatic art analysis, models that besides the visual elements of an artwork represent the relationships between the different artistic attributes could be very informative. Those kinds of relationships, however, usually appear in a very subtle way, being extremely difficult to detect with standard convolutional neural networks. In this work, we propose to capture contextual artistic information from fine-art paintings with a specific ContextNet network. As context can be obtained from multiple sources, we explore two modalities of ContextNets: one based on multitask learning and another one based on knowledge graphs. Once the contextual information is obtained, we use it to enhance visual representations computed with a neural network. In this way, we are able to (1) capture information about the content and the style with the visual representations and (2) encode relationships between different artistic attributes with the ContextNet. We evaluate our models on both painting classification and retrieval, and by visualising the resulting embeddings on a knowledge graph, we can confirm that our models represent specific stylistic aspects present in the data.","tags":["art classification","knowledge graphs","multi-modal retrieval","multitask learning","visualisation","buddha","kvqa"],"title":"ContextNet: representation and exploration for painting classification and retrieval in context","type":"publication"},{"authors":["Liangzhi Li","Manisha Verma","Yuta Nakashima","Hajime Nagahara","Ryo Kawasaki"],"categories":null,"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577836800,"objectID":"d25966619208c9633885f8986cffd891","permalink":"/publication/li-2020/","publishdate":"2020-08-03T06:16:28.571942Z","relpermalink":"/publication/li-2020/","section":"publication","summary":"Retinal vessel segmentation is of great interest for diagnosis of retinal vascular diseases. To further improve the performance of vessel segmentation, we propose IterNet, a new model based on UNet, with the ability to find obscured details of the vessel from the segmented vessel image itself, rather than the raw input image. IterNet consists of multiple iterations of a mini-UNet, which can be 4$backslashtimes$ deeper than the common UNet. IterNet also adopts the weight-sharing and skip-connection features to facilitate training; therefore, even with such a large architecture, IterNet can still learn from merely 10$backslashsim$20 labeled images, without pre-training or any prior knowledge. IterNet achieves AUCs of 0.9816, 0.9851, and 0.9881 on three mainstream datasets, namely DRIVE, CHASE-DB1, and STARE, respectively, which currently are the best scores in the literature. The source code is available.","tags":null,"title":"IterNet: retinal image segmentation utilizing structural redundancy in vessel networks","type":"publication"},{"authors":["Liangzhi Li","Manisha Verma","Yuta Nakashima","Ryo Kawasaki","Hajime Nagahara"],"categories":null,"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577836800,"objectID":"e87104bfce802dcbb3bcdd6e89dd53e0","permalink":"/publication/li-2020-a/","publishdate":"2020-08-03T06:16:28.876885Z","relpermalink":"/publication/li-2020-a/","section":"publication","summary":"Retinal imaging serves as a valuable tool for diagnosis of various diseases. However, reading retinal images is a difficult and time-consuming task even for experienced specialists. The fundamental step towards automated retinal image analysis is vessel segmentation and artery/vein classification, which provide various information on potential disorders. To improve the performance of the existing automated methods for retinal image analysis, we propose a two-step vessel classification. We adopt a UNet-based model, SeqNet, to accurately segment vessels from the background and make prediction on the vessel type. Our model does segmentation and classification sequentially, which alleviates the problem of label distribution bias and facilitates training. To further refine classification results, we post-process them considering the structural information among vessels to propagate highly confident prediction to surrounding vessels. Our experiments show that our method improves AUC to 0.98 for segmentation and the accuracy to 0.92 in classification over DRIVE dataset.","tags":["medical imaging","computer vision","deep learning","retina images","vessel classification","vessel segmentation"],"title":"Joint learning of vessel segmentation and artery/vein classification with post-processing","type":"publication"},{"authors":["Yuta Nakashima","Takaaki Yasui","Leon Nguyen","Noboru Babaguchi"],"categories":null,"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577836800,"objectID":"c477ddc75895f2e0ced06cef2055affe","permalink":"/publication/nakashima-2020/","publishdate":"2020-08-03T06:16:28.686409Z","relpermalink":"/publication/nakashima-2020/","section":"publication","summary":"We present a system for reenacting a person's face driven by speech. Given a video sequence with the corresponding audio track of a person giving a speech and another audio track containing different speech from the same person, we reconstruct a 3D mesh of the face in each frame of the video sequence to match the speech in the second audio track. Audio features are extracted from such two audio tracks. Assuming that the appearance of the mouth is highly correlated to these speech features, we extract the mouth region of the face's 3D mesh from the video sequence when speech features from the second audio track are close to those of the video's audio track. While retaining temporal consistency, these extracted mouth regions then replace the original mouth regions in the video sequence, synthesizing a reenactment video where the person seemingly gives the speech from the second audio track. Our system, coined S2TH (speech to talking head), does not require any special hardware to capture the 3D geometry of faces but uses the state-of-the-art method for facial geometry regression. We visually and subjectively demonstrate reenactment quality.","tags":["3d geometry","face reenactment","speech-driven","talking head","speech-driven","talking head"],"title":"Speech-driven face reenactment for a video sequence","type":"publication"},{"authors":["T. Kimura","N. Takemura","Y. Nakashima","H. Kobori","H. Nagahara","M. Numao","K. Shinohara"],"categories":null,"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577836800,"objectID":"632ce19a34887115b9a173c37fb8d069","permalink":"/publication/kimura-2020/","publishdate":"2020-08-03T06:16:28.881801Z","relpermalink":"/publication/kimura-2020/","section":"publication","summary":"© Copyright © 2020 Kimura, Takemura, Nakashima, Kobori, Nagahara, Numao and Shinohara. Climate change is one of the most important issues for humanity. To defuse this problem, it is considered necessary to improve energy efficiency, make energy sources cleaner, and reduce energy consumption in urban areas. The Japanese government has recommended an air conditioner setting of 28°C in summer and 20°C in winter since 2005. The aim of this setting is to save energy by keeping room temperatures constant. However, it is unclear whether this is an appropriate temperature for workers and students. This study examined whether thermal environments influence task performance over time. To examine whether the relationship between task performance and thermal environments influences the psychological states of participants, we recorded their subjective rating of mental workload along with their working memory score, electroencephalogram (EEG), heart rate variability, skin conductance level (SCL), and tympanum temperature during the task and compared the results among different conditions. In this experiment, participants were asked to read some texts and answer questions related to those texts. Room temperature (18, 22, 25, or 29°C) and humidity (50%) were manipulated during the task and participants performed the task at these temperatures. The results of this study showed that the temporal cost of task and theta power of EEG, which is an index for concentration, decreased over time. However, subjective mental workload increased with time. Moreover, the low frequency to high frequency ratio and SCL increased with time and heat (25 and 29°C). These results suggest that mental workload, especially implicit mental workload, increases in warmer environments, even if learning efficiency is facilitated. This study indicates integrated evidence for relationships among task performance, psychological state, and thermal environment by analyzing behavioral, subjective, and physiological indexes multidirectionally.","tags":["eeg","autonomic nervous system","learning efficiency","mental workload","thermal environment"],"title":"Warmer Environments Increase Implicit Mental Workload Even If Learning Efficiency Is Enhanced","type":"publication"},{"authors":["Kazuki Ashihara","Tomoyuki Kajiwara","Yuki Arase","Satoru Uchida"],"categories":null,"content":"","date":1575158400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1575158400,"objectID":"af6a77bcea40c4339dd01cb9963e6279","permalink":"/publication/ashihara-2019-a/","publishdate":"2020-08-03T06:16:28.892038Z","relpermalink":"/publication/ashihara-2019-a/","section":"publication","summary":"Currently, distributed word representations are employed in many natural language processing tasks. However, when generating one representation for each word, the meanings of a polysemous word cannot be differentiated because the meanings are integrated into one representation. Therefore, several attempts have been made to generate different representations per meaning based on parts of speech or the topic of a sentence. However, these methods are too unrefined to deal with polysemy. In this paper, we proposed two methods to generate more subtle multiple word representations. The first method involves generating multiple word representations using the word in a dependency relationship as a clue. The second approach involves employing a bi-directional language model in which a word representation that considers all the words in the context is generated. The results of the extensive evaluation of the Lexical Substitution task and Context-Aware Word Similarity task confirmed the effectiveness of our approaches to generate more subtle multiple word representations.","tags":null,"title":"Contextualized multi-sense word embedding","type":"publication"},{"authors":["Tatsuya Matsumoto","Hirohiko Niioka","Yasuaki Kumamoto","Junya Sato","Osamu Inamori","Ryuta Nakao","Yoshinori Harada","Eiichi Konishi","Eigo Otsuji","Hideo Tanaka","Jun Miyake","Tetsuro Takamatsu"],"categories":null,"content":"","date":1575158400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1575158400,"objectID":"c3e1054f96fd009755c6cd029d9a1ef5","permalink":"/publication/matsumoto-2019/","publishdate":"2020-08-03T06:16:28.645217Z","relpermalink":"/publication/matsumoto-2019/","section":"publication","summary":"","tags":null,"title":"Deep-UV excitation fluorescence microscopy for detection of lymph node metastasis using deep neural network","type":"publication"},{"authors":["Thanh Trung Ngo","Hajime Nagahara","Ko Nishino","Rin ichiro Taniguchi","Yasushi Yagi"],"categories":null,"content":"","date":1575158400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1575158400,"objectID":"1ac1204728d98dad409b75773a2ea3c4","permalink":"/publication/ngo-2019/","publishdate":"2020-08-03T06:16:28.7018Z","relpermalink":"/publication/ngo-2019/","section":"publication","summary":"Reflectance and shape are two important components in visually perceiving the real world. Inferring the reflectance and shape of an object through cameras is a fundamental research topic in the field of computer vision. While three-dimensional shape recovery is pervasive with varieties of approaches and practical applications, reflectance recovery has only emerged recently. Reflectance recovery is a challenging task that is usually conducted in controlled environments, such as a laboratory environment with a special apparatus. However, it is desirable that the reflectance be recovered in the field with a handy camera so that reflectance can be jointly recovered with the shape. To that end, we present a solution that simultaneously recovers the reflectance and shape (i.e., dense depth and normal maps) of an object under natural illumination with commercially available handy cameras. We employ a light field camera to capture one light field image of the object, and a 360-degree camera to capture the illumination. The proposed method provides positive results in both simulation and real-world experiments.","tags":["light field camera","natural illumination","reflectance","shape from shading"],"title":"Reflectance and Shape Estimation with a Light Field Camera Under Natural Illumination","type":"publication"},{"authors":["Akihiko Sayo","Hayato Onizuka","Diego Thomas","Yuta Nakashima","Hiroshi Kawasaki","Katsushi Ikeuchi"],"categories":null,"content":"","date":1572566400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1572566400,"objectID":"985d2d9d0df24a4e735c0a435603abab","permalink":"/publication/sayo-2019/","publishdate":"2020-08-03T06:16:28.72224Z","relpermalink":"/publication/sayo-2019/","section":"publication","summary":"Reconstructing the entire body of moving human in a computer is important for various applications, such as tele-presence, virtual try-on, etc. For the purpose, realistic representation of loose clothes or non-rigid body deformation is a challenging and important task. Recent approaches for full-body reconstruction use a statistical shape model, which is built upon accurate full-body scans of people in skin-tight clothes. Such a model can be fitted to a point cloud of a person wearing loose clothes, however, it cannot represent the detailed shape of loose clothes, such as wrinkles and/or folds. In this paper, we propose a method that reconstructs 3D model of full-body human with loose clothes by reproducing the deformations as displacements from the skin-tight body mesh. We take advantage of a statistical shape model as base shape of full-body human mesh, and then, obtain displacements from the base mesh by non-rigid registration. To efficiently represent such displacements, we use lower dimensional embeddings of the deformations. This enables us to regress the coefficients corresponding to the small number of bases. We also propose a method to reconstruct shape only from a single 3D scanner, which is realized by shape fitting to only visible meshes as well as intra-frame shape interpolation. Our experiments with both unknown scene and partial body scans confirm the reconstruction ability of our proposed method.","tags":["eigen-deformation","human shape reconstruction","neural network","non-rigid registration"],"title":"Human shape reconstruction with loose clothes from partially observed data by pose specific deformation","type":"publication"},{"authors":["Kazuki Ashihara","Chenhui Chu","Benjamin Renoust","Noriko Okubo","Noriko Takemura","Yuta Nakashima","Hajime Nagahara"],"categories":null,"content":"","date":1572566400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1572566400,"objectID":"da3fc54d1e1ab7797a4ab23f12ffdd2f","permalink":"/publication/ashihara-2019-b/","publishdate":"2020-08-03T06:16:28.727526Z","relpermalink":"/publication/ashihara-2019-b/","section":"publication","summary":"Topic modeling is a key component to computational legal science. Network analysis is also very important to further understand the structure of references in legal documents. In this paper, we improve topic modeling for legal case documents by using homophily networks derived from two families of references: prior cases and statute laws. We perform a detailed analysis on a rich legal case dataset in order to create these networks. The use of the reference-induced homophily topic modeling improves on prior methods.","tags":["homophily","network of legal documents","topic modeling"],"title":"Legal information as a complex network: Improving topic modeling through homophily","type":"publication"},{"authors":["Thuong Nguyen Canh","Hajime Nagahara"],"categories":null,"content":"","date":1569888000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1569888000,"objectID":"5be74e60df2704d6b1d60b449d391add","permalink":"/publication/nguyen-canh-2019/","publishdate":"2020-08-03T06:16:28.806494Z","relpermalink":"/publication/nguyen-canh-2019/","section":"publication","summary":"Detection followed by projection in conventional privacy cameras is vulnerable to software attacks that threaten to expose image sensor data. By multiplexing the incoming light with a coded mask, a FlatCam camera removes the spatial correlation and captures visually protected images. However, FlatCam imaging suffers from poor reconstruction quality and pays no attention to the privacy of visual information. In this paper, we propose a deep learning-based compressive sensing approach to reconstruct and protect sensitive regions from secured FlatCam measurements. We predict sensitive regions via facial segmentation and separate them from the captured measurements. Our deep compressive sensing network was trained with simulated data, and was tested on both simulated and real FlatCam data.","tags":["compressive sensing","deep compressive sensing","deep learning","visual privacy protection"],"title":"Deep compressive sensing for visual privacy protection in flatcam imaging","type":"publication"},{"authors":["Keita Maruyama","Yasutaka Inagaki","Keita Takahashi","Toshiaki Fujii","Hajime Nagahara"],"categories":null,"content":"","date":1567296000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1567296000,"objectID":"c73b2899368297686382311084c8adbe","permalink":"/publication/maruyama-2019/","publishdate":"2020-08-03T06:16:28.812036Z","relpermalink":"/publication/maruyama-2019/","section":"publication","summary":"We propose an efficient pipeline from input to output for a tensor light-field display. Conventionally, a dense light field (i.e., tens of images taken with narrow viewpoint intervals) is required as an input in such displays. However, obtaining dense light fields is a challenging task for real scenes. To make the acquisition process more efficient, we adopted a coded-aperture camera as an input device, which is suitable for acquiring dense light fields in a compressive manner. Moreover, we modeled the entire process from acquisition to display using a convolutional neural network. As a result of training the network on a massive light field data, we can reproduce the whole light field on the display from only a few images taken with the camera. Both simulative and real experiments were conducted to show the effectiveness of our method.","tags":["3-d display","coded-aperture camera","light field"],"title":"A 3-D Display Pipeline from Coded-Aperture Camera to Tensor Light-Field Display Through CNN","type":"publication"},{"authors":["Hiroki Shimanaka","Tomoyuki Kajiwara","Mamoru Komachi"],"categories":null,"content":"","date":1567296000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1567296000,"objectID":"a291cc40a8ee32058af4f6eb09e7a228","permalink":"/publication/shimanaka-2019/","publishdate":"2020-08-03T06:16:28.566995Z","relpermalink":"/publication/shimanaka-2019/","section":"publication","summary":"This study describes a segment-level metric for automatic machine translation evaluation (MTE). Although various MTE metrics have been proposed, most MTE metrics, including the current de facto standard BLEU, can handle only limited information for segment-level MTE. Therefore, we propose an MTE metric using pre-trained sentence embeddings in order to evaluate MT translation considering global information. In our proposed method, we obtain sentence embeddings of MT translation and reference translation using a sentence encoder pre-trained on a large corpus. Then, we estimate the translation quality by a regression model based on sentence embeddings of MT translation and reference translation as input. Our metric achieved state-of-the-art performance in segment-level metrics tasks for all to-English language pairs on the WMT dataset with human evaluation score.","tags":null,"title":"Metric for automatic machine translation evaluation based on pre-trained sentence embeddings","type":"publication"},{"authors":["Masahito Yamanaka","Hirohiko Niioka","Taichi Furukawa","Norihiko Nishizawa"],"categories":null,"content":"","date":1561939200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561939200,"objectID":"00505447884a5740c8eb1e9cf17d4c56","permalink":"/publication/yamanaka-2019/","publishdate":"2020-08-03T06:16:28.558534Z","relpermalink":"/publication/yamanaka-2019/","section":"publication","summary":"","tags":null,"title":"Excitation of erbium-doped nanoparticles in 1550-nm wavelength region for deep tissue imaging with reduced degradation of spatial resolution","type":"publication"},{"authors":["Masahiro Yanagawa","Hirohiko Niioka","Akinori Hata","Noriko Kikuchi","Osamu Honda","Hiroyuki Kurakami","Eiichi Morii","Masayuki Noguchi","Yoshiyuki Watanabe","Jun Miyake","Noriyuki Tomiyama"],"categories":null,"content":"","date":1559347200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559347200,"objectID":"7f970bd13ae55d6c5d39557b4e10c2c9","permalink":"/publication/yanagawa-2019/","publishdate":"2020-08-03T06:16:28.903542Z","relpermalink":"/publication/yanagawa-2019/","section":"publication","summary":"","tags":null,"title":"Application of deep learning (3-dimensional convolutional neural network) for the prediction of pathological invasiveness in lung adenocarcinoma","type":"publication"},{"authors":["Shizuka Shirai","Noriko Takemura","Yuta Nakashima","Hajime Nagahara","Haruo Takemura"],"categories":null,"content":"","date":1551398400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1551398400,"objectID":"55f8e2e1ec664af5ae59d3e5c14bbab6","permalink":"/publication/shirai-2019/","publishdate":"2020-08-03T06:16:28.790007Z","relpermalink":"/publication/shirai-2019/","section":"publication","summary":"","tags":["sensing"],"title":"Multimodal learning analytics: Society 5.0 project in Japan","type":"publication"},{"authors":["Chao Ma","Atsushi Shimada","Hideaki Uchiyama","Hajime Nagahara","Rin ichiro Taniguchi"],"categories":null,"content":"","date":1548979200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1548979200,"objectID":"78d7554a152e4243ba60c7dc10c3345c","permalink":"/publication/ma-2019/","publishdate":"2020-08-03T06:16:28.548417Z","relpermalink":"/publication/ma-2019/","section":"publication","summary":"Fall is one of the leading causes of injury for the elderly individuals. Systems that automatically detect falls can significantly reduce the delay of assistance. Most of commercialized fall detection systems are based on wearable devices, which elderly individuals tend to forget wearing. Using surveillance cameras to detect falls based on computer vision is ideal, because anyone in the monitoring scopes can be under protection. However, the privacy protection issue using surveillance cameras has been bothering people. To effectively protect the privacy, we proposed an optical level anonymous image sensing system, which can protect the privacy by hiding the facial regions optically at the video capturing phase. We apply the system to fall detection. In detecting falls, we propose a neural network by combining a 3D convolutional neural network for feature extraction and an autoencoder for modelling the normal behaviors. The learned autoencoder reconstructs the features extracted from videos with normal behaviors with smaller average errors than those extracted from videos with falls. We evaluated our neural network by a hold-out validation experiment, and showed its effectiveness. In field tests, we showed and discussed the applicability of the optical level anonymous image sensing system for privacy protection and fall detection.","tags":["3d convolutional neural network","computational imaging","fall detection","optical level anonymous","privacy protection"],"title":"Fall detection using optical level anonymous image sensing system","type":"publication"},{"authors":["H. Hamasaki","S. Takeshita","K. Nakai","T. Sonoda","H. Kawasaki","H. Nagahara","S. Ono"],"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"5720335b3b6d80785fa72e85f22005a0","permalink":"/publication/hamasaki-2019/","publishdate":"2020-08-03T06:16:28.853653Z","relpermalink":"/publication/hamasaki-2019/","section":"publication","summary":"© 2019, Springer Nature Switzerland AG. Barcodes and 2D codes are widely used for various purposes, such as electronic payments and product management. Special code readers, and consumer smartphones can be used to scan codes; thus concerns about fraud and authenticity are important. Embedding watermarks in 2D codes, which allows simultaneous recognition and tamper detection by simply analyzing the captured pattern without requiring an additional device is considered a promising solution. However, smartphone cameras frequently suffer misfocus especially if the target object is too close to the lens, which makes the captured image defocused and results in failure to read watermarks. In this paper, we propose the use of a coded aperture imaging technique to recover watermarks. We have designed a coded aperture that is robust against defocus blur by optimizing the aperture pattern using a genetic algorithm. In addition, we have developed a programmable coded aperture that includes an actual optical process that works in an optimization loop; thus, the complicated effects of the optical aberrations can be considered. Experimental results demonstrate that the proposed method can extend the depth of field for watermark extraction to 3.1 times wider than that of a general circular aperture.","tags":["coded aperture","device-based optimization","digital image watermark","extended depth of field","genetic algorithm","two-dimensional code"],"title":"A Coded Aperture for Watermark Extraction from Defocused Images","type":"publication"},{"authors":null,"categories":null,"content":" Location   Institute for Datability ScienceOsaka University\n  Techno-Alliance Building C503, 2-8 Yamadaoka, Suita, Osaka, 565-0871 Japan Phone : +81-6-6105-6074 FAX : +81-6-6105-6075\n","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"a303b732cb756ed20ac9380aa865b22f","permalink":"/access/","publishdate":"2019-01-01T00:00:00Z","relpermalink":"/access/","section":"","summary":"Location   Institute for Datability ScienceOsaka University\n  Techno-Alliance Building C503, 2-8 Yamadaoka, Suita, Osaka, 565-0871 Japan Phone : +81-6-6105-6074 FAX : +81-6-6105-6075","tags":null,"title":"Access","type":"widget_page"},{"authors":["Benjamin Renoust","Matheus Oliveira M.O. Franca","Jacob Chan","Van Le","Ayaka Uesaka","Yuta Nakashima","Hajime Nagahara","Jueren Wang","Yutaka Fujioka"],"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"ce2d696c0ce06437425f7e24d8b75cfb","permalink":"/publication/renoust-2019-a/","publishdate":"2020-08-03T06:16:28.663114Z","relpermalink":"/publication/renoust-2019-a/","section":"publication","summary":"© 2019 Copyright held by the owner/author(s). We introduce BUDA.ART, a system designed to assist researchers in Art History, to explore and analyze an archive of pictures of Buddha statues. The system combines different CBIR and classical retrieval techniques to assemble 2D pictures, 3D statue scans and meta-data, that is focused on the Buddha facial characteristics. We build the system from an archive of 50,000 Buddhism pictures, identify unique Buddha statues, extract contextual information, and provide specific facial embedding to first index the archive. The system allows for mobile, on-site search, and to explore similarities of statues in the archive. In addition, we provide search visualization and 3D analysis of the statues.","tags":["2d","3d","art history","multimedia database","search system","buddha"],"title":"Buda.art: A multimodal content-based analysis and retrieval system for Buddha statues","type":"publication"},{"authors":["Noa Garcia","Benjamin Renoust","Yuta Nakashima"],"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"60905602338f187fde9d4dd0e8e3e274","permalink":"/publication/garcia-2019-a/","publishdate":"2020-08-03T06:16:28.869044Z","relpermalink":"/publication/garcia-2019-a/","section":"publication","summary":"© 2019 Association for Computing Machinery. Automatic art analysis aims to classify and retrieve artistic representations from a collection of images by using computer vision and machine learning techniques. In this work, we propose to enhance visual representations from neural networks with contextual artistic information. Whereas visual representations are able to capture information about the content and the style of an artwork, our proposed context-aware embeddings additionally encode relationships between different artistic attributes, such as author, school, or historical period. We design two different approaches for using context in automatic art analysis. In the first one, contextual data is obtained through a multi-task learning model, in which several attributes are trained together to find visual relationships between elements. In the second approach, context is obtained through an art-specific knowledge graph, which encodes relationships between artistic attributes. An exhaustive evaluation of both of our models in several art analysis problems, such as author identification, type classification, or cross-modal retrieval, show that performance is improved by up to 7.3% in art classification and 37.24% in retrieval when context-aware embeddings are used.","tags":["art classification","knowledge graphs","multi-modal retrieval","buddha","kbqa"],"title":"Context-aware embeddings for automatic art analysis","type":"publication"},{"authors":["Kazuki Ashihara","Tomoyuki Kajiwara","Yuki Arase","Satoru Uchida"],"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"284eef8cf927459c2e133c80ed0766c7","permalink":"/publication/ashihara-2019/","publishdate":"2020-08-03T06:16:28.816809Z","relpermalink":"/publication/ashihara-2019/","section":"publication","summary":"Lexical substitution ranks substitution candidates from the viewpoint of paraphrasability for a target word in a given sentence. There are two major approaches for lexical substitution: (1) generating contextualized word embeddings by assigning multiple embeddings to one word and (2) generating context embeddings using the sentence. Herein we propose a method that combines these two approaches to contextualize word embeddings for lexical substitution. Experiments demonstrate that our method outperforms the current state-of-the-art method. We also create CEFR-LP, a new evaluation dataset for the lexical substitution task. It has a wider coverage of substitution candidates than previous datasets and assigns English proficiency levels to all target words and substitution candidates.","tags":null,"title":"Contextualized context2vec","type":"publication"},{"authors":["Daiki. Nishihara","Tomoyuki. Kajiwara","Yuki. Arase"],"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"29c722bc41de3abbcf523dfafa31eec1","permalink":"/publication/nishihara-2019/","publishdate":"2020-08-03T06:16:28.651485Z","relpermalink":"/publication/nishihara-2019/","section":"publication","summary":"We propose a method to control the level of a sentence in a text simplification task. Text simplification is a monolingual translation task translating a complex sentence into a simpler and easier to understand the alternative. In this study, we use the grade level of the US education system as the level of the sentence. Our text simplification method succeeds in translating an input into a specific grade level by considering levels of both sentences and words. Sentence level is considered by adding the target grade level as input. By contrast, the word level is considered by adding weights to the training loss based on words that frequently appear in sentences of the desired grade level. Although existing models that consider only the sentence level may control the syntactic complexity, they tend to generate words beyond the target level. Our approach can control both the lexical and syntactic complexity and achieve an aggressive rewriting. Experiment results indicate that the proposed method improves the metrics of both BLEU and SARI.","tags":null,"title":"Controllable text simplification with lexical constraint loss","type":"publication"},{"authors":["R. Tsutsumi","T. Ikeda","H. Nagahara","H. Saeki","Y. Nakashima","E. Oki","Y. Maehara","M. Hashizume"],"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"638c49716668b11781de58da4a8930f7","permalink":"/publication/tsutsumi-2019/","publishdate":"2020-08-03T06:16:28.543383Z","relpermalink":"/publication/tsutsumi-2019/","section":"publication","summary":"© 2019 The Authors  Background: Biomedical imaging devices that utilize the optical characteristics of hemoglobin (Hb) have become widespread. In the field of gastroenterology, there is a strong demand for devices that can apply this technique to surgical navigation. We aimed to introduce our novel multispectral device capable of intraoperatively performing quantitative imaging of the oxygen (O 2 ) saturation and Hb amount of tissues noninvasively and in real time, and to examine its application for deciding the appropriate anastomosis point after subtotal or total esophagectomy. Materials and methods: A total of 39 patients with esophageal cancer were studied. Tissue O 2 saturation and Hb amount of the gastric tube just before esophagogastric anastomosis were evaluated using a multispectral tissue quantitative imaging device. The anastomosis point was decided depending on the quantitative values and patterns of both the tissue O 2 saturation and Hb amount. Results: The device can instantaneously and noninvasively quantify and visualize the tissue O 2 saturation and Hb amount using reflected light. The tissue Hb status could be classified into the following four types: good circulation type, congestion type, ischemia type, and mixed type of congestion and ischemia. Postoperative anastomotic failure occurred in 2 cases, and both were mixed cases. Conclusions: The method of quantitatively imaging the tissue O 2 saturation and Hb level in real time and noninvasively using a multispectral device allows instantaneous determination of the anastomosis and related organ conditions, thereby contributing to determining the appropriate treatment direction.","tags":["anastomosis","esophagogastrostomy","hemoglobin","oxygen saturation","quantitative imaging"],"title":"Efficacy of Novel Multispectral Imaging Device to Determine Anastomosis for Esophagogastrostomy","type":"publication"},{"authors":["Manisha Verma","Hirokazu Kobori","Yuta Nakashima","Noriko Takemura","Hajime Nagahara"],"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"37fad99d38c62360885391481b250c6f","permalink":"/publication/verma-2019/","publishdate":"2020-08-03T06:16:28.756123Z","relpermalink":"/publication/verma-2019/","section":"publication","summary":"Deep convolutional neural networks (CNNs) have established their feet in the ground of computer vision and machine learning, used in various applications. In this work, an attempt is made to learn a CNN for a task of facial expression recognition (FER). Our network has convolutional layers linked with an FC layer with a skip-connection to the classification layer. Motivation behind this design is that lower layers of a CNN are responsible for lower level features, and facial expressions can be mainly encoded in low-to-mid level features. Hence, in order to leverage the responses from lower layers, all convo-lutional layers are integrated via FC layers. Moreover, a network with shared parameters is used to extract landmark motion trajectory features. These visual and landmark features are fused to improve the performance. Our method is evaluated on the CK+ and Oulu-CASIA facial expression datasets.","tags":null,"title":"Facial expression recognition with skip-connection to leverage low-level features","type":"publication"},{"authors":["B. Renoust","M.O. Franca","J. Chan","N. Garcia","V. Le","A. Uesaka","Y. Nakashima","H. Nagahara","J. Wang","Y. Fujioka"],"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"bedd185957fb078fbf9a841dfdcd4e11","permalink":"/publication/renoust-2019/","publishdate":"2020-08-03T06:16:28.553857Z","relpermalink":"/publication/renoust-2019/","section":"publication","summary":"© 2019 Copyright held by the owner/author(s). While Buddhism has spread along the Silk Roads, many pieces of art have been displaced. Only a few experts may identify these works, subjectively to their experience. The construction of Buddha statues was taught through the definition of canon rules, but the applications of those rules greatly varies across time and space. Automatic art analysis aims at supporting these challenges. We propose to automatically recover the proportions induced by the construction guidelines, in order to use them and compare between different deep learning features for several classification tasks, in a medium size but rich dataset of Buddha statues, collected with experts of Buddhism art history.","tags":["art history","buddha statues","classification","face landmarks","buddha"],"title":"Historical and modern features for Buddha statue classification","type":"publication"},{"authors":["Tomoyuki Kajiwara"],"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"9042b67921242c6bea0c0027326eba47","permalink":"/publication/kajiwara-2020/","publishdate":"2020-08-03T06:16:28.897899Z","relpermalink":"/publication/kajiwara-2020/","section":"publication","summary":"Paraphrase generation can be regarded as monolingual translation. Unlike bilingual machine translation, paraphrase generation rewrites only a limited portion of an input sentence. Hence, previous methods based on machine translation often perform conservatively to fail to make necessary rewrites. To solve this problem, we propose a neural model for paraphrase generation that first identifies words in the source sentence that should be paraphrased. Then, these words are paraphrased by the negative lexically constrained decoding that avoids outputting these words as they are. Experiments on text simplification and formality transfer show that our model improves the quality of paraphrasing by making necessary rewrites to an input sentence.","tags":null,"title":"Negative lexically constrained decoding for paraphrase generation","type":"publication"},{"authors":null,"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"6087c0ef875554f4409ac52928d79279","permalink":"/projects/","publishdate":"2019-01-01T00:00:00Z","relpermalink":"/projects/","section":"","summary":"","tags":null,"title":"Our Projects","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"c1d17ff2b20dca0ad6653a3161942b64","permalink":"/people/","publishdate":"2019-01-01T00:00:00Z","relpermalink":"/people/","section":"","summary":"","tags":null,"title":"People","type":"widget_page"},{"authors":["Mayu Otani","Yuta Nakashima","Esa Rahtu","Janne Heikkilä","Janne Heikkila"],"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"94d6967997a6d2a4f1e803a853ab6dd5","permalink":"/publication/otani-2019-a/","publishdate":"2020-08-03T06:16:28.745943Z","relpermalink":"/publication/otani-2019-a/","section":"publication","summary":"Video summarization is a technique to create a short skim of the original video while preserving the main stories/content. There exists a substantial interest in automatizing this process due to the rapid growth of the available material. The recent progress has been facilitated by public benchmark datasets, which enable easy and fair comparison of methods. Currently the established evaluation protocol is to compare the generated summary with respect to a set of reference summaries provided by the dataset. In this paper, we will provide in-depth assessment of this pipeline using two popular benchmark datasets. Surprisingly, we observe that randomly generated summaries achieve comparable or better performance to the state-of-the-art. In some cases, the random summaries outperform even the human generated summaries in leave-one-out experiments. Moreover, it turns out that the video segmentation, which is often considered as a fixed pre-processing method, has the most significant impact on the performance measure. Based on our observations, we propose alternative approaches for assessing the importance scores as well as an intuitive visualization of correlation between the estimated scoring and human annotations.","tags":["datasets and evaluation","vision applications and systems"],"title":"Rethinking the evaluation of video summaries","type":"publication"},{"authors":["Noa Garcia","Chenhui Chu","Mayu Otani","Yuta Nakashima"],"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"3558f13e211da3e3a51f0bde9ee6476a","permalink":"/publication/noa-garcia-chenhui-chu-2019/","publishdate":"2020-08-03T06:16:28.780057Z","relpermalink":"/publication/noa-garcia-chenhui-chu-2019/","section":"publication","summary":"In this work, we address knowledge-based visual question answering in videos. First, we introduce KnowIT VQA, a video dataset with 24,282 human-generated question-answer pairs that combines visual, textual and temporal coherence reasoning together with knowledge-based questions. Second, we propose a video understanding model by combining the visual and textual video information with specific knowledge about the dataset. We find that the incorporation of knowledge produces outstanding improvements for VQA in video. However, the performance on KnowIT VQA still lags well behind human accuracy, indicating its usefulness for studying current video modelling limitations.","tags":null,"title":"Video meets knowledge in visual question answering","type":"publication"},{"authors":["Hajime Nagahara","Dengyu Liu","Toshiki Sonoda","Jinwei Gu"],"categories":null,"content":"","date":1543622400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1543622400,"objectID":"5fa26d5585de611ef761d48fdc427f03","permalink":"/publication/nagahara-2018/","publishdate":"2020-08-03T06:16:28.740679Z","relpermalink":"/publication/nagahara-2018/","section":"publication","summary":"Most conventional digital video cameras face a fundamental trade-off between spatial resolution, temporal resolution and dynamic range (i.e., brightness resolution) because of a limited bandwidth for data transmission. A few recent studies have shown that with non-uniform space-time sampling, such as that implemented with pixel-wise coded exposure, one can go beyond this trade-off and achieve high efficiency for scene capture. However, in these studies, the sampling schemes were pre-defined and independent of the target scene content. In this paper, we propose an adaptive space-time-brightness sampling method to further improve the efficiency of video capture. The proposed method adaptively updates a pixel-wise coded exposure pattern using the information analyzed from previously captured frames. We built a prototype camera that enables adaptive coding of patterns online to show the feasibility of the proposed adaptive coded exposure method. Simulation and experimental results show that the adaptive space-time-brightness sampling scheme achieves more accurate video reconstruction results and high dynamic range with less computational cost, than previous method. To the best of our knowledge, our prototype is the first implementation of an adaptive pixel-wise coded exposure camera.","tags":null,"title":"Space-time-brightness sampling using an adaptive pixel-wise coded exposure","type":"publication"},{"authors":["Ryosuke Kimura","Akihiko Sayo","Fabian Lorenzo Dayrit","Yuta Nakashima","Hiroshi Kawasaki","Ambrosio Blanco","Katsushi Ikeuchi"],"categories":null,"content":"","date":1541030400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1541030400,"objectID":"de3b89a37a98aca3e73b0a00dc847ccb","permalink":"/publication/kimura-2018/","publishdate":"2020-08-03T06:16:28.538404Z","relpermalink":"/publication/kimura-2018/","section":"publication","summary":"Reconstruction of the shape and motion of humans from RGB-D is a challenging problem, receiving much attention in recent years. Recent approaches for full-body reconstruction use a statistic shape model, which is built upon accurate full-body scans of people in skin-tight clothes, to complete invisible parts due to occlusion. Such a statistic model may still be fit to an RGB-D measurement with loose clothes but cannot describe its deformations, such as clothing wrinkles. Observed surfaces may be reconstructed precisely from actual measurements, while we have no cues for unobserved surfaces. For full-body reconstruction with loose clothes, we propose to use lower dimensional embeddings of texture and deformation referred to as eigen-texturing and eigen-deformation, to reproduce views of even unobserved surfaces. Provided a full-body reconstruction from a sequence of partial measurements as 3D meshes, the texture and deformation of each triangle are then embedded using eigen-decomposition. Combined with neural-network-based coefficient regression, our method synthesizes the texture and deformation from arbitrary viewpoints. We evaluate our method using simulated data and visually demonstrate how our method works on real data.","tags":["non-rigid 3d deformation","eigen-deformation","eigen-texture","human motion capture","sensing"],"title":"Representing a partially observed non-rigid 3D human using eigen-texture and eigen-deformation","type":"publication"},{"authors":["Mayu Otani","Atsushi Nishida","Yuta Nakashima","Tomokazu Sato","Naokazu Yokoya"],"categories":null,"content":"","date":1538352000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1538352000,"objectID":"881fbed86a03665e2437fded11452f40","permalink":"/publication/otani-2018/","publishdate":"2020-08-03T06:16:28.589132Z","relpermalink":"/publication/otani-2018/","section":"publication","summary":"Finding important regions is essential for applications, such as content-aware video compression and video retargeting to automatically crop a region in a video for small screens. Since people are one of main subjects when taking a video, some methods for finding important regions use a visual attention model based on face/pedestrian detection to incorporate the knowledge that people are important. However, such methods usually do not distinguish important people from passers-by and bystanders, which results in false positives. In this paper, we propose a deep neural network (DNN)-based method, which classifies a person into important or unimportant, given a video containing multiple people in a single frame and captured with a hand-held camera. Intuitively, important/ unimportant labels are highly correlated given that corresponding people's spatial motions are similar. Based on this assumption, we propose to boost the performance of our important/unimportant classification by using conditional random fields (CRFs) built upon the DNN, which can be trained in an end-to-end manner. Our experimental results show that our method successfully classifies important people and the use of a DNN with CRFs improves the accuracy.","tags":["conditional random field","important people classification","neural network"],"title":"Finding important people in a video using deep neural networks with conditional random fields","type":"publication"},{"authors":["Yusuke Yagi","Keita Takahashi","Toshiaki Fujii","Toshiki Sonoda","Hajime Nagahara"],"categories":null,"content":"","date":1535760000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1535760000,"objectID":"af42d4a10c9258d7da1dc01a31d929b0","permalink":"/publication/yagi-2018/","publishdate":"2020-08-03T06:16:28.77157Z","relpermalink":"/publication/yagi-2018/","section":"publication","summary":"A light field, which is often understood as a set of dense multi-view images, has been utilized in various 2D/3D applications. Efficient light field acquisition using a coded aperture camera is the target problem considered in this paper. Specifically, the entire light field, which consists of many images, should be reconstructed from only a few images that are captured through different aperture patterns. In previous work, this problem has often been discussed from the context of compressed sensing (CS), where sparse representations on a pre-trained dictionary or basis are explored to reconstruct the light field. In contrast, we formulated this problem from the perspective of principal component analysis (PCA) and non-negative matrix factorization (NMF), where only a small number of basis vectors are selected in advance based on the analysis of the training dataset. From this formulation, we derived optimal non-negative aperture patterns and a straight-forward reconstruction algorithm. Even though our method is based on conventional techniques, it has proven to be more accurate and much faster than a state-of-the-art CS-based method.","tags":["coded aperture","light field","nonnegative matrix factorization","principal component analysis"],"title":"Designing coded aperture camera based on PCA and NMF for light field acquisition","type":"publication"},{"authors":["Keigo Hirose","Shuichiro Fukushima","Taichi Furukawa","Hirohiko Niioka","Mamoru Hashimoto"],"categories":null,"content":"","date":1535760000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1535760000,"objectID":"61107bc95ba5fa00cb4be68ab8ed0aca","permalink":"/publication/hirose-2018/","publishdate":"2020-08-03T06:16:28.481707Z","relpermalink":"/publication/hirose-2018/","section":"publication","summary":"","tags":null,"title":"Invited Article: Label-free nerve imaging with a coherent anti-Stokes Raman scattering rigid endoscope using two optical fibers for laser delivery","type":"publication"},{"authors":["Takahiro Tanaka","Norihiko Kawai","Yuta Nakashima","Tomokazu Sato","Naokazu Yokoya"],"categories":null,"content":"","date":1533081600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1533081600,"objectID":"cf3b76149aaa3277d6ddfd2c1fd95ac0","permalink":"/publication/tanaka-2018/","publishdate":"2020-08-03T06:16:28.829701Z","relpermalink":"/publication/tanaka-2018/","section":"publication","summary":"Image completion is a technique to fill missing regions in a damaged or redacted image. A patch-based approach is one of major approaches, which solves an optimization problem that involves pixel values in missing regions and similar image patch search. One major problem of this approach is that it sometimes duplicates implausible texture in the image or overly smooths down a missing region when the algorithm cannot find better patches. As a practical remedy, the user may provide an interaction to identify such regions and re-apply image completion iteratively until she/he gets a desirable result. In this work, inspired by this idea, we propose a framework of human-in-the-loop style image completion with automatic failure detection using a deep neural network instead of human interaction. Our neural network takes small patches extracted from multiple feature maps obtained from the completion process as input for the automated interaction process, which is iterated several times. We experimentally show that our neural network outperforms a conventional linear support vector machine. Our subjective evaluation demonstrates that our method drastically improves the visual quality of resulting images compared to non-iterative application.","tags":["convolutional neural network","failure detection","image completion","image inpainting"],"title":"Iterative applications of image completion with CNN-based failure detection","type":"publication"},{"authors":["Antonio Tejero-De-Pablos","Yuta Nakashima","Tomokazu Sato","Naokazu Yokoya","Marko Linna","Esa Rahtu"],"categories":null,"content":"","date":1533081600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1533081600,"objectID":"1642c3e2d34c6fd7fe9b59a1a119b781","permalink":"/publication/tejero-de-pablos-2018/","publishdate":"2020-08-03T06:16:28.625876Z","relpermalink":"/publication/tejero-de-pablos-2018/","section":"publication","summary":"Automatically generating a summary of a sports video poses the challenge of detecting interesting moments, or highlights, of a game. Traditional sports video summarization methods leverage editing conventions of broadcast sports video that facilitate the extraction of high-level semantics. However, user-generated videos are not edited and, thus, traditional methods are not suitable to generate a summary. In order to solve this problem, this paper proposes a novel video summarization method that uses players' actions as a cue to determine the highlights of the original video. A deep neural-network-based approach is used to extract two types of action-related features and to classify video segments into interesting or uninteresting parts. The proposed method can be applied to any sports in which games consist of a succession of actions. Especially, this paper considers the case of Kendo (Japanese fencing) as an example of a sport to evaluate the proposed method. The method is trained using Kendo videos with ground truth labels that indicate the video highlights. The labels are provided by annotators possessing a different experience with respect to Kendo to demonstrate how the proposed method adapts to different needs. The performance of the proposed method is compared with several combinations of different features, and the results show that it outperforms previous summarization methods.","tags":["3d convolutional neural networks","cameras","feature extraction","games","hidden markov models","japanese fencing","kendo videos","semantics","sports video summarization","three-dimensional displays","action recognition","action-related features","deep action recognition features","deep learning","deep neural-network-based approach","feature extraction","high-level semantics","image segmentation","interesting parts","long short-term memory","neural nets","player action","sport","uninteresting parts","user-generated sport video summarization method","user-generated video","video highlights","video segments","video signal processing"],"title":"Summarization of user-generated sports video by using deep action recognition features","type":"publication"},{"authors":null,"categories":null,"content":"   Takao OnoyeExecutive Vice President of ResearchOsaka University   With the dawn of the \u0026ldquo;big data\u0026rdquo; era, a massive amount of data is being generated every day, be it from our day-to-day lives, or from cutting-edge scientific fields. What sort of analysis is required to extract beneficial information from such enormous data sources? Furthermore, how can this be done in a sufficiently timely manner? These questions are critical to the future development of many scientific disciplines, and our society as a whole. In pursuing a better world for future generations, we believe the key is whether or not we can create, and subsequently utilize, new intellectual value from this data. We at Osaka University have established the Institute for Datability Science (IDS), with the chief aim of utilizing the wealth of data we have at our disposal. One core goal at IDS is the pursuit of a new scientific paradigm through \u0026ldquo;Datability.\u0026rdquo; This refers to the sustainable and responsible use of large data resources. In pursuit of this goal, we shall pioneer new academic disciplines in the domains of Life Science, Biomedical Science, Science and Engineering, and the Humanities, through sophisticated technologies of information science such as artificial intelligence, with the final goal of creating new social and economic value through interdisciplinary research projects. At IDS, we seek to build a highly productive working environment of co-creation and harmony, cultivating the next generation of scientists and engineers with \u0026ldquo;datability\u0026rdquo;.\nTakao Onoye Executive Vice President of Research Osaka University\n","date":1530144000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1530144000,"objectID":"cb9066bf79a384efbd323de5831e6c6c","permalink":"/message/","publishdate":"2018-06-28T00:00:00Z","relpermalink":"/message/","section":"","summary":"Takao OnoyeExecutive Vice President of ResearchOsaka University   With the dawn of the \u0026ldquo;big data\u0026rdquo; era, a massive amount of data is being generated every day, be it from our day-to-day lives, or from cutting-edge scientific fields.","tags":null,"title":"Message from Our Director","type":"page"},{"authors":null,"categories":null,"content":"   ","date":1530144000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1530144000,"objectID":"6425ff024d90bbffa37d23fc4bf403ee","permalink":"/organization/","publishdate":"2018-06-28T00:00:00Z","relpermalink":"/organization/","section":"","summary":"   ","tags":null,"title":"Organization","type":"page"},{"authors":["Chenhui Chu","Mayu Otani","Yuta Nakashima"],"categories":null,"content":"","date":1527811200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1527811200,"objectID":"2f7bdd0325533f0191d56b3f3ec3f290","permalink":"/publication/chu-2018-a/","publishdate":"2020-08-03T06:16:28.502053Z","relpermalink":"/publication/chu-2018-a/","section":"publication","summary":"A paraphrase is a restatement of the meaning of a text in other words. Paraphrases have been studied to enhance the performance of many natural language processing tasks. In this paper, we propose a novel task iParaphrasing to extract visually grounded paraphrases (VGPs), which are different phrasal expressions describing the same visual concept in an image. These extracted VGPs have the potential to improve language and image multimodal tasks such as visual question answering and image captioning. How to model the similarity between VGPs is the key of iParaphrasing. We apply various existing methods as well as propose a novel neural network-based method with image attention, and report the results of the first attempt toward iParaphrasing.","tags":["vgp"],"title":"iParaphrasing: Extracting visually grounded paraphrases via an image","type":"publication"},{"authors":["Yusuke Yagi","Keita Takahashi","Toshiaki Fujii","Toshiki Sonoda","Hajime Nagahara"],"categories":null,"content":"","date":1517443200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1517443200,"objectID":"91e70e33833143426d255f83bef1c9d0","permalink":"/publication/yagi-2018-a/","publishdate":"2020-08-03T06:16:28.49684Z","relpermalink":"/publication/yagi-2018-a/","section":"publication","summary":"A light field, which is often understood as a set of dense multi-view images, has been utilized in various 2D/3D applications. Efficient light field acquisition using a coded aperture camera is the target problem considered in this paper. Specifically, the entire light field, which consists of many images, should be reconstructed from only a few images that are captured through different aperture patterns. In previous work, this problem has often been discussed from the context of compressed sensing (CS). In contrast, we formulated this problem from the perspective of principal component analysis (PCA) to derive optimal non-negative aperture patterns and a straight-forward reconstruction algorithm. Even though it is based on a conventional technique, our method has proven to be more accurate and much faster than a state-of-the-art CS-based method.","tags":["coded aperture","light field","pca"],"title":"PCA-coded aperture for light field photography","type":"publication"},{"authors":["K. Hirose","T. Aoki","T. Furukawa","S. Fukushima","H. Niioka","S. Deguchi","M. Hashimoto"],"categories":null,"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514764800,"objectID":"bc685b9853560dd423216f364c575f86","permalink":"/publication/hirose-2018-a/","publishdate":"2020-08-03T06:16:28.784835Z","relpermalink":"/publication/hirose-2018-a/","section":"publication","summary":"© 2018 Optical Society of America. Label-free visualization of nerves and nervous plexuses will improve the preservation of neurological functions in nerve-sparing robot-assisted surgery. We have developed a coherent anti-Stokes Raman scattering (CARS) rigid endoscope to distinguish nerves from other tissues during surgery. The developed endoscope, which has a tube with a diameter of 12 mm and a length of 270 mm, achieved 0.91% image distortion and 8.6% non-uniformity of CARS intensity in the whole field of view (650 µm diameter). We demonstrated CARS imaging of a rat sciatic nerve and visualization of the fine structure of nerve fibers.","tags":null,"title":"Coherent anti-stokes Raman scattering rigid endoscope toward robot-assisted surgery","type":"publication"},{"authors":["Tomoyuki Kajiwara","Mamoru Komachi"],"categories":null,"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514764800,"objectID":"5600eacd9d21942e8464a0aeb6128bac","permalink":"/publication/kajiwara-2018/","publishdate":"2020-08-03T06:16:28.712397Z","relpermalink":"/publication/kajiwara-2018/","section":"publication","summary":"We introduce the TMU systems for the Complex Word Identification (CWI) Shared Task 2018. TMU systems use random forest classifiers and regressors whose features are the number of characters, the number of words, and the frequency of target words in various corpora. Our simple systems performed best on 5 tracks out of 12 tracks. Our ablation analysis revealed the usefulness of a learner corpus for CWI task.","tags":null,"title":"Complex word identification based on frequency in a learner corpus","type":"publication"},{"authors":["J. Miyake","Y. Kaneshita","S. Asatani","S. Tagawa","H. Niioka","T. Hirano"],"categories":null,"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514764800,"objectID":"9bb4bf451233e760f63084abc428a034","permalink":"/publication/miyake-2018/","publishdate":"2020-08-03T06:16:28.82176Z","relpermalink":"/publication/miyake-2018/","section":"publication","summary":"© 2018 The Author(s) Alleles of human leukocyte antigen (HLA)-A DNAs are classified and expressed graphically by using artificial intelligence “Deep Learning (Stacked autoencoder)”. Nucleotide sequence data corresponding to the length of 822 bp, collected from the Immuno Polymorphism Database, were compressed to 2-dimensional representation and were plotted. Profiles of the two-dimensional plots indicate that the alleles can be classified as clusters are formed. The two-dimensional plot of HLA-A DNAs gives a clear outlook for characterizing the various alleles.","tags":["allele","artificial intelligence","autoencoder","deep learning","hla"],"title":"Graphical classification of DNA sequences of HLA alleles by deep learning","type":"publication"},{"authors":["Michitaka Yoshida","Akihiko Torii","Masatoshi Okutomi","Kenta Endo","Yukinobu Sugiyama","Rin Ichiro Taniguchi","Hajime Nagahara"],"categories":null,"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514764800,"objectID":"dbb615688763a892823ddce2b9a215f7","permalink":"/publication/yoshida-2018/","publishdate":"2020-08-03T06:16:28.594459Z","relpermalink":"/publication/yoshida-2018/","section":"publication","summary":"Compressive video sensing is the process of encoding multiple sub-frames into a single frame with controlled sensor exposures and reconstructing the sub-frames from the single compressed frame. It is known that spatially and temporally random exposures provide the most balanced compression in terms of signal recovery. However, sensors that achieve a fully random exposure on each pixel cannot be easily realized in practice because the circuit of the sensor becomes complicated and incompatible with the sensitivity and resolution. Therefore, it is necessary to design an exposure pattern by considering the constraints enforced by hardware. In this paper, we propose a method of jointly optimizing the exposure patterns of compressive sensing and the reconstruction framework under hardware constraints. By conducting a simulation and actual experiments, we demonstrated that the proposed framework can reconstruct multiple sub-frame images with higher quality.","tags":["compressive sensing","deep neural network","video reconstruction"],"title":"Joint optimization for compressive video sensing and reconstruction under hardware constraints","type":"publication"},{"authors":["Yasutaka Inagaki","Yuto Kobayashi","Keita Takahashi","Toshiaki Fujii","Hajime Nagahara"],"categories":null,"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514764800,"objectID":"c2fdd0745d65803646e3904045fba52c","permalink":"/publication/inagaki-2018/","publishdate":"2020-08-03T06:16:28.599636Z","relpermalink":"/publication/inagaki-2018/","section":"publication","summary":"We propose a learning-based framework for acquiring a light field through a coded aperture camera. Acquiring a light field is a challenging task due to the amount of data. To make the acquisition process efficient, coded aperture cameras were successfully adopted; using these cameras, a light field is computationally reconstructed from several images that are acquirToshiakied with different aperture patterns. However, it is still difficult to reconstruct a high-quality light field from only a few acquired images. To tackle this limitation, we formulated the entire pipeline of light field acquisition from the perspective of an auto-encoder. This auto-encoder was implemented as a stack of fully convolutional layers and was trained end-to-end by using a collection of training samples. We experimentally show that our method can successfully learn good image-acquisition and reconstruction strategies. With our method, light fields consisting of 5 × 5 or 8 × 8 images can be successfully reconstructed only from a few acquired images. Moreover, our method achieved superior performance over several state-of-the-art methods. We also applied our method to a real prototype camera to show that it is capable of capturing a real 3-D scene.","tags":["cnn","coded aperture","light field"],"title":"Learning to capture light fields through a coded aperture camera","type":"publication"},{"authors":["Hiroki Shimanaka","Tomoyuki Kajiwara","Mamoru Komachi"],"categories":null,"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514764800,"objectID":"5ce79750724848d4966cec2e04d429dd","permalink":"/publication/shimanaka-2018-a/","publishdate":"2020-08-03T06:16:28.847659Z","relpermalink":"/publication/shimanaka-2018-a/","section":"publication","summary":"Sentence representations can capture a wide range of information that cannot be captured by local features based on character or word N-grams. This paper examines the usefulness of universal sentence representations for evaluating the quality of machine translation. Al-though it is difficult to train sentence representations using small-scale translation datasets with manual evaluation, sentence representations trained from large-scale data in other tasks can improve the automatic evaluation of machine translation. Experimental results of the WMT-2016 dataset show that the proposed method achieves state-of-the-art performance with sentence representation features only.","tags":null,"title":"Metric for automatic machine translation evaluation based on universal sentence representations","type":"publication"},{"authors":["Hiroki Shimanaka","Tomoyuki Kajiwara","Mamoru Komachi"],"categories":null,"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514764800,"objectID":"561405c2caf19e0d288a9b002e399584","permalink":"/publication/shimanaka-2018/","publishdate":"2020-08-03T06:16:28.522534Z","relpermalink":"/publication/shimanaka-2018/","section":"publication","summary":"We introduce the RUSE metric for the WMT18 metrics shared task. Sentence embeddings can capture global information that cannot be captured by local features based on character or word N-grams. Although training sentence embeddings using small-scale translation datasets with manual evaluation is difficult, sentence embeddings trained from large-scale data in other tasks can improve the automatic evaluation of machine translation. We use a multi-layer perceptron regressor based on three types of sentence embeddings. The experimental results of the WMT16 and WMT17 datasets show that the RUSE metric achieves a state-of-the-art performance in both segment- and system-level metrics tasks with embedding features only.","tags":null,"title":"RUSE: Regressor using sentence embeddings for automatic machine translation evaluation","type":"publication"},{"authors":["T. Yoda","H. Nagahara","R.-I. Taniguchi","K. Kagawa","K. Yasutomi","S. Kawahito"],"categories":null,"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514764800,"objectID":"0b1f8df02aaac06dd19060060da5f6fb","permalink":"/publication/yoda-2018/","publishdate":"2020-08-03T06:16:28.489854Z","relpermalink":"/publication/yoda-2018/","section":"publication","summary":"© 2018 by the authors. Licensee MDPI, Basel, Switzerland. The photometric stereo method enables estimation of surface normals from images that have been captured using different but known lighting directions. The classical photometric stereo method requires at least three images to determine the normals in a given scene. However, this method cannot be applied to dynamic scenes because it is assumed that the scene remains static while the required images are captured. In this work, we present a dynamic photometric stereo method for estimation of the surface normals in a dynamic scene. We use a multi-tap complementary metal-oxide-semiconductor (CMOS) image sensor to capture the input images required for the proposed photometric stereo method. This image sensor can divide the electrons from the photodiode from a single pixel into the different taps of the exposures and can thus capture multiple images under different lighting conditions with almost identical timing. We implemented a camera lighting system and created a software application to enable estimation of the normal map in real time. We also evaluated the accuracy of the estimated surface normals and demonstrated that our proposed method can estimate the surface normals of dynamic scenes.","tags":["3d surface recovery","computational photography","photometric stereo","vision sensor"],"title":"The dynamic photometric stereo method using a multi-tap CMOS image sensor","type":"publication"},{"authors":["Chenhui Chu","Mayu Otani","Yuta Nakashima"],"categories":null,"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514764800,"objectID":"e228d482b106dec9692543ceb0b63358","permalink":"/publication/chu-2018/","publishdate":"2020-08-03T06:16:28.717484Z","relpermalink":"/publication/chu-2018/","section":"publication","summary":"","tags":["vgp"],"title":"Visually grounded paraphrase extraction","type":"publication"},{"authors":["Chao Ma","Ngo Thanh Trung","Hideaki Uchiyama","Hajime Nagahara","Atsushi Shimada","Rin Ichiro Taniguchi"],"categories":null,"content":"","date":1512086400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1512086400,"objectID":"01759ab7ebadcd74736c32cc61d62a84","permalink":"/publication/ma-2017-a/","publishdate":"2020-08-03T06:16:28.835393Z","relpermalink":"/publication/ma-2017-a/","section":"publication","summary":"A thermal camera captures the temperature distribution of a scene as a thermal image. In thermal images, facial appearances of different people under different lighting conditions are similar. This is because facial temperature distribution is generally constant and not affected by lighting condition. This similarity in face appearances is advantageous for face detection. To detect faces in thermal images, cascade classifiers with Haar-like features are generally used. However, there are few studies exploring the local features for face detection in thermal images. In this paper, we introduce two approaches relying on local features for face detection in thermal images. First, we create new feature types by extending Multi-Block LBP. We consider a margin around the reference and the generally constant distribution of facial temperature. In this way, we make the features more robust to image noise and more effective for face detection in thermal images. Second, we propose an AdaBoost-based training method to get cascade classifiers with multiple types of local features. These feature types have different advantages. In this way we enhance the description power of local features. We did a hold-out validation experiment and a field experiment. In the hold-out validation experiment, we captured a dataset from 20 participants, comprising 14 males and 6 females. For each participant, we captured 420 images with 10 variations in camera distance, 21 poses, and 2 appearances (participant with/without glasses). We compared the performance of cascade classifiers trained by different sets of the features. The experiment results showed that the proposed approaches effectively improve the performance of face detection in thermal images. In the field experiment, we compared the face detection performance in realistic scenes using thermal and RGB images, and gave discussion based on the results.","tags":["adaboost","face detection","haar-like","histogram of oriented gradient","local binary pattern","local ternary pattern","mixed features","thermal image"],"title":"Adapting local features for face detection in thermal image","type":"publication"},{"authors":["Norihiko Kawai","Tomokazu Sato","Yuta Nakashima","Naokazu Yokoya"],"categories":null,"content":"","date":1506816000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1506816000,"objectID":"c9e0f76900224edf5949fa96333e6555","permalink":"/publication/kawai-2017-a/","publishdate":"2020-08-03T06:16:28.7665Z","relpermalink":"/publication/kawai-2017-a/","section":"publication","summary":"Augmented reality (AR) marker hiding is a technique to visually remove AR markers in a real-time video stream. A conventional approach transforms a background image with a homography matrix calculated on the basis of a camera pose and overlays the transformed image on an AR marker region in a real-time frame, assuming that the AR marker is on a planar surface. However, this approach may cause discontinuities in textures around the boundary between the marker and its surrounding area when the planar surface assumption is not satisfied. This paper proposes a method for AR marker hiding without discontinuities around texture boundaries even under nonplanar background geometry without measuring it. For doing this, our method estimates the dense motion in the marker's background by analyzing the motion of sparse feature points around it, together with a smooth motion assumption, and deforms the background image according to it. Our experiments demonstrate the effectiveness of the proposed method in various environments with different background geometries and textures.","tags":["marker hiding","diminished reality","texture deformation"],"title":"Augmented reality marker hiding with texture deformation","type":"publication"},{"authors":["Tsubasa Minematsu","Hideaki Uchiyama","Atsushi Shimada","Hajime Nagahara","Rin ichiro Taniguchi"],"categories":null,"content":"","date":1504224000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1504224000,"objectID":"2ddb545043d27158f55c960f4cb4d513","permalink":"/publication/minematsu-2017/","publishdate":"2020-08-03T06:16:28.679129Z","relpermalink":"/publication/minematsu-2017/","section":"publication","summary":"We propose a framework for adaptively registering background models with an image for background subtraction with moving cameras. Existing methods search for a background model using a fixed window size, to suppress the number of false positives when detecting the foreground. However, these approaches result in many false negatives because they may use inappropriate window sizes. The appropriate size depends on various factors of the target scenes. To suppress false detections, we propose adaptively controlling the method parameters, which are typically determined heuristically. More specifically, the search window size for background registration and the foreground detection threshold are automatically determined using the re-projection error computed by the homography based camera motion estimate. Our method is based on the fact that the error at a pixel is low if it belongs to background and high if it does not. We quantitatively confirmed that the proposed framework improved the background subtraction accuracy when applied to images from moving cameras in various public datasets.","tags":["background subtraction","moving camera","moving object detection","re-projection error"],"title":"Adaptive background model registration for moving cameras","type":"publication"},{"authors":["Thiwat Rongsirigul","Yuta Nakashima","Tomokazu Sato","Naokazu Yokoya"],"categories":null,"content":"","date":1501545600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1501545600,"objectID":"e36e8ef5ff5f68d95e67a9f8dd91f791","permalink":"/publication/rongsirigul-2017/","publishdate":"2020-08-03T06:16:28.887153Z","relpermalink":"/publication/rongsirigul-2017/","section":"publication","summary":"The proliferation of off-the-shelf head-mounted displays (HMDs) let end-users enjoy virtual reality applications, some of which render a real-world scene using a novel view synthesis (NVS) technique. View-dependent texture mapping (VDTM) has been studied for NVS due to its photo-realistic quality. The VDTM technique renders a novel view by adaptively selecting textures from the most appropriate images. However, this process is computationally expensive because VDTM scans every captured image. For stereoscopic HMDs, the situation is much worse because we need to render novel views once for each eye, almost doubling the cost. This paper proposes light-weight VDTM tailored for an HMD. In order to reduce the computational cost in VDTM, our method leverages the overlapping fields of view between a stereoscopic pair of HMD images and pruning the images to be scanned. We show that the proposed method drastically accelerates the VDTM process without spoiling the image quality through a user study.","tags":["head-mounted displays","image-based rendering","novel view synthesis"],"title":"Novel view synthesis with light-weight view-dependent texture mapping for a stereoscopic HMD","type":"publication"},{"authors":["Mayu Otani","Yuta Nakashima","Tomokazu Sato","Naokazu Yokoya"],"categories":null,"content":"","date":1493596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1493596800,"objectID":"0e139c9d63573db60267d70fcfffa82e","permalink":"/publication/otani-2017-c/","publishdate":"2020-08-03T06:16:28.657282Z","relpermalink":"/publication/otani-2017-c/","section":"publication","summary":"Authoring video blogs requires a video editing process, which is cumbersome for ordinary users. Video summarization can automate this process by extracting important segments from original videos. Because bloggers typically have certain stories for their blog posts, video summaries of a blog post should take the author's intentions into account. However, most prior works address video summarization by mining patterns from the original videos without considering the blog author's intentions. To generate a video summary that reflects the blog author's intention, we focus on supporting texts in video blog posts and present a text-based method, in which the supporting text serves as a prior to the video summary. Given video and text that describe scenes of interest, our method segments videos and assigns to each video segment its priority in the summary based on its relevance to the input text. Our method then selects a subset of segments with content that is similar to the input text. Accordingly, our method produces different video summaries from the same set of videos, depending on the input text. We evaluated summaries generated from both blog viewers' and authors' perspectives in a user study. Experimental results demonstrate the advantages to the proposed text-based method for video blog authoring.","tags":["text-based video summarization","user study","video skimming"],"title":"Video summarization using textual descriptions for authoring video blogs","type":"publication"},{"authors":["Makoto Ohsaki","Hajime Nagahara","Tetsuo Ikeda","Rin-ichiro Taniguchi"],"categories":null,"content":"","date":1488326400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1488326400,"objectID":"e3d763a228c96744bba0e7868de5e675","permalink":"/publication/ohsaki-2017/","publishdate":"2020-08-03T06:16:28.707246Z","relpermalink":"/publication/ohsaki-2017/","section":"publication","summary":"© 2017 SPIE. Hyperspectral imaging is used in various fields because it can obtain much more information than imaging by conventional RGB cameras. Hyperspectral imaging systems using active illumination, prisms, gratings, or narrowband filters have been proposed. Active illumination systems can obtain two-dimensional (2D) spectral images rapidly, and the device can be low-cost and small because of the use of LEDs. However, flicker can occur when different colors of LEDs are switched. The other methods do not have the flicker problem because they use passive imaging. However, these systems take a long time to acquire the 2D spectral images, or they tend to be high-cost or large. In our research, we propose a flickerless active LED illumination system for hyperspectral imaging. This system acquires images while switching the illumination. The switching illumination consists of many narrowband LEDs that have different spectrums. The spectral images of each LED are reconstructed from the acquired images. The switching illumination is designed to reduce the flicker based on human visual characteristics. We reduce the color changes of the switching illumination while maintaining its spectral differences. In the experiment, we obtain the optimal design of a flickerless illumination system for measuring oxygen saturation. To show the feasibility of our system, we clearly show the difference in saturation using the spectral images obtained by a prototype designed using the proposed method.","tags":null,"title":"Hyperspectral imaging using flickerless active LED illumination","type":"publication"},{"authors":["H. Niioka","S. Asatani","A. Yoshimura","H. Ohigashi","S. Tagawa","J. Miyake"],"categories":null,"content":"","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"4befa4f8d81dd5728b54ec9909e49c33","permalink":"/publication/niioka-2017/","publishdate":"2020-08-03T06:16:28.533317Z","relpermalink":"/publication/niioka-2017/","section":"publication","summary":"© 2017 The Author(s) In the field of regenerative medicine, tremendous numbers of cells are necessary for tissue/organ regeneration. Today automatic cell-culturing system has been developed. The next step is constructing a non-invasive method to monitor the conditions of cells automatically. As an image analysis method, convolutional neural network (CNN), one of the deep learning method, is approaching human recognition level. We constructed and applied the CNN algorithm for automatic cellular differentiation recognition of myogenic C2C12 cell line. Phase-contrast images of cultured C2C12 are prepared as input dataset. In differentiation process from myoblasts to myotubes, cellular morphology changes from round shape to elongated tubular shape due to fusion of the cells. CNN abstract the features of the shape of the cells and classify the cells depending on the culturing days from when differentiation is induced. Changes in cellular shape depending on the number of days of culture (Day 0, Day 3, Day 6) are classified with 91.3% accuracy. Image analysis with CNN has a potential to realize regenerative medicine industry.","tags":["automatic target recognition","cell differentiation","convolutional neural network","deep learning","image analysis","phase contrast microscopy"],"title":"Classification of C2C12 cells at differentiation by convolutional neural network of deep learning using phase contrast images","type":"publication"},{"authors":["Mayu Otani","Yuta Nakashima","Esa Rahtu","Janne Heikkilä"],"categories":null,"content":"","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"015caa235bf146db117a2f5251abdae5","permalink":"/publication/otani-2017/","publishdate":"2020-08-03T06:16:28.508902Z","relpermalink":"/publication/otani-2017/","section":"publication","summary":"","tags":null,"title":"Fine-grained video retrieval for multi-clip video","type":"publication"},{"authors":["Fabian Lorenzo Dayrit","Yuta Nakashima","Tomokazu Sato","Naokazu Yokoya"],"categories":null,"content":"","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"f5ccc1c1d41f0119ada94ece7873b5e1","permalink":"/publication/dayrit-2017-a/","publishdate":"2020-08-03T06:16:28.841626Z","relpermalink":"/publication/dayrit-2017-a/","section":"publication","summary":"Standard video does not capture the 3D aspect of human motion, which is important for comprehension of motion that may be ambiguous. In this paper, we apply augmented reality (AR) techniques to give viewers insight into 3D motion by allowing them to manipulate the viewpoint of a motion sequence of a human actor using a handheld mobile device. The motion sequence is captured using a single RGB-D sensor, which is easier for a general user, but presents the unique challenge of synthesizing novel views using images captured from a single viewpoint. To address this challenge, our proposed system reconstructs a 3D model of the actor, then uses a combination of the actor's pose and viewpoint similarity to find appropriate images to texture it. The system then renders the 3D model on the mobile device using visual SLAM to create a map in order to use it to estimate the mobile device's camera pose relative to the original capturing environment. We call this novel view of a moving human actor a reenactment, and evaluate its usefulness and quality with an experiment and a survey.","tags":["augmented reality","mobile","novel view synthesis","reenactment"],"title":"Increasing pose comprehension through augmented reality reenactment","type":"publication"},{"authors":["R. Roberto","H. Uchiyama","J.P. Lima","H. Nagahara","R.-I. Taniguchi","V. Teichrieb"],"categories":null,"content":"","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"118054479bb8a59d17ed659af719af25","permalink":"/publication/roberto-2017/","publishdate":"2020-08-03T06:16:28.527801Z","relpermalink":"/publication/roberto-2017/","section":"publication","summary":"© 2017 MVA Organization All Rights Reserved. This paper presents an incremental structural modeling approach that improves the precision and stability of existing batch based methods for sparse and noisy point clouds from visual SLAM. The main idea is to use the generating process of point clouds on SLAM effectively. First, a batch based method is applied to point clouds that are incrementally generated from SLAM. Then, the temporal history of reconstructed geometric primitives is statistically merged to suppress incorrect reconstruction. The evaluation shows that both precision and stability are improved compared to a batch based method and the proposed method is suitable for real-time structural modeling.","tags":null,"title":"Incremental structural modeling on sparse visual SLAM","type":"publication"},{"authors":["C. Ma","N.T. Trung","H. Uchiyama","H. Nagahara","A. Shimada","R.-I. Taniguchi"],"categories":null,"content":"","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"52c0eab6e347fe9da4e95b0020555c17","permalink":"/publication/ma-2017/","publishdate":"2020-08-03T06:16:28.800689Z","relpermalink":"/publication/ma-2017/","section":"publication","summary":"© 2017 SPIE. An infrared (IR) camera captures the temperature distribution of an object as an IR image. Because facial temperature is almost constant, an IR camera has the potential to be used in detecting facial regions in IR images. However, in detecting faces, a simple temperature thresholding does not always work reliably. The standard face detection algorithm used is AdaBoost with local features, such as Haar-like, MB-LBP, and HOG features in the visible images. However, there are few studies using these local features in IR image analysis. In this paper, we propose an AdaBoost-based training method to mix these local features for face detection in thermal images. In an experiment, we captured a dataset from 20 participants, comprising 14 males and 6 females, with 10 variations in camera distance, 21 poses, and participants with and without glasses. Using leave-one-out cross-validation, we show that the proposed mixed features have an advantage over all the regular local features.","tags":["face detection","haar-like","histogram of oriented gradient","local-binary pattern","mixed features","thermal image"],"title":"Mixed features for face detection in thermal image","type":"publication"},{"authors":["Yuta Nakashima","Fumio Okura","Norihiko Kawai","Hiroshi Kawasaki","Ambrosio Blanco","Katsushi Ikeuchi"],"categories":null,"content":"","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"d7f07c3f836bc7b69bc7816ad030bfb9","permalink":"/publication/nakashima-2017/","publishdate":"2020-08-03T06:16:28.580934Z","relpermalink":"/publication/nakashima-2017/","section":"publication","summary":"Realtime novel view synthesis, which generates a novel view of a real object or scene in realtime, enjoys a wide range of applications including augmented reality, telepresence, and immersive telecommunication. Image-based rendering (IBR) with rough geometry can be done using only an off-the-shelf camera and thus can be used by many users. However, IBR from images in the wild (e.g., lighting condition changes or the scene contains objects with specular surfaces) has been a tough problem due to color discontinuity; IBR with rough geometry picks up appropriate images for a given viewpoint, but the image used for a rendering unit (a face or pixel) switches when the viewpoint moves, which may cause noticeable changes in color. We use the eigen-texture technique, which represents images for a certain face using a point in the eigenspace. We propose to regress a new point in this space, which moves smoothly, given a viewpoint so that we can generate an image whose color smoothly changes according to the point. Our regressor is based on a neural network with a single hidden layer and hyperbolic tangent nonlinearity. We demonstrate the advantages of our IBR approach using our own datasets as well as publicly available datasets for comparison.","tags":null,"title":"Realtime novel view synthesis with eigen-texture regression","type":"publication"},{"authors":["Fabian Lorenzo Dayrit","Ryosuke Kimura","Yuta Nakashima","Ambrosio Blanco","Hiroshi Kawasaki","Katsushi Ikeuchi","Tomokazu Sato","Naokazu Yokoya"],"categories":null,"content":"","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"4805510c08eeaab72769f56111903253","permalink":"/publication/dayrit-2017/","publishdate":"2020-08-03T06:16:28.795266Z","relpermalink":"/publication/dayrit-2017/","section":"publication","summary":"We propose ReMagicMirror, a system to help people learn actions (e.g., martial arts, dances). We first capture the motions of a teacher performing the action to learn, using two RGB-D cameras. Next, we fit a parametric human body model to the depth data and texture it using the color data, reconstructing the teacher's motion and appearance. The learner is then shown the ReMagicMirror system, which acts as a mirror. We overlay the teacher's reconstructed body on top of this mirror in an augmented reality fashion. The learner is able to intuitively manipulate the reconstruction's viewpoint by simply rotating her body, allowing for easy comparisons between the learner and the teacher. We perform a user study to evaluate our system's ease of use, effectiveness, quality, and appeal.","tags":["3d human reconstruction","human reenactment","rgb-d sensors","sensing"],"title":"ReMagicMirror: Action learning using human reenactment with the mirror metaphor","type":"publication"},{"authors":["Mayu Otani","Yuta Nakashima","Esa Rahtu","Janne Heikkilä","Naokazu Yokoya"],"categories":null,"content":"","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"962f0be04a3edce63525516a60ee795e","permalink":"/publication/otani-2017-a/","publishdate":"2020-08-03T06:16:28.691449Z","relpermalink":"/publication/otani-2017-a/","section":"publication","summary":"","tags":null,"title":"Unsupervised Video Summarization using Deep Video Features","type":"publication"},{"authors":["Mayu Otani","Yuta Nakashima","Esa Rahtu","Janne Heikkilä"],"categories":null,"content":"","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"623fb77efe2570b5ed40fdce7f8801da","permalink":"/publication/otani-2017-b/","publishdate":"2020-08-03T06:16:28.619909Z","relpermalink":"/publication/otani-2017-b/","section":"publication","summary":"","tags":null,"title":"Video question answering to find a desired video eegment","type":"publication"},{"authors":["Hajime Nagahara","Toshiki Sonoda","Kenta Endo","Yukinobu Sugiyama","Rin Ichiro Taniguchi"],"categories":null,"content":"","date":1464739200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1464739200,"objectID":"ace610faa0f0abef5cfd9a30be05b28f","permalink":"/publication/nagahara-2016/","publishdate":"2020-08-03T06:16:28.674212Z","relpermalink":"/publication/nagahara-2016/","section":"publication","summary":"Several recent studies in compressive video sensing have realized scene capture beyond the fundamental trade-off limit between spatial resolution and temporal resolution using random space-time sampling. However, most of these studies showed results for higher frame rate video that were produced by simulation experiments or using an optically simulated random sampling camera, because there are currently no commercially available image sensors with random exposure or sampling capabilities. We fabricated a prototype complementary metal oxide semiconductor (CMOS) image sensor with quasi pixel-wise exposure timing that can realize nonuniform space-time sampling. The prototype sensor can reset exposures independently by columns and fix these amount of exposure by rows for each 8×8 pixel block. This CMOS sensor is not fully controllable via the pixels, and has line-dependent controls, but it offers flexibility when compared with regular CMOS or charge-coupled device sensors with global or rolling shutters. We propose a method to realize pseudo-random sampling for high-speed video acquisition that uses the flexibility of the CMOS sensor. We reconstruct the high-speed video sequence from the images produced by pseudo-random sampling using an over-complete dictionary. The proposed method also removes the rolling shutter effect from the reconstructed video.","tags":null,"title":"High-speed imaging using CMOS image sensor with quasi pixel-wise exposure","type":"publication"},{"authors":["Takuya Yoda","Hajime Nagahara","Rin Ichiro Taniguchi","Keiichiro Kagawa","Keita Yasutomi","Shoji Kawahito"],"categories":null,"content":"","date":1451606400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1451606400,"objectID":"1494c683c3e7bdf18da1089f7325131c","permalink":"/publication/yoda-2016/","publishdate":"2020-08-03T06:16:28.761329Z","relpermalink":"/publication/yoda-2016/","section":"publication","summary":"Photometric stereo enables the estimation of surface normals from images that were captured using different known lighting directions. The classical photometric stereo method requires at least three images to determine the normals of a given scene. This method therefore cannot be applied to a dynamic scene, because it is assumed that the scene should remain static while the required images are captured. We present a dynamic photometric stereo method to estimate the surface normals in a dynamic scene. We use a multi-tap complementary metal-oxide-semiconductor (CMOS) image sensor to capture the input images for the photometric stereo method. The image sensor can divide the electrons from the photodiode of a single pixel into different taps of exposures, and can therefore capture multiple images under different lighting conditions with almost the same timing. We implemented a prototype camera that was synchronized with a lighting system, and subsequently realized photometric stereo of a dynamic scene.","tags":null,"title":"Dynamic photometric stereo method using multi-tap CMOS image sensor","type":"publication"}]