<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Zekun Yang | 大阪大学データビリティフロンティア機構</title><link>http://www.ids.osaka-u.ac.jp/test/preview/e17f2badfaa26f05e6d577b35eeb867a434bd466/ja/author/zekun-yang/</link><atom:link href="http://www.ids.osaka-u.ac.jp/test/preview/e17f2badfaa26f05e6d577b35eeb867a434bd466/ja/author/zekun-yang/index.xml" rel="self" type="application/rss+xml"/><description>Zekun Yang</description><generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>ja</language><lastBuildDate>Wed, 01 Jul 2020 10:13:28 +0900</lastBuildDate><image><url>http://www.ids.osaka-u.ac.jp/test/preview/e17f2badfaa26f05e6d577b35eeb867a434bd466/media/large.png</url><title>Zekun Yang</title><link>http://www.ids.osaka-u.ac.jp/test/preview/e17f2badfaa26f05e6d577b35eeb867a434bd466/ja/author/zekun-yang/</link></image><item><title>Knowledge VQA</title><link>http://www.ids.osaka-u.ac.jp/test/preview/e17f2badfaa26f05e6d577b35eeb867a434bd466/ja/project/kiban_b-kvqa/</link><pubDate>Wed, 01 Jul 2020 10:13:28 +0900</pubDate><guid>http://www.ids.osaka-u.ac.jp/test/preview/e17f2badfaa26f05e6d577b35eeb867a434bd466/ja/project/kiban_b-kvqa/</guid><description>&lt;p>Visual question answering (VQA) with knowledge is a task that requires knowledge to answer questions on images/video. This additional requirement of knowledge poses an interesting challenge on top of the classic VQA tasks. Specifically, a system needs to explore external knowledge sources to answer the questions correctly, as well as understanding the visual content.&lt;/p>
&lt;p>We created
&lt;a href="https://knowit-vqa.github.io" target="_blank" rel="noopener">a dedicated dataset for our knowledge VQA task&lt;/a> and made it open to the public so that everyone can enjoy our new task. We have also published several papers on this task.&lt;/p>
&lt;h3 id="publications">Publications&lt;/h3>
&lt;ul>
&lt;li>Noa Garcia, Mayu Otani, Chenhui Chu, and Yuta Nakashima (2019). KnowIT VQA: Answering knowledge-based questions about videos. Proc. AAAI Conference on Artificial Intelligence, Feb. 2020.&lt;/li>
&lt;li>Zekun Yang, Noa Garcia, Chenhui Chu, Mayu Otani, Yuta Nakashima, and Haruo Takemura (2020). BERT representations for video question answering. Proc. IEEE Winter Conference on Applications of Computer Vision.&lt;/li>
&lt;li>Noa Garcia, Chenhui Chu, Mayu Otani, and Yuta Nakashima (2019). Video meets knowledge in visual question answering. MIRU.&lt;/li>
&lt;li>Zekun Yang, Noa Garcia, Chenhui Chu, Mayu Otani, Yuta Nakashima, and Haruo Takemura (2019). Video question answering with BERT. MIRU.&lt;/li>
&lt;/ul></description></item><item><title>BERT representations for video question answering</title><link>http://www.ids.osaka-u.ac.jp/test/preview/e17f2badfaa26f05e6d577b35eeb867a434bd466/ja/publication/yang-2020/</link><pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate><guid>http://www.ids.osaka-u.ac.jp/test/preview/e17f2badfaa26f05e6d577b35eeb867a434bd466/ja/publication/yang-2020/</guid><description/></item></channel></rss>