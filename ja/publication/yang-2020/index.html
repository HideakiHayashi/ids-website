<!doctype html><html lang=ja><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=generator content="Source Themes Academic 4.8.0"><meta name=author content="Web Administrator"><meta name=description content="Visual question answering (VQA) aims at answering questions about the visual content of an image or a video. Currently, most work on VQA is focused on image-based question answering, and less attention has been paid into answering questions about videos. However, VQA in video presents some unique challenges that are worth studying: it not only requires to model a sequence of visual features over time, but often it also needs to reason about associated subtitles. In this work, we propose to use BERT, a sequential modelling technique based on Transformers, to encode the complex semantics from video clips. Our proposed model jointly captures the visual and language information of a video scene by encoding not only the subtitles but also a sequence of visual concepts with a pretrained language-based Transformer. In our experiments, we exhaustively study the performance of our model by taking different input arrangements, showing outstanding improvements when compared against previous work on two well-known video VQA datasets: TVQA and Pororo."><link rel=alternate hreflang=en href=http://www.ids.osaka-u.ac.jp/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/publication/yang-2020/><link rel=alternate hreflang=ja href=http://www.ids.osaka-u.ac.jp/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/ja/publication/yang-2020/><meta name=theme-color content="#2962ff"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.0-1/css/all.min.css integrity="sha256-4w9DunooKSr3MFXHXWyFER38WmPdm361bQS/2KUWZbU=" crossorigin=anonymous><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css integrity="sha256-Vzbj7sDDS/woiFS3uNKo8eIuni59rjyNGtXfstRzStA=" crossorigin=anonymous><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/github.min.css crossorigin=anonymous title=hl-light><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/dracula.min.css crossorigin=anonymous title=hl-dark disabled><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.5.1/leaflet.css integrity="sha256-SHMGCYmST46SoyGgo4YR/9AlK1vf3ff84Aq9yK4hdqM=" crossorigin=anonymous><script src=https://cdnjs.cloudflare.com/ajax/libs/lazysizes/5.1.2/lazysizes.min.js integrity="sha256-Md1qLToewPeKjfAHU1zyPwOutccPAm5tahnaw7Osw0A=" crossorigin=anonymous async></script><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Montserrat:400,700%7CRoboto:400,400italic,700%7CRoboto+Mono&display=swap"><link rel=stylesheet href=/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/css/academic.css><link rel=manifest href=/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/ja/index.webmanifest><link rel=icon type=image/png href=/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_32x32_fill_lanczos_center_2.png><link rel=apple-touch-icon type=image/png href=/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_192x192_fill_lanczos_center_2.png><link rel=canonical href=http://www.ids.osaka-u.ac.jp/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/ja/publication/yang-2020/><meta property="twitter:card" content="summary_large_image"><meta property="og:site_name" content="大阪大学データビリティフロンティア機構"><meta property="og:url" content="http://www.ids.osaka-u.ac.jp/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/ja/publication/yang-2020/"><meta property="og:title" content="BERT representations for video question answering | 大阪大学データビリティフロンティア機構"><meta property="og:description" content="Visual question answering (VQA) aims at answering questions about the visual content of an image or a video. Currently, most work on VQA is focused on image-based question answering, and less attention has been paid into answering questions about videos. However, VQA in video presents some unique challenges that are worth studying: it not only requires to model a sequence of visual features over time, but often it also needs to reason about associated subtitles. In this work, we propose to use BERT, a sequential modelling technique based on Transformers, to encode the complex semantics from video clips. Our proposed model jointly captures the visual and language information of a video scene by encoding not only the subtitles but also a sequence of visual concepts with a pretrained language-based Transformer. In our experiments, we exhaustively study the performance of our model by taking different input arrangements, showing outstanding improvements when compared against previous work on two well-known video VQA datasets: TVQA and Pororo."><meta property="og:image" content="http://www.ids.osaka-u.ac.jp/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/media/large.png"><meta property="twitter:image" content="http://www.ids.osaka-u.ac.jp/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/media/large.png"><meta property="og:locale" content="ja"><meta property="article:published_time" content="2020-08-03T06:16:28+00:00"><meta property="article:modified_time" content="2020-01-01T00:00:00+00:00"><script type=application/ld+json>{"@context":"https://schema.org","@type":"Article","mainEntityOfPage":{"@type":"WebPage","@id":"http://www.ids.osaka-u.ac.jp/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/ja/publication/yang-2020/"},"headline":"BERT representations for video question answering","datePublished":"2020-08-03T06:16:28Z","dateModified":"2020-01-01T00:00:00Z","author":{"@type":"Person","name":"Zekun Yang"},"publisher":{"@type":"Organization","name":"Institute for Datability Science, Osaka University","logo":{"@type":"ImageObject","url":"http://www.ids.osaka-u.ac.jp/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/images/logo_hu43423524dbd337905d6e12d1b20a6666_22772_192x192_fit_lanczos_2.png"}},"description":"Visual question answering (VQA) aims at answering questions about the visual content of an image or a video. Currently, most work on VQA is focused on image-based question answering, and less attention has been paid into answering questions about videos. However, VQA in video presents some unique challenges that are worth studying: it not only requires to model a sequence of visual features over time, but often it also needs to reason about associated subtitles. In this work, we propose to use BERT, a sequential modelling technique based on Transformers, to encode the complex semantics from video clips. Our proposed model jointly captures the visual and language information of a video scene by encoding not only the subtitles but also a sequence of visual concepts with a pretrained language-based Transformer. In our experiments, we exhaustively study the performance of our model by taking different input arrangements, showing outstanding improvements when compared against previous work on two well-known video VQA datasets: TVQA and Pororo."}</script><title>BERT representations for video question answering | 大阪大学データビリティフロンティア機構</title></head><body id=top data-spy=scroll data-offset=70 data-target=#TableOfContents><aside class=search-results id=search><div class=container><section class=search-header><div class="row no-gutters justify-content-between mb-3"><div class=col-6><h1>Search</h1></div><div class="col-6 col-search-close"><a class=js-search href=#><i class="fas fa-times-circle text-muted" aria-hidden=true></i></a></div></div><div id=search-box><input name=q id=search-query placeholder=検索... autocapitalize=off autocomplete=off autocorrect=off spellcheck=false type=search class=form-control></div></section><section class=section-search-results><div id=search-hits></div></section></div></aside><nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id=navbar-main><div class=container><div class="d-none d-lg-inline-flex"><a class=navbar-brand href=/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/ja/><img src=/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/images/logo_hu43423524dbd337905d6e12d1b20a6666_22772_0x70_resize_lanczos_2.png alt=大阪大学データビリティフロンティア機構></a></div><button type=button class=navbar-toggler data-toggle=collapse data-target=#navbar-content aria-controls=navbar aria-expanded=false aria-label=ナビゲーションの切り替え>
<span><i class="fas fa-bars"></i></span></button><div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none"><a class=navbar-brand href=/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/ja/><img src=/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/images/logo_hu43423524dbd337905d6e12d1b20a6666_22772_0x70_resize_lanczos_2.png alt=大阪大学データビリティフロンティア機構></a></div><div class="navbar-collapse main-menu-item collapse justify-content-start" id=navbar-content><ul class="navbar-nav d-md-inline-flex"><li class=nav-item><a class="nav-link active" href=/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/ja/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/><span>トップ</span></a></li><li class=nav-item><a class=nav-link href=/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/ja/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/message><span>ご挨拶</span></a></li><li class=nav-item><a class=nav-link href=/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/ja/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/organization><span>組織</span></a></li><li class=nav-item><a class=nav-link href=/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/ja/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/people><span>スタッフ</span></a></li><li class=nav-item><a class=nav-link href=/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/ja/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/projects><span>プロジェクト</span></a></li><li class=nav-item><a class=nav-link href=/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/ja/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/publication><span>発表文献</span></a></li><li class=nav-item><a class=nav-link href=/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/ja/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/access><span>アクセス</span></a></li></ul></div><ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2"><li class=nav-item><a class="nav-link js-search" href=# aria-label=Search><i class="fas fa-search" aria-hidden=true></i></a></li><li class="nav-item dropdown theme-dropdown"><a href=# class="nav-link js-theme-selector" data-toggle=dropdown aria-haspopup=true><i class="fas fa-palette" aria-hidden=true></i></a><div class=dropdown-menu><a href=# class="dropdown-item js-set-theme-light"><span>Light</span></a>
<a href=# class="dropdown-item js-set-theme-dark"><span>Dark</span></a>
<a href=# class="dropdown-item js-set-theme-auto"><span>Automatic</span></a></div></li><li class="nav-item dropdown i18n-dropdown"><a href=# class="nav-link dropdown-toggle" data-toggle=dropdown aria-haspopup=true><i class="fas fa-globe mr-1" aria-hidden=true></i><span class="d-none d-lg-inline">日本語</span></a><div class=dropdown-menu><div class="dropdown-item dropdown-item-active"><span>日本語</span></div><a class=dropdown-item href=http://www.ids.osaka-u.ac.jp/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/publication/yang-2020/><span>English</span></a></div></li></ul></div></nav><div class=pub><div class="article-container pt-3"><h1>BERT representations for video question answering</h1><div class=article-metadata><div><span>Zekun Yang</span>, <span>Noa Garcia</span>, <span>Chenhui Chu</span>, <span>Mayu Otani</span>, <span>Yuta Nakashima</span>, <span>Haruo Takemura</span></div><span class=article-date>January 2020</span></div><div class="btn-links mb-3"><button type=button class="btn btn-outline-primary my-1 mr-1 js-cite-modal" data-filename=/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/ja/publication/yang-2020/cite.bib>
引用</button>
<a class="btn btn-outline-primary my-1 mr-1" href=https://doi.org/10.1109/WACV45572.2020.9093596 target=_blank rel=noopener>DOI</a></div></div><div class=article-container><h3>概要</h3><p class=pub-abstract>Visual question answering (VQA) aims at answering questions about the visual content of an image or a video. Currently, most work on VQA is focused on image-based question answering, and less attention has been paid into answering questions about videos. However, VQA in video presents some unique challenges that are worth studying: it not only requires to model a sequence of visual features over time, but often it also needs to reason about associated subtitles. In this work, we propose to use BERT, a sequential modelling technique based on Transformers, to encode the complex semantics from video clips. Our proposed model jointly captures the visual and language information of a video scene by encoding not only the subtitles but also a sequence of visual concepts with a pretrained language-based Transformer. In our experiments, we exhaustively study the performance of our model by taking different input arrangements, showing outstanding improvements when compared against previous work on two well-known video VQA datasets: TVQA and Pororo.</p><div class=row><div class=col-md-1></div><div class=col-md-10><div class=row><div class="col-12 col-md-3 pub-row-heading">タイプ</div><div class="col-12 col-md-9"><a href=/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/ja/publication/#1>Conference paper</a></div></div></div><div class=col-md-1></div></div><div class="d-md-none space-below"></div><div class=row><div class=col-md-1></div><div class=col-md-10><div class=row><div class="col-12 col-md-3 pub-row-heading">収録</div><div class="col-12 col-md-9"><em>Proceedings - 2020 IEEE Winter Conference on Applications of Computer Vision, WACV 2020</em></div></div></div><div class=col-md-1></div></div><div class="d-md-none space-below"></div><div class=space-below></div><div class=article-style></div><div class=article-tags><a class="badge badge-light" href=/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/ja/tag/kvqa/>kvqa</a></div><div class=share-box aria-hidden=true><ul class=share><li><a href="https://twitter.com/intent/tweet?url=http://www.ids.osaka-u.ac.jp/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/ja/publication/yang-2020/&text=BERT%20representations%20for%20video%20question%20answering" target=_blank rel=noopener class=share-btn-twitter><i class="fab fa-twitter"></i></a></li><li><a href="https://www.facebook.com/sharer.php?u=http://www.ids.osaka-u.ac.jp/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/ja/publication/yang-2020/&t=BERT%20representations%20for%20video%20question%20answering" target=_blank rel=noopener class=share-btn-facebook><i class="fab fa-facebook"></i></a></li><li><a href="mailto:?subject=BERT%20representations%20for%20video%20question%20answering&body=http://www.ids.osaka-u.ac.jp/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/ja/publication/yang-2020/" target=_blank rel=noopener class=share-btn-email><i class="fas fa-envelope"></i></a></li><li><a href="https://www.linkedin.com/shareArticle?url=http://www.ids.osaka-u.ac.jp/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/ja/publication/yang-2020/&title=BERT%20representations%20for%20video%20question%20answering" target=_blank rel=noopener class=share-btn-linkedin><i class="fab fa-linkedin-in"></i></a></li><li><a href="whatsapp://send?text=BERT%20representations%20for%20video%20question%20answering%20http://www.ids.osaka-u.ac.jp/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/ja/publication/yang-2020/" target=_blank rel=noopener class=share-btn-whatsapp><i class="fab fa-whatsapp"></i></a></li><li><a href="https://service.weibo.com/share/share.php?url=http://www.ids.osaka-u.ac.jp/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/ja/publication/yang-2020/&title=BERT%20representations%20for%20video%20question%20answering" target=_blank rel=noopener class=share-btn-weibo><i class="fab fa-weibo"></i></a></li></ul></div><div class="media author-card content-widget-hr"><img class="avatar mr-3 avatar-circle" src=/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/ja/author/zekun-yang/avatar_hu3630e481e1019c912ace1e7f94f11f60_7301736_270x270_fill_q90_lanczos_center.jpg alt="Zekun Yang"><div class=media-body><h5 class=card-title><a href=/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/ja/author/zekun-yang/>Zekun Yang</a></h5><h6 class=card-subtitle>PhD Student</h6><ul class=network-icon aria-hidden=true></ul></div></div><div class="media author-card content-widget-hr"><img class="avatar mr-3 avatar-circle" src=/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/ja/author/noa-garcia/avatar_hud6b8eaaa656c56448d26d3718c538605_118476_270x270_fill_lanczos_center_2.png alt="Noa Garcia"><div class=media-body><h5 class=card-title><a href=/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/ja/author/noa-garcia/>Noa Garcia</a></h5><h6 class=card-subtitle>Specially-Appointed Researcher/Fellow</h6><p class=card-text>Her research interests lie in computer vision and machine learning applied to visual retrieval and joint models of vision and language for high-level understanding tasks.</p><ul class=network-icon aria-hidden=true><li><a href=/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/ja/#contact><i class="fas fa-envelope"></i></a></li><li><a href=/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/ja><i class="fas fa-phone"></i></a></li><li><a href=http://noagarciad.com/ target=_blank rel=noopener><i class="fas fa-home"></i></a></li></ul></div></div><div class="media author-card content-widget-hr"><img class="avatar mr-3 avatar-circle" src=/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/ja/author/chenhui-chu/avatar_hua400fea5d8782bfc1e4686cbd803d9a0_15806357_270x270_fill_lanczos_center_2.png alt="Chenhui Chu"><div class=media-body><h5 class=card-title><a href=/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/ja/author/chenhui-chu/>Chenhui Chu</a></h5><h6 class=card-subtitle>Guest Associate Professor</h6><ul class=network-icon aria-hidden=true></ul></div></div><div class="media author-card content-widget-hr"><img class="avatar mr-3 avatar-circle" src=/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/ja/author/yuta-nakashima/avatar_hu25d333fcded103db2f52f7476cb1420a_78230_270x270_fill_q90_lanczos_center.jpg alt="Yuta Nakashima"><div class=media-body><h5 class=card-title><a href=/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/ja/author/yuta-nakashima/>Yuta Nakashima</a></h5><h6 class=card-subtitle>Associate Professor</h6><p class=card-text>Yuta Nakashima is an associate professor with Institute for Datability Science, Osaka University. His research interests include computer vision, pattern recognition, natural langauge processing, and their applications.</p><ul class=network-icon aria-hidden=true><li><a href=/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/ja/#contact><i class="fas fa-envelope"></i></a></li><li><a href=/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/ja><i class="fas fa-phone"></i></a></li><li><a href=http://n-yuta.jp/ target=_blank rel=noopener><i class="fas fa-home"></i></a></li><li><a href="https://scholar.google.com/citations?user=LNvd0VQAAAAJ" target=_blank rel=noopener><i class="ai ai-google-scholar"></i></a></li><li><a href=https://github.com/USERNAME target=_blank rel=noopener><i class="fab fa-github"></i></a></li></ul></div></div><div class="article-widget content-widget-hr"><h3>関連項目</h3><ul><li><a href=/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/ja/publication/garcia-2020-a/>KnowIT VQA: Answering knowledge-based questions about videos</a></li><li><a href=/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/ja/project/kiban_b-kvqa/>Knowledge VQA</a></li><li><a href=/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/ja/publication/garcia-2020/>ContextNet: representation and exploration for painting classification and retrieval in context</a></li><li><a href=/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/ja/publication/noa-garcia-chenhui-chu-2019/>Video meets knowledge in visual question answering</a></li></ul></div></div></div><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js integrity="sha256-yt2kYMy0w8AbtF89WXb2P1rfjcP/HTHLT7097U8Y5b8=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/instant.page/5.1.0/instantpage.min.js integrity="sha512-1+qUtKoh9XZW7j+6LhRMAyOrgSQKenQ4mluTR+cvxXjP1Z54RxZuzstR/H9kgPXQsVB8IW7DMDFUJpzLjvhGSQ==" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js integrity="sha256-eOgo0OtLL4cdq7RdwRUiGKLX9XsIJ7nGhWEKbohmVAQ=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/languages/r.min.js></script><script src=https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.5.1/leaflet.js integrity="sha256-EErZamuLefUnbMBQbsEqu1USa+btR2oIlCpBJbyD4/g=" crossorigin=anonymous></script><script>const code_highlighting=true;</script><script>const isSiteThemeDark=false;</script><script>const search_config={"indexURI":"/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/ja/index.json","minLength":1,"threshold":0.3};const i18n={"no_results":"結果が見つかりませんでした","placeholder":"検索...","results":"results found"};const content_type={'post':"投稿",'project':"プロジェクト",'publication':"発表文献",'talk':"登壇",'slides':"Slides"};</script><script id=search-hit-fuse-template type=text/x-template>
      <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
      </div>
    </script><script src=https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin=anonymous></script><script src=/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/js/academic.min.66c553246b0f279a03be6e5597f72b52.js></script><div class=container><footer class=site-footer><div class="d-none d-lg-inline-flex"><a class=navbar-brand href=/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/ja/><img src=/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/images/logo_hu43423524dbd337905d6e12d1b20a6666_22772_0x70_resize_lanczos_2.png alt=大阪大学データビリティフロンティア機構></a></div><p class=powered-by style=font-size:70%;text-align:left;margin-left:40px>TEL: +81 6 6105 6074<br>FAX: +81 6 6105 6075<br>Techno-alliance bldg. C503, 2-8, Yamadaoka, Suita, Osaka 565-0971</p><p class=powered-by></p><p class=powered-by>Published with
<a href=https://sourcethemes.com/academic/ target=_blank rel=noopener>Academic Website Builder</a>
<span class=float-right aria-hidden=true><a href=# class=back-to-top><span class=button_icon><i class="fas fa-chevron-up fa-2x"></i></span></a></span></p></footer></div><div id=modal class="modal fade" role=dialog><div class=modal-dialog><div class=modal-content><div class=modal-header><h5 class=modal-title>引用</h5><button type=button class=close data-dismiss=modal aria-label=Close>
<span aria-hidden=true>&#215;</span></button></div><div class=modal-body><pre><code class="tex hljs"></code></pre></div><div class=modal-footer><a class="btn btn-outline-primary my-1 js-copy-cite" href=# target=_blank><i class="fas fa-copy"></i>コピー</a>
<a class="btn btn-outline-primary my-1 js-download-cite" href=# target=_blank><i class="fas fa-download"></i>ダウンロード</a><div id=modal-error></div></div></div></div></div></body></html>