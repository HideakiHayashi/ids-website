<!doctype html><html lang=ja><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=generator content="Source Themes Academic 4.8.0"><meta name=author content="Web Administrator"><meta name=description content="Drowsiness is a major factor that hinders learning. To improve learning efficiency, it is important to understand students' physical status such as wakefulness during online coursework. In this study, we have proposed a drowsiness estimation method based on learners' head and facial movements while viewing video lectures. To examine the effectiveness of head and facial movements in drowsiness estimation, we collected learner video data recorded during e-learning and applied a deep learning approach under the following conditions: (a) using only facial movement data, (b) using only head movement data, and (c) using both facial and head movement data.We achieved an average F1-macro score of 0.74 in personalized models for detecting learner drowsiness using both facial and head movement data."><link rel=alternate hreflang=en href=http://www.ids.osaka-u.ac.jp/test/preview/e17f2badfaa26f05e6d577b35eeb867a434bd466/publication/terai-2020/><link rel=alternate hreflang=ja href=http://www.ids.osaka-u.ac.jp/test/preview/e17f2badfaa26f05e6d577b35eeb867a434bd466/ja/publication/terai-2020/><meta name=theme-color content="#2962ff"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.0-1/css/all.min.css integrity="sha256-4w9DunooKSr3MFXHXWyFER38WmPdm361bQS/2KUWZbU=" crossorigin=anonymous><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css integrity="sha256-Vzbj7sDDS/woiFS3uNKo8eIuni59rjyNGtXfstRzStA=" crossorigin=anonymous><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/github.min.css crossorigin=anonymous title=hl-light><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/dracula.min.css crossorigin=anonymous title=hl-dark disabled><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.5.1/leaflet.css integrity="sha256-SHMGCYmST46SoyGgo4YR/9AlK1vf3ff84Aq9yK4hdqM=" crossorigin=anonymous><script src=https://cdnjs.cloudflare.com/ajax/libs/lazysizes/5.1.2/lazysizes.min.js integrity="sha256-Md1qLToewPeKjfAHU1zyPwOutccPAm5tahnaw7Osw0A=" crossorigin=anonymous async></script><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Montserrat:400,700%7CRoboto:400,400italic,700%7CRoboto+Mono&display=swap"><link rel=stylesheet href=/test/preview/e17f2badfaa26f05e6d577b35eeb867a434bd466/css/academic.css><link rel=manifest href=/test/preview/e17f2badfaa26f05e6d577b35eeb867a434bd466/ja/index.webmanifest><link rel=icon type=image/png href=/test/preview/e17f2badfaa26f05e6d577b35eeb867a434bd466/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_32x32_fill_lanczos_center_2.png><link rel=apple-touch-icon type=image/png href=/test/preview/e17f2badfaa26f05e6d577b35eeb867a434bd466/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_192x192_fill_lanczos_center_2.png><link rel=canonical href=http://www.ids.osaka-u.ac.jp/test/preview/e17f2badfaa26f05e6d577b35eeb867a434bd466/ja/publication/terai-2020/><meta property="twitter:card" content="summary_large_image"><meta property="og:site_name" content="大阪大学データビリティフロンティア機構"><meta property="og:url" content="http://www.ids.osaka-u.ac.jp/test/preview/e17f2badfaa26f05e6d577b35eeb867a434bd466/ja/publication/terai-2020/"><meta property="og:title" content="Detecting learner drowsiness based on facial expressions and head movements in online courses | 大阪大学データビリティフロンティア機構"><meta property="og:description" content="Drowsiness is a major factor that hinders learning. To improve learning efficiency, it is important to understand students' physical status such as wakefulness during online coursework. In this study, we have proposed a drowsiness estimation method based on learners' head and facial movements while viewing video lectures. To examine the effectiveness of head and facial movements in drowsiness estimation, we collected learner video data recorded during e-learning and applied a deep learning approach under the following conditions: (a) using only facial movement data, (b) using only head movement data, and (c) using both facial and head movement data.We achieved an average F1-macro score of 0.74 in personalized models for detecting learner drowsiness using both facial and head movement data."><meta property="og:image" content="http://www.ids.osaka-u.ac.jp/test/preview/e17f2badfaa26f05e6d577b35eeb867a434bd466/media/large.png"><meta property="twitter:image" content="http://www.ids.osaka-u.ac.jp/test/preview/e17f2badfaa26f05e6d577b35eeb867a434bd466/media/large.png"><meta property="og:locale" content="ja"><meta property="article:published_time" content="2020-08-03T06:16:28+00:00"><meta property="article:modified_time" content="2020-03-01T00:00:00+00:00"><script type=application/ld+json>{"@context":"https://schema.org","@type":"Article","mainEntityOfPage":{"@type":"WebPage","@id":"http://www.ids.osaka-u.ac.jp/test/preview/e17f2badfaa26f05e6d577b35eeb867a434bd466/ja/publication/terai-2020/"},"headline":"Detecting learner drowsiness based on facial expressions and head movements in online courses","datePublished":"2020-08-03T06:16:28Z","dateModified":"2020-03-01T00:00:00Z","author":{"@type":"Person","name":"Shogo Terai"},"publisher":{"@type":"Organization","name":"Institute for Datability Science, Osaka University","logo":{"@type":"ImageObject","url":"http://www.ids.osaka-u.ac.jp/test/preview/e17f2badfaa26f05e6d577b35eeb867a434bd466/images/logo_hu43423524dbd337905d6e12d1b20a6666_22772_192x192_fit_lanczos_2.png"}},"description":"Drowsiness is a major factor that hinders learning. To improve learning efficiency, it is important to understand students' physical status such as wakefulness during online coursework. In this study, we have proposed a drowsiness estimation method based on learners' head and facial movements while viewing video lectures. To examine the effectiveness of head and facial movements in drowsiness estimation, we collected learner video data recorded during e-learning and applied a deep learning approach under the following conditions: (a) using only facial movement data, (b) using only head movement data, and (c) using both facial and head movement data.We achieved an average F1-macro score of 0.74 in personalized models for detecting learner drowsiness using both facial and head movement data."}</script><title>Detecting learner drowsiness based on facial expressions and head movements in online courses | 大阪大学データビリティフロンティア機構</title></head><body id=top data-spy=scroll data-offset=70 data-target=#TableOfContents><aside class=search-results id=search><div class=container><section class=search-header><div class="row no-gutters justify-content-between mb-3"><div class=col-6><h1>Search</h1></div><div class="col-6 col-search-close"><a class=js-search href=#><i class="fas fa-times-circle text-muted" aria-hidden=true></i></a></div></div><div id=search-box><input name=q id=search-query placeholder=検索... autocapitalize=off autocomplete=off autocorrect=off spellcheck=false type=search class=form-control></div></section><section class=section-search-results><div id=search-hits></div></section></div></aside><nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id=navbar-main><div class=container><div class="d-none d-lg-inline-flex"><a class=navbar-brand href=/test/preview/e17f2badfaa26f05e6d577b35eeb867a434bd466/ja/><img src=/test/preview/e17f2badfaa26f05e6d577b35eeb867a434bd466/images/logo_hu43423524dbd337905d6e12d1b20a6666_22772_0x70_resize_lanczos_2.png alt=大阪大学データビリティフロンティア機構></a></div><button type=button class=navbar-toggler data-toggle=collapse data-target=#navbar-content aria-controls=navbar aria-expanded=false aria-label=ナビゲーションの切り替え>
<span><i class="fas fa-bars"></i></span></button><div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none"><a class=navbar-brand href=/test/preview/e17f2badfaa26f05e6d577b35eeb867a434bd466/ja/><img src=/test/preview/e17f2badfaa26f05e6d577b35eeb867a434bd466/images/logo_hu43423524dbd337905d6e12d1b20a6666_22772_0x70_resize_lanczos_2.png alt=大阪大学データビリティフロンティア機構></a></div><div class="navbar-collapse main-menu-item collapse justify-content-start" id=navbar-content><ul class="navbar-nav d-md-inline-flex"><li class=nav-item><a class="nav-link active" href=/test/preview/e17f2badfaa26f05e6d577b35eeb867a434bd466/ja/test/preview/e17f2badfaa26f05e6d577b35eeb867a434bd466/><span>トップ</span></a></li><li class=nav-item><a class=nav-link href=/test/preview/e17f2badfaa26f05e6d577b35eeb867a434bd466/ja/test/preview/e17f2badfaa26f05e6d577b35eeb867a434bd466/message><span>ご挨拶</span></a></li><li class=nav-item><a class=nav-link href=/test/preview/e17f2badfaa26f05e6d577b35eeb867a434bd466/ja/test/preview/e17f2badfaa26f05e6d577b35eeb867a434bd466/organization><span>組織</span></a></li><li class=nav-item><a class=nav-link href=/test/preview/e17f2badfaa26f05e6d577b35eeb867a434bd466/ja/test/preview/e17f2badfaa26f05e6d577b35eeb867a434bd466/people><span>スタッフ</span></a></li><li class=nav-item><a class=nav-link href=/test/preview/e17f2badfaa26f05e6d577b35eeb867a434bd466/ja/test/preview/e17f2badfaa26f05e6d577b35eeb867a434bd466/projects><span>プロジェクト</span></a></li><li class=nav-item><a class=nav-link href=/test/preview/e17f2badfaa26f05e6d577b35eeb867a434bd466/ja/test/preview/e17f2badfaa26f05e6d577b35eeb867a434bd466/publication><span>発表文献</span></a></li><li class=nav-item><a class=nav-link href=/test/preview/e17f2badfaa26f05e6d577b35eeb867a434bd466/ja/test/preview/e17f2badfaa26f05e6d577b35eeb867a434bd466/access><span>アクセス</span></a></li></ul></div><ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2"><li class=nav-item><a class="nav-link js-search" href=# aria-label=Search><i class="fas fa-search" aria-hidden=true></i></a></li><li class="nav-item dropdown theme-dropdown"><a href=# class="nav-link js-theme-selector" data-toggle=dropdown aria-haspopup=true><i class="fas fa-palette" aria-hidden=true></i></a><div class=dropdown-menu><a href=# class="dropdown-item js-set-theme-light"><span>Light</span></a>
<a href=# class="dropdown-item js-set-theme-dark"><span>Dark</span></a>
<a href=# class="dropdown-item js-set-theme-auto"><span>Automatic</span></a></div></li><li class="nav-item dropdown i18n-dropdown"><a href=# class="nav-link dropdown-toggle" data-toggle=dropdown aria-haspopup=true><i class="fas fa-globe mr-1" aria-hidden=true></i><span class="d-none d-lg-inline">日本語</span></a><div class=dropdown-menu><div class="dropdown-item dropdown-item-active"><span>日本語</span></div><a class=dropdown-item href=http://www.ids.osaka-u.ac.jp/test/preview/e17f2badfaa26f05e6d577b35eeb867a434bd466/publication/terai-2020/><span>English</span></a></div></li></ul></div></nav><div class=pub><div class="article-container pt-3"><h1>Detecting learner drowsiness based on facial expressions and head movements in online courses</h1><div class=article-metadata><div><span>Shogo Terai</span>, <span>Shizuka Shirai</span>, <span>Mehrasa Alizadeh</span>, <span>Ryosuke Kawamura</span>, <span>Noriko Takemura</span>, <span>Yuki Uranishi</span>, <span>Haruo Takemura</span>, <span>Hajime Nagahara</span></div><span class=article-date>March 2020</span></div><div class="btn-links mb-3"><button type=button class="btn btn-outline-primary my-1 mr-1 js-cite-modal" data-filename=/test/preview/e17f2badfaa26f05e6d577b35eeb867a434bd466/ja/publication/terai-2020/cite.bib>
引用</button>
<a class="btn btn-outline-primary my-1 mr-1" href=https://doi.org/10.1145/3379336.3381500 target=_blank rel=noopener>DOI</a></div></div><div class=article-container><h3>概要</h3><p class=pub-abstract>Drowsiness is a major factor that hinders learning. To improve learning efficiency, it is important to understand students&rsquo; physical status such as wakefulness during online coursework. In this study, we have proposed a drowsiness estimation method based on learners&rsquo; head and facial movements while viewing video lectures. To examine the effectiveness of head and facial movements in drowsiness estimation, we collected learner video data recorded during e-learning and applied a deep learning approach under the following conditions: (a) using only facial movement data, (b) using only head movement data, and (c) using both facial and head movement data.We achieved an average F1-macro score of 0.74 in personalized models for detecting learner drowsiness using both facial and head movement data.</p><div class=row><div class=col-md-1></div><div class=col-md-10><div class=row><div class="col-12 col-md-3 pub-row-heading">タイプ</div><div class="col-12 col-md-9"><a href=/test/preview/e17f2badfaa26f05e6d577b35eeb867a434bd466/ja/publication/#1>Conference paper</a></div></div></div><div class=col-md-1></div></div><div class="d-md-none space-below"></div><div class=row><div class=col-md-1></div><div class=col-md-10><div class=row><div class="col-12 col-md-3 pub-row-heading">収録</div><div class="col-12 col-md-9"><em>International Conference on Intelligent User Interfaces, Proceedings IUI</em></div></div></div><div class=col-md-1></div></div><div class="d-md-none space-below"></div><div class=space-below></div><div class=article-style></div><div class=article-tags><a class="badge badge-light" href=/test/preview/e17f2badfaa26f05e6d577b35eeb867a434bd466/ja/tag/drowsiness-estimation/>drowsiness estimation</a>
<a class="badge badge-light" href=/test/preview/e17f2badfaa26f05e6d577b35eeb867a434bd466/ja/tag/e-learning/>e-learning</a>
<a class="badge badge-light" href=/test/preview/e17f2badfaa26f05e6d577b35eeb867a434bd466/ja/tag/facial-movements/>facial movements</a>
<a class="badge badge-light" href=/test/preview/e17f2badfaa26f05e6d577b35eeb867a434bd466/ja/tag/head-movements/>head movements</a></div><div class=share-box aria-hidden=true><ul class=share><li><a href="https://twitter.com/intent/tweet?url=http://www.ids.osaka-u.ac.jp/test/preview/e17f2badfaa26f05e6d577b35eeb867a434bd466/ja/publication/terai-2020/&text=Detecting%20learner%20drowsiness%20based%20on%20facial%20expressions%20and%20head%20movements%20in%20online%20courses" target=_blank rel=noopener class=share-btn-twitter><i class="fab fa-twitter"></i></a></li><li><a href="https://www.facebook.com/sharer.php?u=http://www.ids.osaka-u.ac.jp/test/preview/e17f2badfaa26f05e6d577b35eeb867a434bd466/ja/publication/terai-2020/&t=Detecting%20learner%20drowsiness%20based%20on%20facial%20expressions%20and%20head%20movements%20in%20online%20courses" target=_blank rel=noopener class=share-btn-facebook><i class="fab fa-facebook"></i></a></li><li><a href="mailto:?subject=Detecting%20learner%20drowsiness%20based%20on%20facial%20expressions%20and%20head%20movements%20in%20online%20courses&body=http://www.ids.osaka-u.ac.jp/test/preview/e17f2badfaa26f05e6d577b35eeb867a434bd466/ja/publication/terai-2020/" target=_blank rel=noopener class=share-btn-email><i class="fas fa-envelope"></i></a></li><li><a href="https://www.linkedin.com/shareArticle?url=http://www.ids.osaka-u.ac.jp/test/preview/e17f2badfaa26f05e6d577b35eeb867a434bd466/ja/publication/terai-2020/&title=Detecting%20learner%20drowsiness%20based%20on%20facial%20expressions%20and%20head%20movements%20in%20online%20courses" target=_blank rel=noopener class=share-btn-linkedin><i class="fab fa-linkedin-in"></i></a></li><li><a href="whatsapp://send?text=Detecting%20learner%20drowsiness%20based%20on%20facial%20expressions%20and%20head%20movements%20in%20online%20courses%20http://www.ids.osaka-u.ac.jp/test/preview/e17f2badfaa26f05e6d577b35eeb867a434bd466/ja/publication/terai-2020/" target=_blank rel=noopener class=share-btn-whatsapp><i class="fab fa-whatsapp"></i></a></li><li><a href="https://service.weibo.com/share/share.php?url=http://www.ids.osaka-u.ac.jp/test/preview/e17f2badfaa26f05e6d577b35eeb867a434bd466/ja/publication/terai-2020/&title=Detecting%20learner%20drowsiness%20based%20on%20facial%20expressions%20and%20head%20movements%20in%20online%20courses" target=_blank rel=noopener class=share-btn-weibo><i class="fab fa-weibo"></i></a></li></ul></div><div class="media author-card content-widget-hr"><img class="avatar mr-3 avatar-circle" src=/test/preview/e17f2badfaa26f05e6d577b35eeb867a434bd466/ja/author/noriko-takemura/avatar_hu996ae61f250b1cca239f161e0fa0c584_24891_270x270_fill_q90_lanczos_center.jpg alt="Noriko Takemura"><div class=media-body><h5 class=card-title><a href=/test/preview/e17f2badfaa26f05e6d577b35eeb867a434bd466/ja/author/noriko-takemura/>Noriko Takemura</a></h5><h6 class=card-subtitle>Associate Professor</h6><p class=card-text>She is working on ambient intelligence and gait recognition using pattern recognition and machine learning.</p><ul class=network-icon aria-hidden=true><li><a href=/test/preview/e17f2badfaa26f05e6d577b35eeb867a434bd466/ja/#contact><i class="fas fa-envelope"></i></a></li><li><a href=/test/preview/e17f2badfaa26f05e6d577b35eeb867a434bd466/ja><i class="fas fa-phone"></i></a></li><li><a href=https://sites.google.com/site/norikotakemurashomepage/ target=_blank rel=noopener><i class="fas fa-home"></i></a></li></ul></div></div><div class="media author-card content-widget-hr"><img class="avatar mr-3 avatar-circle" src=/test/preview/e17f2badfaa26f05e6d577b35eeb867a434bd466/ja/author/hajime-nagahara/avatar_huc8d8cedf74443a99c5e73ee96d17a7a6_5028372_270x270_fill_lanczos_center_2.png alt="Hajime Nagahara"><div class=media-body><h5 class=card-title><a href=/test/preview/e17f2badfaa26f05e6d577b35eeb867a434bd466/ja/author/hajime-nagahara/>Hajime Nagahara</a></h5><h6 class=card-subtitle>Professor</h6><p class=card-text>He is working on computer vision and pattern recognition. His main research interests lie in image/video recognition and understanding, as well as applications of natural language processing techniques.</p><ul class=network-icon aria-hidden=true><li><a href=/test/preview/e17f2badfaa26f05e6d577b35eeb867a434bd466/ja/#contact><i class="fas fa-envelope"></i></a></li><li><a href=/test/preview/e17f2badfaa26f05e6d577b35eeb867a434bd466/ja><i class="fas fa-phone"></i></a></li><li><a href=http://www.ids.osaka-u.ac.jp/nagahara/ target=_blank rel=noopener><i class="fas fa-home"></i></a></li><li><a href="https://scholar.google.com/citations?user=CZyXjREAAAAJ&hl=ja" target=_blank rel=noopener><i class="ai ai-google-scholar"></i></a></li></ul></div></div><div class="article-widget content-widget-hr"><h3>関連項目</h3><ul><li><a href=/test/preview/e17f2badfaa26f05e6d577b35eeb867a434bd466/ja/publication/shirai-2019/>Multimodal learning analytics: Society 5.0 project in Japan</a></li></ul></div></div></div><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js integrity="sha256-yt2kYMy0w8AbtF89WXb2P1rfjcP/HTHLT7097U8Y5b8=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/instant.page/5.1.0/instantpage.min.js integrity="sha512-1+qUtKoh9XZW7j+6LhRMAyOrgSQKenQ4mluTR+cvxXjP1Z54RxZuzstR/H9kgPXQsVB8IW7DMDFUJpzLjvhGSQ==" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js integrity="sha256-eOgo0OtLL4cdq7RdwRUiGKLX9XsIJ7nGhWEKbohmVAQ=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/languages/r.min.js></script><script src=https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.5.1/leaflet.js integrity="sha256-EErZamuLefUnbMBQbsEqu1USa+btR2oIlCpBJbyD4/g=" crossorigin=anonymous></script><script>const code_highlighting=true;</script><script>const isSiteThemeDark=false;</script><script>const search_config={"indexURI":"/test/preview/e17f2badfaa26f05e6d577b35eeb867a434bd466/ja/index.json","minLength":1,"threshold":0.3};const i18n={"no_results":"結果が見つかりませんでした","placeholder":"検索...","results":"results found"};const content_type={'post':"投稿",'project':"プロジェクト",'publication':"発表文献",'talk':"登壇",'slides':"Slides"};</script><script id=search-hit-fuse-template type=text/x-template>
      <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
      </div>
    </script><script src=https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin=anonymous></script><script src=/test/preview/e17f2badfaa26f05e6d577b35eeb867a434bd466/js/academic.min.66c553246b0f279a03be6e5597f72b52.js></script><div class=container><footer class=site-footer><div class="d-none d-lg-inline-flex"><a class=navbar-brand href=/test/preview/e17f2badfaa26f05e6d577b35eeb867a434bd466/ja/><img src=/test/preview/e17f2badfaa26f05e6d577b35eeb867a434bd466/images/logo_hu43423524dbd337905d6e12d1b20a6666_22772_0x70_resize_lanczos_2.png alt=大阪大学データビリティフロンティア機構></a></div><p class=powered-by style=font-size:70%;text-align:left;margin-left:40px>TEL: +81 6 6105 6074<br>FAX: +81 6 6105 6075<br>Techno-alliance bldg. C503, 2-8, Yamadaoka, Suita, Osaka 565-0971</p><p class=powered-by></p><p class=powered-by>Published with
<a href=https://sourcethemes.com/academic/ target=_blank rel=noopener>Academic Website Builder</a>
<span class=float-right aria-hidden=true><a href=# class=back-to-top><span class=button_icon><i class="fas fa-chevron-up fa-2x"></i></span></a></span></p></footer></div><div id=modal class="modal fade" role=dialog><div class=modal-dialog><div class=modal-content><div class=modal-header><h5 class=modal-title>引用</h5><button type=button class=close data-dismiss=modal aria-label=Close>
<span aria-hidden=true>&#215;</span></button></div><div class=modal-body><pre><code class="tex hljs"></code></pre></div><div class=modal-footer><a class="btn btn-outline-primary my-1 js-copy-cite" href=# target=_blank><i class="fas fa-copy"></i>コピー</a>
<a class="btn btn-outline-primary my-1 js-download-cite" href=# target=_blank><i class="fas fa-download"></i>ダウンロード</a><div id=modal-error></div></div></div></div></div></body></html>