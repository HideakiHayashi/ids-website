<!doctype html><html lang=ja><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=generator content="Source Themes Academic 4.8.0"><meta name=author content="Web Administrator"><meta name=description content="Retinal imaging serves as a valuable tool for diagnosis of various diseases. However, reading retinal images is a difficult and time-consuming task even for experienced specialists. The fundamental step towards automated retinal image analysis is vessel segmentation and artery/vein classification, which provide various information on potential disorders. To improve the performance of the existing automated methods for retinal image analysis, we propose a two-step vessel classification. We adopt a UNet-based model, SeqNet, to accurately segment vessels from the background and make prediction on the vessel type. Our model does segmentation and classification sequentially, which alleviates the problem of label distribution bias and facilitates training. To further refine classification results, we post-process them considering the structural information among vessels to propagate highly confident prediction to surrounding vessels. Our experiments show that our method improves AUC to 0.98 for segmentation and the accuracy to 0.92 in classification over DRIVE dataset."><link rel=alternate hreflang=en href=/publication/li-2020-a/><link rel=alternate hreflang=ja href=/ja/publication/li-2020-a/><meta name=theme-color content="#2962ff"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.0-1/css/all.min.css integrity="sha256-4w9DunooKSr3MFXHXWyFER38WmPdm361bQS/2KUWZbU=" crossorigin=anonymous><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css integrity="sha256-Vzbj7sDDS/woiFS3uNKo8eIuni59rjyNGtXfstRzStA=" crossorigin=anonymous><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/github.min.css crossorigin=anonymous title=hl-light><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/dracula.min.css crossorigin=anonymous title=hl-dark disabled><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.5.1/leaflet.css integrity="sha256-SHMGCYmST46SoyGgo4YR/9AlK1vf3ff84Aq9yK4hdqM=" crossorigin=anonymous><script src=https://cdnjs.cloudflare.com/ajax/libs/lazysizes/5.1.2/lazysizes.min.js integrity="sha256-Md1qLToewPeKjfAHU1zyPwOutccPAm5tahnaw7Osw0A=" crossorigin=anonymous async></script><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Montserrat:400,700%7CRoboto:400,400italic,700%7CRoboto+Mono&display=swap"><link rel=stylesheet href=/css/academic.css><link rel=manifest href=/ja/index.webmanifest><link rel=icon type=image/png href=/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_32x32_fill_lanczos_center_2.png><link rel=apple-touch-icon type=image/png href=/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_192x192_fill_lanczos_center_2.png><link rel=canonical href=/ja/publication/li-2020-a/><meta property="twitter:card" content="summary_large_image"><meta property="og:site_name" content="大阪大学データビリティフロンティア機構"><meta property="og:url" content="/ja/publication/li-2020-a/"><meta property="og:title" content="Joint learning of vessel segmentation and artery/vein classification with post-processing | 大阪大学データビリティフロンティア機構"><meta property="og:description" content="Retinal imaging serves as a valuable tool for diagnosis of various diseases. However, reading retinal images is a difficult and time-consuming task even for experienced specialists. The fundamental step towards automated retinal image analysis is vessel segmentation and artery/vein classification, which provide various information on potential disorders. To improve the performance of the existing automated methods for retinal image analysis, we propose a two-step vessel classification. We adopt a UNet-based model, SeqNet, to accurately segment vessels from the background and make prediction on the vessel type. Our model does segmentation and classification sequentially, which alleviates the problem of label distribution bias and facilitates training. To further refine classification results, we post-process them considering the structural information among vessels to propagate highly confident prediction to surrounding vessels. Our experiments show that our method improves AUC to 0.98 for segmentation and the accuracy to 0.92 in classification over DRIVE dataset."><meta property="og:image" content="/media/large.png"><meta property="twitter:image" content="/media/large.png"><meta property="og:locale" content="ja"><meta property="article:published_time" content="2020-08-03T06:16:28+00:00"><meta property="article:modified_time" content="2020-01-01T00:00:00+00:00"><script type=application/ld+json>{"@context":"https://schema.org","@type":"Article","mainEntityOfPage":{"@type":"WebPage","@id":"/ja/publication/li-2020-a/"},"headline":"Joint learning of vessel segmentation and artery/vein classification with post-processing","datePublished":"2020-08-03T06:16:28Z","dateModified":"2020-01-01T00:00:00Z","author":{"@type":"Person","name":"Liangzhi Li"},"publisher":{"@type":"Organization","name":"Institute for Datability Science, Osaka University","logo":{"@type":"ImageObject","url":"/images/logo_hu43423524dbd337905d6e12d1b20a6666_22772_192x192_fit_lanczos_2.png"}},"description":"Retinal imaging serves as a valuable tool for diagnosis of various diseases. However, reading retinal images is a difficult and time-consuming task even for experienced specialists. The fundamental step towards automated retinal image analysis is vessel segmentation and artery/vein classification, which provide various information on potential disorders. To improve the performance of the existing automated methods for retinal image analysis, we propose a two-step vessel classification. We adopt a UNet-based model, SeqNet, to accurately segment vessels from the background and make prediction on the vessel type. Our model does segmentation and classification sequentially, which alleviates the problem of label distribution bias and facilitates training. To further refine classification results, we post-process them considering the structural information among vessels to propagate highly confident prediction to surrounding vessels. Our experiments show that our method improves AUC to 0.98 for segmentation and the accuracy to 0.92 in classification over DRIVE dataset."}</script><title>Joint learning of vessel segmentation and artery/vein classification with post-processing | 大阪大学データビリティフロンティア機構</title></head><body id=top data-spy=scroll data-offset=70 data-target=#TableOfContents><aside class=search-results id=search><div class=container><section class=search-header><div class="row no-gutters justify-content-between mb-3"><div class=col-6><h1>Search</h1></div><div class="col-6 col-search-close"><a class=js-search href=#><i class="fas fa-times-circle text-muted" aria-hidden=true></i></a></div></div><div id=search-box><input name=q id=search-query placeholder=検索... autocapitalize=off autocomplete=off autocorrect=off spellcheck=false type=search class=form-control></div></section><section class=section-search-results><div id=search-hits></div></section></div></aside><nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id=navbar-main><div class=container><div class="d-none d-lg-inline-flex"><a class=navbar-brand href=/ja><img src=/images/logo_hu43423524dbd337905d6e12d1b20a6666_22772_0x70_resize_lanczos_2.png alt=大阪大学データビリティフロンティア機構></a></div><button type=button class=navbar-toggler data-toggle=collapse data-target=#navbar-content aria-controls=navbar aria-expanded=false aria-label=ナビゲーションの切り替え>
<span><i class="fas fa-bars"></i></span></button><div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none"><a class=navbar-brand href=/ja><img src=/images/logo_hu43423524dbd337905d6e12d1b20a6666_22772_0x70_resize_lanczos_2.png alt=大阪大学データビリティフロンティア機構></a></div><div class="navbar-collapse main-menu-item collapse justify-content-start" id=navbar-content><ul class="navbar-nav d-md-inline-flex"><li class=nav-item><a class="nav-link active" href=/ja><span>トップ</span></a></li><li class=nav-item><a class=nav-link href=/ja/message><span>ご挨拶</span></a></li><li class=nav-item><a class=nav-link href=/ja/organization><span>組織</span></a></li><li class=nav-item><a class=nav-link href=/ja/people><span>スタッフ</span></a></li><li class=nav-item><a class=nav-link href=/ja/projects><span>プロジェクト</span></a></li><li class=nav-item><a class="nav-link active" href=/ja/publication><span>発表文献</span></a></li><li class=nav-item><a class=nav-link href=/ja/access><span>アクセス</span></a></li></ul></div><ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2"><li class=nav-item><a class="nav-link js-search" href=# aria-label=Search><i class="fas fa-search" aria-hidden=true></i></a></li><li class="nav-item dropdown theme-dropdown"><a href=# class="nav-link js-theme-selector" data-toggle=dropdown aria-haspopup=true><i class="fas fa-palette" aria-hidden=true></i></a><div class=dropdown-menu><a href=# class="dropdown-item js-set-theme-light"><span>Light</span></a>
<a href=# class="dropdown-item js-set-theme-dark"><span>Dark</span></a>
<a href=# class="dropdown-item js-set-theme-auto"><span>Automatic</span></a></div></li><li class="nav-item dropdown i18n-dropdown"><a href=# class="nav-link dropdown-toggle" data-toggle=dropdown aria-haspopup=true><i class="fas fa-globe mr-1" aria-hidden=true></i><span class="d-none d-lg-inline">日本語</span></a><div class=dropdown-menu><div class="dropdown-item dropdown-item-active"><span>日本語</span></div><a class=dropdown-item href=/publication/li-2020-a/><span>English</span></a></div></li></ul></div></nav><div class=pub><div class="article-container pt-3"><h1>Joint learning of vessel segmentation and artery/vein classification with post-processing</h1><div class=article-metadata><div><span>Liangzhi Li</span>, <span>Manisha Verma</span>, <span>Yuta Nakashima</span>, <span>Ryo Kawasaki</span>, <span>Hajime Nagahara</span></div><span class=article-date>January 2020</span></div><div class="btn-links mb-3"><a class="btn btn-outline-primary my-1 mr-1" href=https://www.liangzhili.com/publication/li-2020-joint/ target=_blank rel=noopener>PDF</a>
<button type=button class="btn btn-outline-primary my-1 mr-1 js-cite-modal" data-filename=/ja/publication/li-2020-a/cite.bib>
引用</button></div></div><div class=article-container><h3>概要</h3><p class=pub-abstract>Retinal imaging serves as a valuable tool for diagnosis of various diseases. However, reading retinal images is a difficult and time-consuming task even for experienced specialists. The fundamental step towards automated retinal image analysis is vessel segmentation and artery/vein classification, which provide various information on potential disorders. To improve the performance of the existing automated methods for retinal image analysis, we propose a two-step vessel classification. We adopt a UNet-based model, SeqNet, to accurately segment vessels from the background and make prediction on the vessel type. Our model does segmentation and classification sequentially, which alleviates the problem of label distribution bias and facilitates training. To further refine classification results, we post-process them considering the structural information among vessels to propagate highly confident prediction to surrounding vessels. Our experiments show that our method improves AUC to 0.98 for segmentation and the accuracy to 0.92 in classification over DRIVE dataset.</p><div class=row><div class=col-md-1></div><div class=col-md-10><div class=row><div class="col-12 col-md-3 pub-row-heading">タイプ</div><div class="col-12 col-md-9"><a href=/ja/publication/#1>Conference paper</a></div></div></div><div class=col-md-1></div></div><div class="d-md-none space-below"></div><div class=row><div class=col-md-1></div><div class=col-md-10><div class=row><div class="col-12 col-md-3 pub-row-heading">収録</div><div class="col-12 col-md-9"><em>Medical Imaging with Deep Learning (MIDL)</em></div></div></div><div class=col-md-1></div></div><div class="d-md-none space-below"></div><div class=space-below></div><div class=article-style></div><div class=article-tags><a class="badge badge-light" href=/ja/tag/medical-imaging/>medical imaging</a>
<a class="badge badge-light" href=/ja/tag/computer-vision/>computer vision</a>
<a class="badge badge-light" href=/ja/tag/deep-learning/>deep learning</a>
<a class="badge badge-light" href=/ja/tag/retina-images/>retina images</a>
<a class="badge badge-light" href=/ja/tag/vessel-classification/>vessel classification</a>
<a class="badge badge-light" href=/ja/tag/vessel-segmentation/>vessel segmentation</a></div><div class=share-box aria-hidden=true><ul class=share><li><a href="https://twitter.com/intent/tweet?url=/ja/publication/li-2020-a/&text=Joint%20learning%20of%20vessel%20segmentation%20and%20artery/vein%20classification%20with%20post-processing" target=_blank rel=noopener class=share-btn-twitter><i class="fab fa-twitter"></i></a></li><li><a href="https://www.facebook.com/sharer.php?u=/ja/publication/li-2020-a/&t=Joint%20learning%20of%20vessel%20segmentation%20and%20artery/vein%20classification%20with%20post-processing" target=_blank rel=noopener class=share-btn-facebook><i class="fab fa-facebook"></i></a></li><li><a href="mailto:?subject=Joint%20learning%20of%20vessel%20segmentation%20and%20artery/vein%20classification%20with%20post-processing&body=/ja/publication/li-2020-a/" target=_blank rel=noopener class=share-btn-email><i class="fas fa-envelope"></i></a></li><li><a href="https://www.linkedin.com/shareArticle?url=/ja/publication/li-2020-a/&title=Joint%20learning%20of%20vessel%20segmentation%20and%20artery/vein%20classification%20with%20post-processing" target=_blank rel=noopener class=share-btn-linkedin><i class="fab fa-linkedin-in"></i></a></li><li><a href="whatsapp://send?text=Joint%20learning%20of%20vessel%20segmentation%20and%20artery/vein%20classification%20with%20post-processing%20/ja/publication/li-2020-a/" target=_blank rel=noopener class=share-btn-whatsapp><i class="fab fa-whatsapp"></i></a></li><li><a href="https://service.weibo.com/share/share.php?url=/ja/publication/li-2020-a/&title=Joint%20learning%20of%20vessel%20segmentation%20and%20artery/vein%20classification%20with%20post-processing" target=_blank rel=noopener class=share-btn-weibo><i class="fab fa-weibo"></i></a></li></ul></div><div class="media author-card content-widget-hr"><img class="avatar mr-3 avatar-circle" src=/ja/author/liangzhi-li/avatar_huc78ea4d522f8b97a72ade3dfb31dabac_89271_270x270_fill_lanczos_center_2.png alt="Liangzhi Li"><div class=media-body><h5 class=card-title><a href=/ja/author/liangzhi-li/>Liangzhi Li</a></h5><h6 class=card-subtitle>Specially-Appointed Researcher/Fellow</h6><p class=card-text>His research interests lie in deep learning, computer vision, robotics, and medical images.</p><ul class=network-icon aria-hidden=true><li><a href=https://www.liangzhili.com target=_blank rel=noopener><i class="fas fa-safari"></i></a></li><li><a href=/ja/#contact><i class="fas fa-envelope"></i></a></li><li><a href="https://scholar.google.com/citations?user=JIRw_tMAAAAJ" target=_blank rel=noopener><i class="ai ai-google-scholar"></i></a></li><li><a href=https://github.com/conscienceli target=_blank rel=noopener><i class="fab fa-github"></i></a></li><li><a href=https://twitter.com/jconlee8 target=_blank rel=noopener><i class="fab fa-twitter"></i></a></li><li><a href=https://www.facebook.com/liliangzhi target=_blank rel=noopener><i class="fab fa-facebook"></i></a></li><li><a href=https://www.linkedin.com/in/liangzhi-li-80a553154/ target=_blank rel=noopener><i class="fab fa-linkedin"></i></a></li><li><a href=/ja/><i class="fas fa-phone"></i></a></li></ul></div></div><div class="media author-card content-widget-hr"><img class="avatar mr-3 avatar-circle" src=/ja/author/manisha-verma/avatar_hufb97c9e49b4b12e88be9eeb69e57899e_3079726_270x270_fill_lanczos_center_2.png alt="Manisha Verma"><div class=media-body><h5 class=card-title><a href=/ja/author/manisha-verma/>Manisha Verma</a></h5><h6 class=card-subtitle>Specially-Appointed Researcher/Fellow</h6><p class=card-text>Manisha&rsquo;s research interest broadly lies in computer vision and image processing. Currently, she is working on micro facial expression recognition using multi-model deep learning frameworks.</p><ul class=network-icon aria-hidden=true><li><a href=/ja/#contact><i class="fas fa-envelope"></i></a></li><li><a href=/ja/><i class="fas fa-phone"></i></a></li><li><a href=https://sites.google.com/site/manishaverma89/ target=_blank rel=noopener><i class="fas fa-home"></i></a></li><li><a href="https://scholar.google.co.jp/citations?user=XA2Qul0AAAAJ&hl=en" target=_blank rel=noopener><i class="ai ai-google-scholar"></i></a></li><li><a href=https://www.linkedin.com/in/manishaverma071989 target=_blank rel=noopener><i class="fab fa-linkedin"></i></a></li></ul></div></div><div class="media author-card content-widget-hr"><img class="avatar mr-3 avatar-circle" src=/ja/author/yuta-nakashima/avatar_hu25d333fcded103db2f52f7476cb1420a_78230_270x270_fill_q90_lanczos_center.jpg alt="Yuta Nakashima"><div class=media-body><h5 class=card-title><a href=/ja/author/yuta-nakashima/>Yuta Nakashima</a></h5><h6 class=card-subtitle>Associate Professor</h6><p class=card-text>Yuta Nakashima is an associate professor with Institute for Datability Science, Osaka University. His research interests include computer vision, pattern recognition, natural langauge processing, and their applications.</p><ul class=network-icon aria-hidden=true><li><a href=/ja/#contact><i class="fas fa-envelope"></i></a></li><li><a href=/ja/><i class="fas fa-phone"></i></a></li><li><a href=http://n-yuta.jp/ target=_blank rel=noopener><i class="fas fa-home"></i></a></li><li><a href="https://scholar.google.com/citations?user=LNvd0VQAAAAJ" target=_blank rel=noopener><i class="ai ai-google-scholar"></i></a></li><li><a href=https://github.com/USERNAME target=_blank rel=noopener><i class="fab fa-github"></i></a></li></ul></div></div><div class="media author-card content-widget-hr"><img class="avatar mr-3 avatar-circle" src=/ja/author/hajime-nagahara/avatar_huc8d8cedf74443a99c5e73ee96d17a7a6_5028372_270x270_fill_lanczos_center_2.png alt="Hajime Nagahara"><div class=media-body><h5 class=card-title><a href=/ja/author/hajime-nagahara/>Hajime Nagahara</a></h5><h6 class=card-subtitle>Professor</h6><p class=card-text>He is working on computer vision and pattern recognition. His main research interests lie in image/video recognition and understanding, as well as applications of natural language processing techniques.</p><ul class=network-icon aria-hidden=true><li><a href=/ja/#contact><i class="fas fa-envelope"></i></a></li><li><a href=/ja/><i class="fas fa-phone"></i></a></li><li><a href=http://www.ids.osaka-u.ac.jp/nagahara/ target=_blank rel=noopener><i class="fas fa-home"></i></a></li><li><a href="https://scholar.google.com/citations?user=CZyXjREAAAAJ&hl=ja" target=_blank rel=noopener><i class="ai ai-google-scholar"></i></a></li></ul></div></div><div class="article-widget content-widget-hr"><h3>関連項目</h3><ul><li><a href=/ja/publication/nguyen-canh-2019/>Deep compressive sensing for visual privacy protection in flatcam imaging</a></li><li><a href=/ja/publication/tejero-de-pablos-2018/>Summarization of user-generated sports video by using deep action recognition features</a></li><li><a href=/ja/publication/li-2020/>IterNet: retinal image segmentation utilizing structural redundancy in vessel networks</a></li><li><a href=/ja/publication/miyake-2018/>Graphical classification of DNA sequences of HLA alleles by deep learning</a></li><li><a href=/ja/publication/niioka-2017/>Classification of C2C12 cells at differentiation by convolutional neural network of deep learning using phase contrast images</a></li></ul></div></div></div><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js integrity="sha256-yt2kYMy0w8AbtF89WXb2P1rfjcP/HTHLT7097U8Y5b8=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/instant.page/5.1.0/instantpage.min.js integrity="sha512-1+qUtKoh9XZW7j+6LhRMAyOrgSQKenQ4mluTR+cvxXjP1Z54RxZuzstR/H9kgPXQsVB8IW7DMDFUJpzLjvhGSQ==" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js integrity="sha256-eOgo0OtLL4cdq7RdwRUiGKLX9XsIJ7nGhWEKbohmVAQ=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/languages/r.min.js></script><script src=https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.5.1/leaflet.js integrity="sha256-EErZamuLefUnbMBQbsEqu1USa+btR2oIlCpBJbyD4/g=" crossorigin=anonymous></script><script>const code_highlighting=true;</script><script>const isSiteThemeDark=false;</script><script>const search_config={"indexURI":"/ja/index.json","minLength":1,"threshold":0.3};const i18n={"no_results":"結果が見つかりませんでした","placeholder":"検索...","results":"results found"};const content_type={'post':"投稿",'project':"プロジェクト",'publication':"発表文献",'talk':"登壇",'slides':"Slides"};</script><script id=search-hit-fuse-template type=text/x-template>
      <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
      </div>
    </script><script src=https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin=anonymous></script><script src=/js/academic.min.66c553246b0f279a03be6e5597f72b52.js></script><div class=container><footer class=site-footer><div class="d-none d-lg-inline-flex"><a class=navbar-brand href=/ja><img src=/images/logo_hu43423524dbd337905d6e12d1b20a6666_22772_0x70_resize_lanczos_2.png alt=大阪大学データビリティフロンティア機構></a></div><p class=powered-by style=font-size:70%;text-align:left;margin-left:40px>TEL: +81 6 6105 6074<br>FAX: +81 6 6105 6075<br>Techno-alliance bldg. C503, 2-8, Yamadaoka, Suita, Osaka 565-0971</p><p class=powered-by></p><p class=powered-by>Published with
<a href=https://sourcethemes.com/academic/ target=_blank rel=noopener>Academic Website Builder</a>
<span class=float-right aria-hidden=true><a href=# class=back-to-top><span class=button_icon><i class="fas fa-chevron-up fa-2x"></i></span></a></span></p></footer></div><div id=modal class="modal fade" role=dialog><div class=modal-dialog><div class=modal-content><div class=modal-header><h5 class=modal-title>引用</h5><button type=button class=close data-dismiss=modal aria-label=Close>
<span aria-hidden=true>&#215;</span></button></div><div class=modal-body><pre><code class="tex hljs"></code></pre></div><div class=modal-footer><a class="btn btn-outline-primary my-1 js-copy-cite" href=# target=_blank><i class="fas fa-copy"></i>コピー</a>
<a class="btn btn-outline-primary my-1 js-download-cite" href=# target=_blank><i class="fas fa-download"></i>ダウンロード</a><div id=modal-error></div></div></div></div></div></body></html>