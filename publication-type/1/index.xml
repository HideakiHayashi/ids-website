<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>1 | Institute for Datability Science, Osaka University</title><link>http://www.ids.osaka-u.ac.jp/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/publication-type/1/</link><atom:link href="http://www.ids.osaka-u.ac.jp/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/publication-type/1/index.xml" rel="self" type="application/rss+xml"/><description>1</description><generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Wed, 01 Apr 2020 00:00:00 +0000</lastBuildDate><image><url>http://www.ids.osaka-u.ac.jp/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/media/large.png</url><title>1</title><link>http://www.ids.osaka-u.ac.jp/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/publication-type/1/</link></image><item><title>Yoga-82: a new dataset for fine-grained classification of human poses</title><link>http://www.ids.osaka-u.ac.jp/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/publication/verma-2020/</link><pubDate>Wed, 01 Apr 2020 00:00:00 +0000</pubDate><guid>http://www.ids.osaka-u.ac.jp/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/publication/verma-2020/</guid><description/></item><item><title>Detecting learner drowsiness based on facial expressions and head movements in online courses</title><link>http://www.ids.osaka-u.ac.jp/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/publication/terai-2020/</link><pubDate>Sun, 01 Mar 2020 00:00:00 +0000</pubDate><guid>http://www.ids.osaka-u.ac.jp/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/publication/terai-2020/</guid><description/></item><item><title>KnowIT VQA: Answering knowledge-based questions about videos</title><link>http://www.ids.osaka-u.ac.jp/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/publication/garcia-2020-a/</link><pubDate>Sat, 01 Feb 2020 00:00:00 +0000</pubDate><guid>http://www.ids.osaka-u.ac.jp/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/publication/garcia-2020-a/</guid><description/></item><item><title>3D Image Reconstruction from Multi-focus Microscopic Images</title><link>http://www.ids.osaka-u.ac.jp/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/publication/yamaguchi-2020/</link><pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate><guid>http://www.ids.osaka-u.ac.jp/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/publication/yamaguchi-2020/</guid><description/></item><item><title>BERT representations for video question answering</title><link>http://www.ids.osaka-u.ac.jp/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/publication/yang-2020/</link><pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate><guid>http://www.ids.osaka-u.ac.jp/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/publication/yang-2020/</guid><description/></item><item><title>IterNet: retinal image segmentation utilizing structural redundancy in vessel networks</title><link>http://www.ids.osaka-u.ac.jp/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/publication/li-2020/</link><pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate><guid>http://www.ids.osaka-u.ac.jp/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/publication/li-2020/</guid><description/></item><item><title>Joint learning of vessel segmentation and artery/vein classification with post-processing</title><link>http://www.ids.osaka-u.ac.jp/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/publication/li-2020-a/</link><pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate><guid>http://www.ids.osaka-u.ac.jp/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/publication/li-2020-a/</guid><description/></item><item><title>Human shape reconstruction with loose clothes from partially observed data by pose specific deformation</title><link>http://www.ids.osaka-u.ac.jp/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/publication/sayo-2019/</link><pubDate>Fri, 01 Nov 2019 00:00:00 +0000</pubDate><guid>http://www.ids.osaka-u.ac.jp/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/publication/sayo-2019/</guid><description/></item><item><title>Legal information as a complex network: Improving topic modeling through homophily</title><link>http://www.ids.osaka-u.ac.jp/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/publication/ashihara-2019-b/</link><pubDate>Fri, 01 Nov 2019 00:00:00 +0000</pubDate><guid>http://www.ids.osaka-u.ac.jp/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/publication/ashihara-2019-b/</guid><description/></item><item><title>Deep compressive sensing for visual privacy protection in flatcam imaging</title><link>http://www.ids.osaka-u.ac.jp/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/publication/nguyen-canh-2019/</link><pubDate>Tue, 01 Oct 2019 00:00:00 +0000</pubDate><guid>http://www.ids.osaka-u.ac.jp/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/publication/nguyen-canh-2019/</guid><description/></item><item><title>A 3-D Display Pipeline from Coded-Aperture Camera to Tensor Light-Field Display Through CNN</title><link>http://www.ids.osaka-u.ac.jp/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/publication/maruyama-2019/</link><pubDate>Sun, 01 Sep 2019 00:00:00 +0000</pubDate><guid>http://www.ids.osaka-u.ac.jp/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/publication/maruyama-2019/</guid><description/></item><item><title>Multimodal learning analytics: Society 5.0 project in Japan</title><link>http://www.ids.osaka-u.ac.jp/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/publication/shirai-2019/</link><pubDate>Fri, 01 Mar 2019 00:00:00 +0000</pubDate><guid>http://www.ids.osaka-u.ac.jp/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/publication/shirai-2019/</guid><description/></item><item><title>Buda.art: A multimodal content-based analysis and retrieval system for Buddha statues</title><link>http://www.ids.osaka-u.ac.jp/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/publication/renoust-2019-a/</link><pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate><guid>http://www.ids.osaka-u.ac.jp/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/publication/renoust-2019-a/</guid><description/></item><item><title>Context-aware embeddings for automatic art analysis</title><link>http://www.ids.osaka-u.ac.jp/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/publication/garcia-2019-a/</link><pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate><guid>http://www.ids.osaka-u.ac.jp/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/publication/garcia-2019-a/</guid><description/></item><item><title>Contextualized context2vec</title><link>http://www.ids.osaka-u.ac.jp/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/publication/ashihara-2019/</link><pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate><guid>http://www.ids.osaka-u.ac.jp/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/publication/ashihara-2019/</guid><description/></item><item><title>Controllable text simplification with lexical constraint loss</title><link>http://www.ids.osaka-u.ac.jp/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/publication/nishihara-2019/</link><pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate><guid>http://www.ids.osaka-u.ac.jp/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/publication/nishihara-2019/</guid><description/></item><item><title>Facial expression recognition with skip-connection to leverage low-level features</title><link>http://www.ids.osaka-u.ac.jp/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/publication/verma-2019/</link><pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate><guid>http://www.ids.osaka-u.ac.jp/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/publication/verma-2019/</guid><description/></item><item><title>Historical and modern features for Buddha statue classification</title><link>http://www.ids.osaka-u.ac.jp/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/publication/renoust-2019/</link><pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate><guid>http://www.ids.osaka-u.ac.jp/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/publication/renoust-2019/</guid><description/></item><item><title>Negative lexically constrained decoding for paraphrase generation</title><link>http://www.ids.osaka-u.ac.jp/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/publication/kajiwara-2020/</link><pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate><guid>http://www.ids.osaka-u.ac.jp/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/publication/kajiwara-2020/</guid><description/></item><item><title>Rethinking the evaluation of video summaries</title><link>http://www.ids.osaka-u.ac.jp/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/publication/otani-2019-a/</link><pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate><guid>http://www.ids.osaka-u.ac.jp/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/publication/otani-2019-a/</guid><description/></item><item><title>Video meets knowledge in visual question answering</title><link>http://www.ids.osaka-u.ac.jp/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/publication/noa-garcia-chenhui-chu-2019/</link><pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate><guid>http://www.ids.osaka-u.ac.jp/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/publication/noa-garcia-chenhui-chu-2019/</guid><description/></item><item><title>Space-time-brightness sampling using an adaptive pixel-wise coded exposure</title><link>http://www.ids.osaka-u.ac.jp/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/publication/nagahara-2018/</link><pubDate>Sat, 01 Dec 2018 00:00:00 +0000</pubDate><guid>http://www.ids.osaka-u.ac.jp/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/publication/nagahara-2018/</guid><description/></item><item><title>Representing a partially observed non-rigid 3D human using eigen-texture and eigen-deformation</title><link>http://www.ids.osaka-u.ac.jp/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/publication/kimura-2018/</link><pubDate>Thu, 01 Nov 2018 00:00:00 +0000</pubDate><guid>http://www.ids.osaka-u.ac.jp/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/publication/kimura-2018/</guid><description/></item><item><title>iParaphrasing: Extracting visually grounded paraphrases via an image</title><link>http://www.ids.osaka-u.ac.jp/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/publication/chu-2018-a/</link><pubDate>Fri, 01 Jun 2018 00:00:00 +0000</pubDate><guid>http://www.ids.osaka-u.ac.jp/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/publication/chu-2018-a/</guid><description/></item><item><title>PCA-coded aperture for light field photography</title><link>http://www.ids.osaka-u.ac.jp/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/publication/yagi-2018-a/</link><pubDate>Thu, 01 Feb 2018 00:00:00 +0000</pubDate><guid>http://www.ids.osaka-u.ac.jp/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/publication/yagi-2018-a/</guid><description/></item><item><title>Complex word identification based on frequency in a learner corpus</title><link>http://www.ids.osaka-u.ac.jp/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/publication/kajiwara-2018/</link><pubDate>Mon, 01 Jan 2018 00:00:00 +0000</pubDate><guid>http://www.ids.osaka-u.ac.jp/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/publication/kajiwara-2018/</guid><description/></item><item><title>Joint optimization for compressive video sensing and reconstruction under hardware constraints</title><link>http://www.ids.osaka-u.ac.jp/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/publication/yoshida-2018/</link><pubDate>Mon, 01 Jan 2018 00:00:00 +0000</pubDate><guid>http://www.ids.osaka-u.ac.jp/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/publication/yoshida-2018/</guid><description/></item><item><title>Learning to capture light fields through a coded aperture camera</title><link>http://www.ids.osaka-u.ac.jp/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/publication/inagaki-2018/</link><pubDate>Mon, 01 Jan 2018 00:00:00 +0000</pubDate><guid>http://www.ids.osaka-u.ac.jp/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/publication/inagaki-2018/</guid><description/></item><item><title>Metric for automatic machine translation evaluation based on universal sentence representations</title><link>http://www.ids.osaka-u.ac.jp/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/publication/shimanaka-2018-a/</link><pubDate>Mon, 01 Jan 2018 00:00:00 +0000</pubDate><guid>http://www.ids.osaka-u.ac.jp/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/publication/shimanaka-2018-a/</guid><description/></item><item><title>RUSE: Regressor using sentence embeddings for automatic machine translation evaluation</title><link>http://www.ids.osaka-u.ac.jp/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/publication/shimanaka-2018/</link><pubDate>Mon, 01 Jan 2018 00:00:00 +0000</pubDate><guid>http://www.ids.osaka-u.ac.jp/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/publication/shimanaka-2018/</guid><description/></item><item><title>Visually grounded paraphrase extraction</title><link>http://www.ids.osaka-u.ac.jp/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/publication/chu-2018/</link><pubDate>Mon, 01 Jan 2018 00:00:00 +0000</pubDate><guid>http://www.ids.osaka-u.ac.jp/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/publication/chu-2018/</guid><description/></item><item><title>Novel view synthesis with light-weight view-dependent texture mapping for a stereoscopic HMD</title><link>http://www.ids.osaka-u.ac.jp/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/publication/rongsirigul-2017/</link><pubDate>Tue, 01 Aug 2017 00:00:00 +0000</pubDate><guid>http://www.ids.osaka-u.ac.jp/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/publication/rongsirigul-2017/</guid><description/></item><item><title>Hyperspectral imaging using flickerless active LED illumination</title><link>http://www.ids.osaka-u.ac.jp/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/publication/ohsaki-2017/</link><pubDate>Wed, 01 Mar 2017 00:00:00 +0000</pubDate><guid>http://www.ids.osaka-u.ac.jp/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/publication/ohsaki-2017/</guid><description/></item><item><title>Fine-grained video retrieval for multi-clip video</title><link>http://www.ids.osaka-u.ac.jp/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/publication/otani-2017/</link><pubDate>Sun, 01 Jan 2017 00:00:00 +0000</pubDate><guid>http://www.ids.osaka-u.ac.jp/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/publication/otani-2017/</guid><description/></item><item><title>Incremental structural modeling on sparse visual SLAM</title><link>http://www.ids.osaka-u.ac.jp/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/publication/roberto-2017/</link><pubDate>Sun, 01 Jan 2017 00:00:00 +0000</pubDate><guid>http://www.ids.osaka-u.ac.jp/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/publication/roberto-2017/</guid><description/></item><item><title>Mixed features for face detection in thermal image</title><link>http://www.ids.osaka-u.ac.jp/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/publication/ma-2017/</link><pubDate>Sun, 01 Jan 2017 00:00:00 +0000</pubDate><guid>http://www.ids.osaka-u.ac.jp/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/publication/ma-2017/</guid><description/></item><item><title>Realtime novel view synthesis with eigen-texture regression</title><link>http://www.ids.osaka-u.ac.jp/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/publication/nakashima-2017/</link><pubDate>Sun, 01 Jan 2017 00:00:00 +0000</pubDate><guid>http://www.ids.osaka-u.ac.jp/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/publication/nakashima-2017/</guid><description/></item><item><title>ReMagicMirror: Action learning using human reenactment with the mirror metaphor</title><link>http://www.ids.osaka-u.ac.jp/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/publication/dayrit-2017/</link><pubDate>Sun, 01 Jan 2017 00:00:00 +0000</pubDate><guid>http://www.ids.osaka-u.ac.jp/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/publication/dayrit-2017/</guid><description/></item><item><title>Unsupervised Video Summarization using Deep Video Features</title><link>http://www.ids.osaka-u.ac.jp/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/publication/otani-2017-a/</link><pubDate>Sun, 01 Jan 2017 00:00:00 +0000</pubDate><guid>http://www.ids.osaka-u.ac.jp/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/publication/otani-2017-a/</guid><description/></item><item><title>Video question answering to find a desired video eegment</title><link>http://www.ids.osaka-u.ac.jp/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/publication/otani-2017-b/</link><pubDate>Sun, 01 Jan 2017 00:00:00 +0000</pubDate><guid>http://www.ids.osaka-u.ac.jp/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/publication/otani-2017-b/</guid><description/></item><item><title>High-speed imaging using CMOS image sensor with quasi pixel-wise exposure</title><link>http://www.ids.osaka-u.ac.jp/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/publication/nagahara-2016/</link><pubDate>Wed, 01 Jun 2016 00:00:00 +0000</pubDate><guid>http://www.ids.osaka-u.ac.jp/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/publication/nagahara-2016/</guid><description/></item><item><title>Dynamic photometric stereo method using multi-tap CMOS image sensor</title><link>http://www.ids.osaka-u.ac.jp/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/publication/yoda-2016/</link><pubDate>Fri, 01 Jan 2016 00:00:00 +0000</pubDate><guid>http://www.ids.osaka-u.ac.jp/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/publication/yoda-2016/</guid><description/></item></channel></rss>