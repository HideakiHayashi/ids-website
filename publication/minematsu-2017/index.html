<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=generator content="Source Themes Academic 4.8.0"><meta name=author content="Web Administrator"><meta name=description content="We propose a framework for adaptively registering background models with an image for background subtraction with moving cameras. Existing methods search for a background model using a fixed window size, to suppress the number of false positives when detecting the foreground. However, these approaches result in many false negatives because they may use inappropriate window sizes. The appropriate size depends on various factors of the target scenes. To suppress false detections, we propose adaptively controlling the method parameters, which are typically determined heuristically. More specifically, the search window size for background registration and the foreground detection threshold are automatically determined using the re-projection error computed by the homography based camera motion estimate. Our method is based on the fact that the error at a pixel is low if it belongs to background and high if it does not. We quantitatively confirmed that the proposed framework improved the background subtraction accuracy when applied to images from moving cameras in various public datasets."><link rel=alternate hreflang=ja href=http://www.ids.osaka-u.ac.jp/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/ja/publication/minematsu-2017/><link rel=alternate hreflang=en-us href=http://www.ids.osaka-u.ac.jp/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/publication/minematsu-2017/><meta name=theme-color content="#2962ff"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.0-1/css/all.min.css integrity="sha256-4w9DunooKSr3MFXHXWyFER38WmPdm361bQS/2KUWZbU=" crossorigin=anonymous><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css integrity="sha256-Vzbj7sDDS/woiFS3uNKo8eIuni59rjyNGtXfstRzStA=" crossorigin=anonymous><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/github.min.css crossorigin=anonymous title=hl-light><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/dracula.min.css crossorigin=anonymous title=hl-dark disabled><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.5.1/leaflet.css integrity="sha256-SHMGCYmST46SoyGgo4YR/9AlK1vf3ff84Aq9yK4hdqM=" crossorigin=anonymous><script src=https://cdnjs.cloudflare.com/ajax/libs/lazysizes/5.1.2/lazysizes.min.js integrity="sha256-Md1qLToewPeKjfAHU1zyPwOutccPAm5tahnaw7Osw0A=" crossorigin=anonymous async></script><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Montserrat:400,700%7CRoboto:400,400italic,700%7CRoboto+Mono&display=swap"><link rel=stylesheet href=/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/css/academic.css><link rel=manifest href=/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/index.webmanifest><link rel=icon type=image/png href=/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_32x32_fill_lanczos_center_2.png><link rel=apple-touch-icon type=image/png href=/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_192x192_fill_lanczos_center_2.png><link rel=canonical href=http://www.ids.osaka-u.ac.jp/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/publication/minematsu-2017/><meta property="twitter:card" content="summary_large_image"><meta property="og:site_name" content="Institute for Datability Science, Osaka University"><meta property="og:url" content="http://www.ids.osaka-u.ac.jp/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/publication/minematsu-2017/"><meta property="og:title" content="Adaptive background model registration for moving cameras | Institute for Datability Science, Osaka University"><meta property="og:description" content="We propose a framework for adaptively registering background models with an image for background subtraction with moving cameras. Existing methods search for a background model using a fixed window size, to suppress the number of false positives when detecting the foreground. However, these approaches result in many false negatives because they may use inappropriate window sizes. The appropriate size depends on various factors of the target scenes. To suppress false detections, we propose adaptively controlling the method parameters, which are typically determined heuristically. More specifically, the search window size for background registration and the foreground detection threshold are automatically determined using the re-projection error computed by the homography based camera motion estimate. Our method is based on the fact that the error at a pixel is low if it belongs to background and high if it does not. We quantitatively confirmed that the proposed framework improved the background subtraction accuracy when applied to images from moving cameras in various public datasets."><meta property="og:image" content="http://www.ids.osaka-u.ac.jp/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/media/large.png"><meta property="twitter:image" content="http://www.ids.osaka-u.ac.jp/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/media/large.png"><meta property="og:locale" content="en-us"><meta property="article:published_time" content="2020-08-03T06:16:28+00:00"><meta property="article:modified_time" content="2017-09-01T00:00:00+00:00"><script type=application/ld+json>{"@context":"https://schema.org","@type":"Article","mainEntityOfPage":{"@type":"WebPage","@id":"http://www.ids.osaka-u.ac.jp/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/publication/minematsu-2017/"},"headline":"Adaptive background model registration for moving cameras","datePublished":"2020-08-03T06:16:28Z","dateModified":"2017-09-01T00:00:00Z","author":{"@type":"Person","name":"Tsubasa Minematsu"},"publisher":{"@type":"Organization","name":"Institute for Datability Science, Osaka University","logo":{"@type":"ImageObject","url":"http://www.ids.osaka-u.ac.jp/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/images/logo_hu43423524dbd337905d6e12d1b20a6666_22772_192x192_fit_lanczos_2.png"}},"description":"We propose a framework for adaptively registering background models with an image for background subtraction with moving cameras. Existing methods search for a background model using a fixed window size, to suppress the number of false positives when detecting the foreground. However, these approaches result in many false negatives because they may use inappropriate window sizes. The appropriate size depends on various factors of the target scenes. To suppress false detections, we propose adaptively controlling the method parameters, which are typically determined heuristically. More specifically, the search window size for background registration and the foreground detection threshold are automatically determined using the re-projection error computed by the homography based camera motion estimate. Our method is based on the fact that the error at a pixel is low if it belongs to background and high if it does not. We quantitatively confirmed that the proposed framework improved the background subtraction accuracy when applied to images from moving cameras in various public datasets."}</script><title>Adaptive background model registration for moving cameras | Institute for Datability Science, Osaka University</title></head><body id=top data-spy=scroll data-offset=70 data-target=#TableOfContents><aside class=search-results id=search><div class=container><section class=search-header><div class="row no-gutters justify-content-between mb-3"><div class=col-6><h1>Search</h1></div><div class="col-6 col-search-close"><a class=js-search href=#><i class="fas fa-times-circle text-muted" aria-hidden=true></i></a></div></div><div id=search-box><input name=q id=search-query placeholder=Search... autocapitalize=off autocomplete=off autocorrect=off spellcheck=false type=search class=form-control></div></section><section class=section-search-results><div id=search-hits></div></section></div></aside><nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id=navbar-main><div class=container><div class="d-none d-lg-inline-flex"><a class=navbar-brand href=/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/><img src=/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/images/logo_hu43423524dbd337905d6e12d1b20a6666_22772_0x70_resize_lanczos_2.png alt="Institute for Datability Science, Osaka University"></a></div><button type=button class=navbar-toggler data-toggle=collapse data-target=#navbar-content aria-controls=navbar aria-expanded=false aria-label="Toggle navigation">
<span><i class="fas fa-bars"></i></span></button><div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none"><a class=navbar-brand href=/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/><img src=/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/images/logo_hu43423524dbd337905d6e12d1b20a6666_22772_0x70_resize_lanczos_2.png alt="Institute for Datability Science, Osaka University"></a></div><div class="navbar-collapse main-menu-item collapse justify-content-start" id=navbar-content><ul class="navbar-nav d-md-inline-flex"><li class=nav-item><a class="nav-link active" href=/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/><span>Home</span></a></li><li class=nav-item><a class=nav-link href=/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/message><span>Message</span></a></li><li class=nav-item><a class=nav-link href=/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/organization><span>Organization</span></a></li><li class=nav-item><a class=nav-link href=/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/people><span>People</span></a></li><li class=nav-item><a class=nav-link href=/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/projects><span>Projects</span></a></li><li class=nav-item><a class="nav-link active" href=/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/publication><span>Publiaction</span></a></li><li class=nav-item><a class=nav-link href=/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/access><span>Access</span></a></li></ul></div><ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2"><li class=nav-item><a class="nav-link js-search" href=# aria-label=Search><i class="fas fa-search" aria-hidden=true></i></a></li><li class="nav-item dropdown theme-dropdown"><a href=# class="nav-link js-theme-selector" data-toggle=dropdown aria-haspopup=true><i class="fas fa-palette" aria-hidden=true></i></a><div class=dropdown-menu><a href=# class="dropdown-item js-set-theme-light"><span>Light</span></a>
<a href=# class="dropdown-item js-set-theme-dark"><span>Dark</span></a>
<a href=# class="dropdown-item js-set-theme-auto"><span>Automatic</span></a></div></li><li class="nav-item dropdown i18n-dropdown"><a href=# class="nav-link dropdown-toggle" data-toggle=dropdown aria-haspopup=true><i class="fas fa-globe mr-1" aria-hidden=true></i><span class="d-none d-lg-inline">English</span></a><div class=dropdown-menu><div class="dropdown-item dropdown-item-active"><span>English</span></div><a class=dropdown-item href=http://www.ids.osaka-u.ac.jp/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/ja/publication/minematsu-2017/><span>日本語</span></a></div></li></ul></div></nav><div class=pub><div class="article-container pt-3"><h1>Adaptive background model registration for moving cameras</h1><div class=article-metadata><div><span>Tsubasa Minematsu</span>, <span>Hideaki Uchiyama</span>, <span>Atsushi Shimada</span>, <span>Hajime Nagahara</span>, <span>Rin ichiro Taniguchi</span></div><span class=article-date>September 2017</span></div><div class="btn-links mb-3"><button type=button class="btn btn-outline-primary my-1 mr-1 js-cite-modal" data-filename=/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/publication/minematsu-2017/cite.bib>
Cite</button>
<a class="btn btn-outline-primary my-1 mr-1" href=https://doi.org/10.1016/j.patrec.2017.03.010 target=_blank rel=noopener>DOI</a></div></div><div class=article-container><h3>Abstract</h3><p class=pub-abstract>We propose a framework for adaptively registering background models with an image for background subtraction with moving cameras. Existing methods search for a background model using a fixed window size, to suppress the number of false positives when detecting the foreground. However, these approaches result in many false negatives because they may use inappropriate window sizes. The appropriate size depends on various factors of the target scenes. To suppress false detections, we propose adaptively controlling the method parameters, which are typically determined heuristically. More specifically, the search window size for background registration and the foreground detection threshold are automatically determined using the re-projection error computed by the homography based camera motion estimate. Our method is based on the fact that the error at a pixel is low if it belongs to background and high if it does not. We quantitatively confirmed that the proposed framework improved the background subtraction accuracy when applied to images from moving cameras in various public datasets.</p><div class=row><div class=col-md-1></div><div class=col-md-10><div class=row><div class="col-12 col-md-3 pub-row-heading">Type</div><div class="col-12 col-md-9"><a href=/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/publication/#2>Journal article</a></div></div></div><div class=col-md-1></div></div><div class="d-md-none space-below"></div><div class=row><div class=col-md-1></div><div class=col-md-10><div class=row><div class="col-12 col-md-3 pub-row-heading">Publication</div><div class="col-12 col-md-9"><em>Pattern Recognition Letters</em></div></div></div><div class=col-md-1></div></div><div class="d-md-none space-below"></div><div class=space-below></div><div class=article-style></div><div class=article-tags><a class="badge badge-light" href=/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/tag/background-subtraction/>background subtraction</a>
<a class="badge badge-light" href=/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/tag/moving-camera/>moving camera</a>
<a class="badge badge-light" href=/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/tag/moving-object-detection/>moving object detection</a>
<a class="badge badge-light" href=/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/tag/re-projection-error/>re-projection error</a></div><div class=share-box aria-hidden=true><ul class=share><li><a href="https://twitter.com/intent/tweet?url=http://www.ids.osaka-u.ac.jp/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/publication/minematsu-2017/&text=Adaptive%20background%20model%20registration%20for%20moving%20cameras" target=_blank rel=noopener class=share-btn-twitter><i class="fab fa-twitter"></i></a></li><li><a href="https://www.facebook.com/sharer.php?u=http://www.ids.osaka-u.ac.jp/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/publication/minematsu-2017/&t=Adaptive%20background%20model%20registration%20for%20moving%20cameras" target=_blank rel=noopener class=share-btn-facebook><i class="fab fa-facebook"></i></a></li><li><a href="mailto:?subject=Adaptive%20background%20model%20registration%20for%20moving%20cameras&body=http://www.ids.osaka-u.ac.jp/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/publication/minematsu-2017/" target=_blank rel=noopener class=share-btn-email><i class="fas fa-envelope"></i></a></li><li><a href="https://www.linkedin.com/shareArticle?url=http://www.ids.osaka-u.ac.jp/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/publication/minematsu-2017/&title=Adaptive%20background%20model%20registration%20for%20moving%20cameras" target=_blank rel=noopener class=share-btn-linkedin><i class="fab fa-linkedin-in"></i></a></li><li><a href="whatsapp://send?text=Adaptive%20background%20model%20registration%20for%20moving%20cameras%20http://www.ids.osaka-u.ac.jp/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/publication/minematsu-2017/" target=_blank rel=noopener class=share-btn-whatsapp><i class="fab fa-whatsapp"></i></a></li><li><a href="https://service.weibo.com/share/share.php?url=http://www.ids.osaka-u.ac.jp/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/publication/minematsu-2017/&title=Adaptive%20background%20model%20registration%20for%20moving%20cameras" target=_blank rel=noopener class=share-btn-weibo><i class="fab fa-weibo"></i></a></li></ul></div><div class="media author-card content-widget-hr"><img class="avatar mr-3 avatar-circle" src=/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/author/hajime-nagahara/avatar_huc8d8cedf74443a99c5e73ee96d17a7a6_5028372_270x270_fill_lanczos_center_2.png alt="Hajime Nagahara"><div class=media-body><h5 class=card-title><a href=/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/author/hajime-nagahara/>Hajime Nagahara</a></h5><h6 class=card-subtitle>Professor</h6><p class=card-text>He is working on computer vision and pattern recognition. His main research interests lie in image/video recognition and understanding, as well as applications of natural language processing techniques.</p><ul class=network-icon aria-hidden=true><li><a href=/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/#contact><i class="fas fa-envelope"></i></a></li><li><a href=/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c><i class="fas fa-phone"></i></a></li><li><a href=http://www.ids.osaka-u.ac.jp/nagahara/ target=_blank rel=noopener><i class="fas fa-home"></i></a></li><li><a href="https://scholar.google.com/citations?user=CZyXjREAAAAJ&hl=ja" target=_blank rel=noopener><i class="ai ai-google-scholar"></i></a></li></ul></div></div><div class="article-widget content-widget-hr"><h3>Related</h3><ul><li><a href=/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/publication/ma-2017-a/>Adapting local features for face detection in thermal image</a></li><li><a href=/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/publication/ma-2019/>Fall detection using optical level anonymous image sensing system</a></li></ul></div></div></div><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js integrity="sha256-yt2kYMy0w8AbtF89WXb2P1rfjcP/HTHLT7097U8Y5b8=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/instant.page/5.1.0/instantpage.min.js integrity="sha512-1+qUtKoh9XZW7j+6LhRMAyOrgSQKenQ4mluTR+cvxXjP1Z54RxZuzstR/H9kgPXQsVB8IW7DMDFUJpzLjvhGSQ==" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js integrity="sha256-eOgo0OtLL4cdq7RdwRUiGKLX9XsIJ7nGhWEKbohmVAQ=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/languages/r.min.js></script><script src=https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.5.1/leaflet.js integrity="sha256-EErZamuLefUnbMBQbsEqu1USa+btR2oIlCpBJbyD4/g=" crossorigin=anonymous></script><script>const code_highlighting=true;</script><script>const isSiteThemeDark=false;</script><script>const search_config={"indexURI":"/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/index.json","minLength":1,"threshold":0.3};const i18n={"no_results":"No results found","placeholder":"Search...","results":"results found"};const content_type={'post':"Posts",'project':"Projects",'publication':"Publications",'talk':"Talks",'slides':"Slides"};</script><script id=search-hit-fuse-template type=text/x-template>
      <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
      </div>
    </script><script src=https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin=anonymous></script><script src=/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/js/academic.min.66c553246b0f279a03be6e5597f72b52.js></script><div class=container><footer class=site-footer><div class="d-none d-lg-inline-flex"><a class=navbar-brand href=/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/><img src=/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/images/logo_hu43423524dbd337905d6e12d1b20a6666_22772_0x70_resize_lanczos_2.png alt="Institute for Datability Science, Osaka University"></a></div><p class=powered-by style=font-size:70%;text-align:left;margin-left:40px>TEL: +81 6 6105 6074<br>FAX: +81 6 6105 6075<br>Techno-alliance bldg. C503, 2-8, Yamadaoka, Suita, Osaka 565-0971</p><p class=powered-by></p><p class=powered-by>Published with
<a href=https://sourcethemes.com/academic/ target=_blank rel=noopener>Academic Website Builder</a>
<span class=float-right aria-hidden=true><a href=# class=back-to-top><span class=button_icon><i class="fas fa-chevron-up fa-2x"></i></span></a></span></p></footer></div><div id=modal class="modal fade" role=dialog><div class=modal-dialog><div class=modal-content><div class=modal-header><h5 class=modal-title>Cite</h5><button type=button class=close data-dismiss=modal aria-label=Close>
<span aria-hidden=true>&#215;</span></button></div><div class=modal-body><pre><code class="tex hljs"></code></pre></div><div class=modal-footer><a class="btn btn-outline-primary my-1 js-copy-cite" href=# target=_blank><i class="fas fa-copy"></i>Copy</a>
<a class="btn btn-outline-primary my-1 js-download-cite" href=# target=_blank><i class="fas fa-download"></i>Download</a><div id=modal-error></div></div></div></div></div></body></html>