<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Noa Garcia | Institute for Datability Science, Osaka University</title><link>/author/noa-garcia/</link><atom:link href="/author/noa-garcia/index.xml" rel="self" type="application/rss+xml"/><description>Noa Garcia</description><generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Wed, 01 Jul 2020 10:13:28 +0900</lastBuildDate><image><url>/media/large.png</url><title>Noa Garcia</title><link>/author/noa-garcia/</link></image><item><title>Knowledge VQA</title><link>/project/kiban_b-kvqa/</link><pubDate>Wed, 01 Jul 2020 10:13:28 +0900</pubDate><guid>/project/kiban_b-kvqa/</guid><description>&lt;p>Visual question answering (VQA) with knowledge is a task that requires knowledge to answer questions on images/video. This additional requirement of knowledge poses an interesting challenge on top of the classic VQA tasks. Specifically, a system needs to explore external knowledge sources to answer the questions correctly, as well as understanding the visual content.&lt;/p>
&lt;p>We created
&lt;a href="https://knowit-vqa.github.io" target="_blank" rel="noopener">a dedicated dataset for our knowledge VQA task&lt;/a> and made it open to the public so that everyone can enjoy our new task. We have also published several papers on this task.&lt;/p>
&lt;h3 id="publications">Publications&lt;/h3>
&lt;ul>
&lt;li>Noa Garcia, Mayu Otani, Chenhui Chu, and Yuta Nakashima (2019). KnowIT VQA: Answering knowledge-based questions about videos. Proc. AAAI Conference on Artificial Intelligence, Feb. 2020.&lt;/li>
&lt;li>Zekun Yang, Noa Garcia, Chenhui Chu, Mayu Otani, Yuta Nakashima, and Haruo Takemura (2020). BERT representations for video question answering. Proc. IEEE Winter Conference on Applications of Computer Vision.&lt;/li>
&lt;li>Noa Garcia, Chenhui Chu, Mayu Otani, and Yuta Nakashima (2019). Video meets knowledge in visual question answering. MIRU.&lt;/li>
&lt;li>Zekun Yang, Noa Garcia, Chenhui Chu, Mayu Otani, Yuta Nakashima, and Haruo Takemura (2019). Video question answering with BERT. MIRU.&lt;/li>
&lt;/ul></description></item><item><title>Buddha Face and AI</title><link>/project/buddha-face/</link><pubDate>Wed, 17 Jun 2020 22:52:41 +0900</pubDate><guid>/project/buddha-face/</guid><description>&lt;p>In collaboration with
&lt;a href="http://www.dma.jim.osaka-u.ac.jp/view?l=en&amp;amp;u=6617" target="_blank" rel="noopener">Prof. Fujioka&lt;/a> with Graduate School of Letters/School of Letters, Osaka University, we are attempting to create an AI for analyzing various aspects of Buddha faces in images.&lt;/p>
&lt;p>Focusing on the face of the Buddha image, i.e., &amp;ldquo;Buddha face&amp;rdquo;, we analyze the characteristics of the style of each region, era, and author using statistical and machine learning approaches based on images and 3D geometric data, building a genealogy of Buddha faces. This is to realize style judgment based on the knowledge obtained from data, not based on the experience of art historians, which promotes the globalization of the Buddha statue research and also helps to identify the genealogy of Buddha faces propagated through the Silk Road, giving a new perspective on the spread of culture in Asia.&lt;/p>
&lt;p>We have built several interfaces to browse through a large corpus of precious Buddha faces for facilitating annotations on the basic meta-data on the statues, which will then serve as a source to train more sophisticated models for analyzing them.&lt;/p>
&lt;figure >
&lt;a data-fancybox="" href="/project/buddha-face/interfaces_hu412127b82145130d68f689675871a563_688796_2000x2000_fit_lanczos_2.png" >
&lt;img data-src="/project/buddha-face/interfaces_hu412127b82145130d68f689675871a563_688796_2000x2000_fit_lanczos_2.png" class="lazyload" alt="" width="1519" height="343">
&lt;/a>
&lt;/figure>
&lt;p>For example, we built a model that can embed various information on target entities (i.e., Buddha status), such as authors, eras, places, etc., into a vector representation of images and use them for other tasks like classification, through the model below.&lt;/p>
&lt;figure >
&lt;a data-fancybox="" href="/project/buddha-face/contextnet_hu255c0008cb8fe6340d26b769ee3d244e_313842_2000x2000_fit_lanczos_2.png" >
&lt;img data-src="/project/buddha-face/contextnet_hu255c0008cb8fe6340d26b769ee3d244e_313842_2000x2000_fit_lanczos_2.png" class="lazyload" alt="" width="1358" height="600">
&lt;/a>
&lt;/figure></description></item></channel></rss>