<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Chenhui Chu | Institute for Datability Science, Osaka University</title><link>http://www.ids.osaka-u.ac.jp/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/author/chenhui-chu/</link><atom:link href="http://www.ids.osaka-u.ac.jp/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/author/chenhui-chu/index.xml" rel="self" type="application/rss+xml"/><description>Chenhui Chu</description><generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Wed, 01 Jul 2020 10:13:28 +0900</lastBuildDate><image><url>http://www.ids.osaka-u.ac.jp/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/media/large.png</url><title>Chenhui Chu</title><link>http://www.ids.osaka-u.ac.jp/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/author/chenhui-chu/</link></image><item><title>Knowledge VQA</title><link>http://www.ids.osaka-u.ac.jp/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/project/kiban_b-kvqa/</link><pubDate>Wed, 01 Jul 2020 10:13:28 +0900</pubDate><guid>http://www.ids.osaka-u.ac.jp/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/project/kiban_b-kvqa/</guid><description>&lt;p>Visual question answering (VQA) with knowledge is a task that requires knowledge to answer questions on images/video. This additional requirement of knowledge poses an interesting challenge on top of the classic VQA tasks. Specifically, a system needs to explore external knowledge sources to answer the questions correctly, as well as understanding the visual content.&lt;/p>
&lt;p>We created
&lt;a href="https://knowit-vqa.github.io" target="_blank" rel="noopener">a dedicated dataset for our knowledge VQA task&lt;/a> and made it open to the public so that everyone can enjoy our new task. We have also published several papers on this task.&lt;/p>
&lt;h3 id="publications">Publications&lt;/h3>
&lt;ul>
&lt;li>Noa Garcia, Mayu Otani, Chenhui Chu, and Yuta Nakashima (2019). KnowIT VQA: Answering knowledge-based questions about videos. Proc. AAAI Conference on Artificial Intelligence, Feb. 2020.&lt;/li>
&lt;li>Zekun Yang, Noa Garcia, Chenhui Chu, Mayu Otani, Yuta Nakashima, and Haruo Takemura (2020). BERT representations for video question answering. Proc. IEEE Winter Conference on Applications of Computer Vision.&lt;/li>
&lt;li>Noa Garcia, Chenhui Chu, Mayu Otani, and Yuta Nakashima (2019). Video meets knowledge in visual question answering. MIRU.&lt;/li>
&lt;li>Zekun Yang, Noa Garcia, Chenhui Chu, Mayu Otani, Yuta Nakashima, and Haruo Takemura (2019). Video question answering with BERT. MIRU.&lt;/li>
&lt;/ul></description></item><item><title>Australian History in Newspaper and AI</title><link>http://www.ids.osaka-u.ac.jp/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/project/australian-history/</link><pubDate>Wed, 01 Jul 2020 10:13:06 +0900</pubDate><guid>http://www.ids.osaka-u.ac.jp/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/project/australian-history/</guid><description>&lt;p>In collaboration with
&lt;a href="http://www.let.osaka-u.ac.jp/seiyousi/fujikawa.html" target="_blank" rel="noopener">Prof. Fujikawa&lt;/a> at Graduate School of Letters, Osaka University, we are working on exploring Australian history through public meetings, of which call for participation appears in newspapers from back then.&lt;/p>
&lt;p>We explore ways to analyze such newspapers with state-of-the-art technologies in NLP to make OCR output better and to automatically detect/structure call for participation.&lt;/p></description></item><item><title>Law and AI</title><link>http://www.ids.osaka-u.ac.jp/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/project/green_law/</link><pubDate>Wed, 17 Jun 2020 23:02:32 +0900</pubDate><guid>http://www.ids.osaka-u.ac.jp/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/project/green_law/</guid><description>&lt;p>In collaboration with
&lt;a href="https://researchmap.jp/read0180483?lang=en" target="_blank" rel="noopener">Prof. Noriko Okubo&lt;/a> at Graduate School of Law and Politics, Osaka University, we are studying to automatically evaluate how green laws are enforced in different countries.&lt;/p>
&lt;p>Green laws&amp;rsquo; participation principle consists of 1) the information access right, 2) participation in the policy decision process, 3) the judicial access; however, actual implementation varies country to country, and legal methodologies have been explored for evaluating their effectiveness. This work investigates legal evaluation criteria on the green laws&amp;rsquo; participation principle, analyzes Japanese participation system&amp;rsquo;s pros and cons in a comparative perspective, and propose some recommendations to establish the environmental democracy.&lt;/p>
&lt;p>The difficulty lies in how to automatically find out related legislations, cases, statutes, etc. in different languages. As the first attempt, we proposed a method for identifying the topic of such legal documents through analyzing citation networks in addition to classic topic modeling. The figure below shows citation networks among different types of legal documents (e.g., cases-prior cases).&lt;/p>
&lt;figure >
&lt;a data-fancybox="" href="http://www.ids.osaka-u.ac.jp/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/project/green_law/citation_networks_huab3f0c6cd7d27657c2614c3ccbddca1e_853493_2000x2000_fit_lanczos_2.png" >
&lt;img data-src="http://www.ids.osaka-u.ac.jp/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/project/green_law/citation_networks_huab3f0c6cd7d27657c2614c3ccbddca1e_853493_2000x2000_fit_lanczos_2.png" class="lazyload" alt="" width="1915" height="624">
&lt;/a>
&lt;/figure></description></item><item><title>KnowIT VQA: Answering knowledge-based questions about videos</title><link>http://www.ids.osaka-u.ac.jp/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/publication/garcia-2020-a/</link><pubDate>Sat, 01 Feb 2020 00:00:00 +0000</pubDate><guid>http://www.ids.osaka-u.ac.jp/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/publication/garcia-2020-a/</guid><description/></item><item><title>BERT representations for video question answering</title><link>http://www.ids.osaka-u.ac.jp/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/publication/yang-2020/</link><pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate><guid>http://www.ids.osaka-u.ac.jp/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/publication/yang-2020/</guid><description/></item><item><title>Legal information as a complex network: Improving topic modeling through homophily</title><link>http://www.ids.osaka-u.ac.jp/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/publication/ashihara-2019-b/</link><pubDate>Fri, 01 Nov 2019 00:00:00 +0000</pubDate><guid>http://www.ids.osaka-u.ac.jp/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/publication/ashihara-2019-b/</guid><description/></item><item><title>Video meets knowledge in visual question answering</title><link>http://www.ids.osaka-u.ac.jp/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/publication/noa-garcia-chenhui-chu-2019/</link><pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate><guid>http://www.ids.osaka-u.ac.jp/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/publication/noa-garcia-chenhui-chu-2019/</guid><description/></item><item><title>iParaphrasing: Extracting visually grounded paraphrases via an image</title><link>http://www.ids.osaka-u.ac.jp/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/publication/chu-2018-a/</link><pubDate>Fri, 01 Jun 2018 00:00:00 +0000</pubDate><guid>http://www.ids.osaka-u.ac.jp/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/publication/chu-2018-a/</guid><description/></item><item><title>Visually grounded paraphrase extraction</title><link>http://www.ids.osaka-u.ac.jp/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/publication/chu-2018/</link><pubDate>Mon, 01 Jan 2018 00:00:00 +0000</pubDate><guid>http://www.ids.osaka-u.ac.jp/test/preview/c1303b61d75385f5d51787e83049009c4f8f467c/publication/chu-2018/</guid><description/></item></channel></rss>