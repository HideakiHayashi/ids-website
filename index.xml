<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Institute for Datability Science, Osaka University</title><link>/</link><atom:link href="/index.xml" rel="self" type="application/rss+xml"/><description>Institute for Datability Science, Osaka University</description><generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Wed, 01 Jul 2020 10:13:28 +0900</lastBuildDate><image><url>/media/large.png</url><title>Institute for Datability Science, Osaka University</title><link>/</link></image><item><title>Knowledge VQA</title><link>/project/kiban_b-kvqa/</link><pubDate>Wed, 01 Jul 2020 10:13:28 +0900</pubDate><guid>/project/kiban_b-kvqa/</guid><description>&lt;p>Visual question answering (VQA) with knowledge is a task that requires knowledge to answer questions on images/video. This additional requirement of knowledge poses an interesting challenge on top of the classic VQA tasks. Specifically, a system needs to explore external knowledge sources to answer the questions correctly, as well as understanding the visual content.&lt;/p>
&lt;p>We created
&lt;a href="https://knowit-vqa.github.io" target="_blank" rel="noopener">a dedicated dataset for our knowledge VQA task&lt;/a> and made it open to the public so that everyone can enjoy our new task. We have also published several papers on this task.&lt;/p>
&lt;h3 id="publications">Publications&lt;/h3>
&lt;ul>
&lt;li>Noa Garcia, Mayu Otani, Chenhui Chu, and Yuta Nakashima (2019). KnowIT VQA: Answering knowledge-based questions about videos. Proc. AAAI Conference on Artificial Intelligence, Feb. 2020.&lt;/li>
&lt;li>Zekun Yang, Noa Garcia, Chenhui Chu, Mayu Otani, Yuta Nakashima, and Haruo Takemura (2020). BERT representations for video question answering. Proc. IEEE Winter Conference on Applications of Computer Vision.&lt;/li>
&lt;li>Noa Garcia, Chenhui Chu, Mayu Otani, and Yuta Nakashima (2019). Video meets knowledge in visual question answering. MIRU.&lt;/li>
&lt;li>Zekun Yang, Noa Garcia, Chenhui Chu, Mayu Otani, Yuta Nakashima, and Haruo Takemura (2019). Video question answering with BERT. MIRU.&lt;/li>
&lt;/ul></description></item><item><title>Australian History in Newspaper and AI</title><link>/project/australian-history/</link><pubDate>Wed, 01 Jul 2020 10:13:06 +0900</pubDate><guid>/project/australian-history/</guid><description>&lt;p>In collaboration with
&lt;a href="http://www.let.osaka-u.ac.jp/seiyousi/fujikawa.html" target="_blank" rel="noopener">Prof. Fujikawa&lt;/a> at Graduate School of Letters, Osaka University, we are working on exploring Australian history through public meetings, of which call for participation appears in newspapers from back then.&lt;/p>
&lt;p>We explore ways to analyze such newspapers with state-of-the-art technologies in NLP to make OCR output better and to automatically detect/structure call for participation.&lt;/p></description></item><item><title>Kiban_s Plenoptic</title><link>/project/kiban_s-plenoptic/</link><pubDate>Wed, 01 Jul 2020 10:12:37 +0900</pubDate><guid>/project/kiban_s-plenoptic/</guid><description>&lt;p>Coming soon.&lt;/p></description></item><item><title>Crest 3d Cancer</title><link>/project/crest-3d-cancer/</link><pubDate>Wed, 01 Jul 2020 10:08:48 +0900</pubDate><guid>/project/crest-3d-cancer/</guid><description>&lt;p>Coming soon.&lt;/p></description></item><item><title>Society 5.0 Projects</title><link>/project/society5_0/</link><pubDate>Wed, 01 Jul 2020 10:07:15 +0900</pubDate><guid>/project/society5_0/</guid><description>&lt;p>Institute for Datability Science, Osaka University is now working on
&lt;a href="http://www.ids.osaka-u.ac.jp/ildi/en/index.html" target="_blank" rel="noopener">Society 5.0&lt;/a> using information science and technology.&lt;/p>
&lt;blockquote>
&lt;p>In the world of Society 5.0, innovations in IoT, Big Data, robotics, and AI will be part of everyday life, helping people lead active and high-quality lives, creating a super-smart society. This project encourages collaboration across projects and university organizations, thus promoting faster adoption of research results in real-world society.&lt;/p>
&lt;/blockquote>
&lt;p>Under this big project, we are working two sub-projects:&lt;/p>
&lt;h2 id="social-sensing-for-society-50">Social sensing for Society 5.0&lt;/h2>
&lt;p>Sensing technologies that aggregate various types of information from publicly and ubiquitously available social probes including social networking services are essential for providing various services in the world of Society 5.0. We are working towards establishing a social sensing technology that can infer emotional states of people for timely services.&lt;/p>
&lt;h2 id="future-school-technology-for-society-50">Future school technology for Society 5.0&lt;/h2>
&lt;p>Education everywhere is one of SDGs, and e-learning is one solution toward this. We collectively working on a broad range of technologies related to e-learning.&lt;/p></description></item><item><title>Esthetic Dentistry and Optical Analysis</title><link>/project/esthetic-dentistry/</link><pubDate>Wed, 01 Jul 2020 10:06:58 +0900</pubDate><guid>/project/esthetic-dentistry/</guid><description>&lt;p>There have been ever-increasing demands for esthetics in the oral cavity after tooth restorations. Especially in the anterior, functional restoration is not enough; restored teeth should have similar color tones and light transmissions to natural teeth because the appearance of restored teeth when exposed to light varies greatly depending on the material used for the crown restoration device and the abutment structure.&lt;/p>
&lt;p>Therefore, by analyzing the optical properties of various dental restoration materials and dental tissues via optical simulation, the behavior of light in natural teeth and dental restorations is visualized and analyzed for esthetic tooth restorations.&lt;/p>
&lt;figure >
&lt;a data-fancybox="" href="/project/esthetic-dentistry/bs02_ja_hufa2c9813ab5ec072b2ff3aff4129a81c_207794_2000x2000_fit_lanczos_2.png" >
&lt;img data-src="/project/esthetic-dentistry/bs02_ja_hufa2c9813ab5ec072b2ff3aff4129a81c_207794_2000x2000_fit_lanczos_2.png" class="lazyload" alt="" width="654" height="272">
&lt;/a>
&lt;/figure></description></item><item><title>Brain Pharmaceutics</title><link>/project/brain-pharmaceutics/</link><pubDate>Wed, 01 Jul 2020 10:06:11 +0900</pubDate><guid>/project/brain-pharmaceutics/</guid><description>&lt;p>Coming soon&lt;/p></description></item><item><title>AI Hospital</title><link>/project/ai-hospital/</link><pubDate>Wed, 01 Jul 2020 10:05:12 +0900</pubDate><guid>/project/ai-hospital/</guid><description>&lt;p>Osaka University Medical Hospital has launched
&lt;a href="https://www.hosp.med.osaka-u.ac.jp/english/departments/ai.html" target="_blank" rel="noopener">Artificical Intelligence Center for Medical Research and Application (AIM)&lt;/a>, which supports physicians, nurses, and all the medical staff collaborating with medical information specialists and data scientists to boost the medical application of AI in daily practices of the hospital.&lt;/p>
&lt;p>We are collaborating with AIM to provide cutting-edge technologies.&lt;/p>
&lt;h2 id="ophthalmology-and-ai">Ophthalmology and AI&lt;/h2>
&lt;p>In ophthalmology or any other departments, vessels in retinal fundus images provide rich information on the cardiovascular system of human bodies. We proposed a state-of-the-art method, coined
&lt;a href="publication/li-2020-a/">IterNet&lt;/a> for extracting vessels from retinal fundus images as in the (e) in the figure below.&lt;/p>
&lt;figure >
&lt;a data-fancybox="" href="/project/ai-hospital/iternet_hu5ea4eead4f7a5d7e4e3c54a3ef2dbf33_96994_2000x2000_fit_q90_lanczos.jpg" >
&lt;img data-src="/project/ai-hospital/iternet_hu5ea4eead4f7a5d7e4e3c54a3ef2dbf33_96994_2000x2000_fit_q90_lanczos.jpg" class="lazyload" alt="" width="720" height="380">
&lt;/a>
&lt;/figure>
&lt;p>On top of this technology, we also invented a new method for classifying vessels into artery/vein, which takes a two-step approach: We firstly segment vessels in input images using an IterNet-based method and then classify them into artery/vein with some post-processing.&lt;/p>
&lt;figure >
&lt;a data-fancybox="" href="/project/ai-hospital/segmentation_hu6603ce41fa995fbca303679a56d9cb90_58078_2000x2000_fit_q90_lanczos.jpg" >
&lt;img data-src="/project/ai-hospital/segmentation_hu6603ce41fa995fbca303679a56d9cb90_58078_2000x2000_fit_q90_lanczos.jpg" class="lazyload" alt="" width="720" height="371">
&lt;/a>
&lt;/figure></description></item><item><title>Law and AI</title><link>/project/green_law/</link><pubDate>Wed, 17 Jun 2020 23:02:32 +0900</pubDate><guid>/project/green_law/</guid><description>&lt;p>In collaboration with
&lt;a href="https://researchmap.jp/read0180483?lang=en" target="_blank" rel="noopener">Prof. Noriko Okubo&lt;/a> at Graduate School of Law and Politics, Osaka University, we are studying to automatically evaluate how green laws are enforced in different countries.&lt;/p>
&lt;p>Green laws&amp;rsquo; participation principle consists of 1) the information access right, 2) participation in the policy decision process, 3) the judicial access; however, actual implementation varies country to country, and legal methodologies have been explored for evaluating their effectiveness. This work investigates legal evaluation criteria on the green laws&amp;rsquo; participation principle, analyzes Japanese participation system&amp;rsquo;s pros and cons in a comparative perspective, and propose some recommendations to establish the environmental democracy.&lt;/p>
&lt;p>The difficulty lies in how to automatically find out related legislations, cases, statutes, etc. in different languages. As the first attempt, we proposed a method for identifying the topic of such legal documents through analyzing citation networks in addition to classic topic modeling. The figure below shows citation networks among different types of legal documents (e.g., cases-prior cases).&lt;/p>
&lt;figure >
&lt;a data-fancybox="" href="/project/green_law/citation_networks_huab3f0c6cd7d27657c2614c3ccbddca1e_853493_2000x2000_fit_lanczos_2.png" >
&lt;img data-src="/project/green_law/citation_networks_huab3f0c6cd7d27657c2614c3ccbddca1e_853493_2000x2000_fit_lanczos_2.png" class="lazyload" alt="" width="1915" height="624">
&lt;/a>
&lt;/figure></description></item><item><title>Buddha Face and AI</title><link>/project/buddha-face/</link><pubDate>Wed, 17 Jun 2020 22:52:41 +0900</pubDate><guid>/project/buddha-face/</guid><description>&lt;p>In collaboration with
&lt;a href="http://www.dma.jim.osaka-u.ac.jp/view?l=en&amp;amp;u=6617" target="_blank" rel="noopener">Prof. Fujioka&lt;/a> with Graduate School of Letters/School of Letters, Osaka University, we are attempting to create an AI for analyzing various aspects of Buddha faces in images.&lt;/p>
&lt;p>Focusing on the face of the Buddha image, i.e., &amp;ldquo;Buddha face&amp;rdquo;, we analyze the characteristics of the style of each region, era, and author using statistical and machine learning approaches based on images and 3D geometric data, building a genealogy of Buddha faces. This is to realize style judgment based on the knowledge obtained from data, not based on the experience of art historians, which promotes the globalization of the Buddha statue research and also helps to identify the genealogy of Buddha faces propagated through the Silk Road, giving a new perspective on the spread of culture in Asia.&lt;/p>
&lt;p>We have built several interfaces to browse through a large corpus of precious Buddha faces for facilitating annotations on the basic meta-data on the statues, which will then serve as a source to train more sophisticated models for analyzing them.&lt;/p>
&lt;figure >
&lt;a data-fancybox="" href="/project/buddha-face/interfaces_hu412127b82145130d68f689675871a563_688796_2000x2000_fit_lanczos_2.png" >
&lt;img data-src="/project/buddha-face/interfaces_hu412127b82145130d68f689675871a563_688796_2000x2000_fit_lanczos_2.png" class="lazyload" alt="" width="1519" height="343">
&lt;/a>
&lt;/figure>
&lt;p>For example, we built a model that can embed various information on target entities (i.e., Buddha status), such as authors, eras, places, etc., into a vector representation of images and use them for other tasks like classification, through the model below.&lt;/p>
&lt;figure >
&lt;a data-fancybox="" href="/project/buddha-face/contextnet_hu255c0008cb8fe6340d26b769ee3d244e_313842_2000x2000_fit_lanczos_2.png" >
&lt;img data-src="/project/buddha-face/contextnet_hu255c0008cb8fe6340d26b769ee3d244e_313842_2000x2000_fit_lanczos_2.png" class="lazyload" alt="" width="1358" height="600">
&lt;/a>
&lt;/figure></description></item><item><title>IDS Concluded comprehensive partnership with University of Yamanashi and Kyoto Tachibana University</title><link>/post/comprehensive_partnership_with_u-yamanashi_and_tachibana-u/</link><pubDate>Wed, 10 Jun 2020 00:00:00 +0900</pubDate><guid>/post/comprehensive_partnership_with_u-yamanashi_and_tachibana-u/</guid><description/></item><item><title>Convolutional Neural Network Can Recognize Drug Resistance of Single Cancer Cells</title><link>/publication/yanagisawa-2020/</link><pubDate>Wed, 01 Apr 2020 00:00:00 +0000</pubDate><guid>/publication/yanagisawa-2020/</guid><description/></item><item><title>Yoga-82: a new dataset for fine-grained classification of human poses</title><link>/publication/verma-2020/</link><pubDate>Wed, 01 Apr 2020 00:00:00 +0000</pubDate><guid>/publication/verma-2020/</guid><description/></item><item><title>Detecting learner drowsiness based on facial expressions and head movements in online courses</title><link>/publication/terai-2020/</link><pubDate>Sun, 01 Mar 2020 00:00:00 +0000</pubDate><guid>/publication/terai-2020/</guid><description/></item><item><title>Hosted IDS Symposium (in Japanese)</title><link>/post/ids_symposium_2020-02/</link><pubDate>Fri, 07 Feb 2020 00:00:00 +0900</pubDate><guid>/post/ids_symposium_2020-02/</guid><description/></item><item><title>KnowIT VQA: Answering knowledge-based questions about videos</title><link>/publication/garcia-2020-a/</link><pubDate>Sat, 01 Feb 2020 00:00:00 +0000</pubDate><guid>/publication/garcia-2020-a/</guid><description/></item><item><title>3D Image Reconstruction from Multi-focus Microscopic Images</title><link>/publication/yamaguchi-2020/</link><pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate><guid>/publication/yamaguchi-2020/</guid><description/></item><item><title>BERT representations for video question answering</title><link>/publication/yang-2020/</link><pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate><guid>/publication/yang-2020/</guid><description/></item><item><title>ContextNet: representation and exploration for painting classification and retrieval in context</title><link>/publication/garcia-2020/</link><pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate><guid>/publication/garcia-2020/</guid><description/></item><item><title>IterNet: retinal image segmentation utilizing structural redundancy in vessel networks</title><link>/publication/li-2020/</link><pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate><guid>/publication/li-2020/</guid><description/></item><item><title>Joint learning of vessel segmentation and artery/vein classification with post-processing</title><link>/publication/li-2020-a/</link><pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate><guid>/publication/li-2020-a/</guid><description/></item><item><title>Speech-driven face reenactment for a video sequence</title><link>/publication/nakashima-2020/</link><pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate><guid>/publication/nakashima-2020/</guid><description/></item><item><title>Warmer Environments Increase Implicit Mental Workload Even If Learning Efficiency Is Enhanced</title><link>/publication/kimura-2020/</link><pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate><guid>/publication/kimura-2020/</guid><description/></item><item><title>Contextualized multi-sense word embedding</title><link>/publication/ashihara-2019-a/</link><pubDate>Sun, 01 Dec 2019 00:00:00 +0000</pubDate><guid>/publication/ashihara-2019-a/</guid><description/></item><item><title>Deep-UV excitation fluorescence microscopy for detection of lymph node metastasis using deep neural network</title><link>/publication/matsumoto-2019/</link><pubDate>Sun, 01 Dec 2019 00:00:00 +0000</pubDate><guid>/publication/matsumoto-2019/</guid><description/></item><item><title>Reflectance and Shape Estimation with a Light Field Camera Under Natural Illumination</title><link>/publication/ngo-2019/</link><pubDate>Sun, 01 Dec 2019 00:00:00 +0000</pubDate><guid>/publication/ngo-2019/</guid><description/></item><item><title>Human shape reconstruction with loose clothes from partially observed data by pose specific deformation</title><link>/publication/sayo-2019/</link><pubDate>Fri, 01 Nov 2019 00:00:00 +0000</pubDate><guid>/publication/sayo-2019/</guid><description/></item><item><title>Legal information as a complex network: Improving topic modeling through homophily</title><link>/publication/ashihara-2019-b/</link><pubDate>Fri, 01 Nov 2019 00:00:00 +0000</pubDate><guid>/publication/ashihara-2019-b/</guid><description/></item><item><title>Deep compressive sensing for visual privacy protection in flatcam imaging</title><link>/publication/nguyen-canh-2019/</link><pubDate>Tue, 01 Oct 2019 00:00:00 +0000</pubDate><guid>/publication/nguyen-canh-2019/</guid><description/></item><item><title>A 3-D Display Pipeline from Coded-Aperture Camera to Tensor Light-Field Display Through CNN</title><link>/publication/maruyama-2019/</link><pubDate>Sun, 01 Sep 2019 00:00:00 +0000</pubDate><guid>/publication/maruyama-2019/</guid><description/></item><item><title>Metric for automatic machine translation evaluation based on pre-trained sentence embeddings</title><link>/publication/shimanaka-2019/</link><pubDate>Sun, 01 Sep 2019 00:00:00 +0000</pubDate><guid>/publication/shimanaka-2019/</guid><description/></item><item><title>Excitation of erbium-doped nanoparticles in 1550-nm wavelength region for deep tissue imaging with reduced degradation of spatial resolution</title><link>/publication/yamanaka-2019/</link><pubDate>Mon, 01 Jul 2019 00:00:00 +0000</pubDate><guid>/publication/yamanaka-2019/</guid><description/></item><item><title>Application of deep learning (3-dimensional convolutional neural network) for the prediction of pathological invasiveness in lung adenocarcinoma</title><link>/publication/yanagawa-2019/</link><pubDate>Sat, 01 Jun 2019 00:00:00 +0000</pubDate><guid>/publication/yanagawa-2019/</guid><description/></item><item><title>Multimodal learning analytics: Society 5.0 project in Japan</title><link>/publication/shirai-2019/</link><pubDate>Fri, 01 Mar 2019 00:00:00 +0000</pubDate><guid>/publication/shirai-2019/</guid><description/></item><item><title>Fall detection using optical level anonymous image sensing system</title><link>/publication/ma-2019/</link><pubDate>Fri, 01 Feb 2019 00:00:00 +0000</pubDate><guid>/publication/ma-2019/</guid><description/></item><item><title>A Coded Aperture for Watermark Extraction from Defocused Images</title><link>/publication/hamasaki-2019/</link><pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate><guid>/publication/hamasaki-2019/</guid><description/></item><item><title>Access</title><link>/access/</link><pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate><guid>/access/</guid><description>&lt;h2 id="hahahugoshortcode-s0-hbhb-location">
&lt;i class="fas fa-thumbtack pr-1 fa-fw">&lt;/i> Location&lt;/h2>
&lt;p>
&lt;i class="fas fa-university pr-1 fa-fw">&lt;/i> Institute for Datability ScienceOsaka University&lt;/p>
&lt;p>
&lt;i class="fas fa-map-pin pr-1 fa-fw">&lt;/i> Techno-Alliance Building C503, 2-8 Yamadaoka, Suita, Osaka, 565-0871 Japan
Phone : +81-6-6105-6074 FAX : +81-6-6105-6075&lt;/p></description></item><item><title>Buda.art: A multimodal content-based analysis and retrieval system for Buddha statues</title><link>/publication/renoust-2019-a/</link><pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate><guid>/publication/renoust-2019-a/</guid><description/></item><item><title>Context-aware embeddings for automatic art analysis</title><link>/publication/garcia-2019-a/</link><pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate><guid>/publication/garcia-2019-a/</guid><description/></item><item><title>Contextualized context2vec</title><link>/publication/ashihara-2019/</link><pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate><guid>/publication/ashihara-2019/</guid><description/></item><item><title>Controllable text simplification with lexical constraint loss</title><link>/publication/nishihara-2019/</link><pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate><guid>/publication/nishihara-2019/</guid><description/></item><item><title>Efficacy of Novel Multispectral Imaging Device to Determine Anastomosis for Esophagogastrostomy</title><link>/publication/tsutsumi-2019/</link><pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate><guid>/publication/tsutsumi-2019/</guid><description/></item><item><title>Facial expression recognition with skip-connection to leverage low-level features</title><link>/publication/verma-2019/</link><pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate><guid>/publication/verma-2019/</guid><description/></item><item><title>Historical and modern features for Buddha statue classification</title><link>/publication/renoust-2019/</link><pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate><guid>/publication/renoust-2019/</guid><description/></item><item><title>Negative lexically constrained decoding for paraphrase generation</title><link>/publication/kajiwara-2020/</link><pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate><guid>/publication/kajiwara-2020/</guid><description/></item><item><title>Our Projects</title><link>/projects/</link><pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate><guid>/projects/</guid><description/></item><item><title>People</title><link>/people/</link><pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate><guid>/people/</guid><description/></item><item><title>Rethinking the evaluation of video summaries</title><link>/publication/otani-2019-a/</link><pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate><guid>/publication/otani-2019-a/</guid><description/></item><item><title>Video meets knowledge in visual question answering</title><link>/publication/noa-garcia-chenhui-chu-2019/</link><pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate><guid>/publication/noa-garcia-chenhui-chu-2019/</guid><description/></item><item><title>Space-time-brightness sampling using an adaptive pixel-wise coded exposure</title><link>/publication/nagahara-2018/</link><pubDate>Sat, 01 Dec 2018 00:00:00 +0000</pubDate><guid>/publication/nagahara-2018/</guid><description/></item><item><title>Representing a partially observed non-rigid 3D human using eigen-texture and eigen-deformation</title><link>/publication/kimura-2018/</link><pubDate>Thu, 01 Nov 2018 00:00:00 +0000</pubDate><guid>/publication/kimura-2018/</guid><description/></item><item><title>Finding important people in a video using deep neural networks with conditional random fields</title><link>/publication/otani-2018/</link><pubDate>Mon, 01 Oct 2018 00:00:00 +0000</pubDate><guid>/publication/otani-2018/</guid><description/></item><item><title>Designing coded aperture camera based on PCA and NMF for light field acquisition</title><link>/publication/yagi-2018/</link><pubDate>Sat, 01 Sep 2018 00:00:00 +0000</pubDate><guid>/publication/yagi-2018/</guid><description/></item><item><title>Invited Article: Label-free nerve imaging with a coherent anti-Stokes Raman scattering rigid endoscope using two optical fibers for laser delivery</title><link>/publication/hirose-2018/</link><pubDate>Sat, 01 Sep 2018 00:00:00 +0000</pubDate><guid>/publication/hirose-2018/</guid><description/></item><item><title>Iterative applications of image completion with CNN-based failure detection</title><link>/publication/tanaka-2018/</link><pubDate>Wed, 01 Aug 2018 00:00:00 +0000</pubDate><guid>/publication/tanaka-2018/</guid><description/></item><item><title>Summarization of user-generated sports video by using deep action recognition features</title><link>/publication/tejero-de-pablos-2018/</link><pubDate>Wed, 01 Aug 2018 00:00:00 +0000</pubDate><guid>/publication/tejero-de-pablos-2018/</guid><description/></item><item><title>Message from Our Director</title><link>/message/</link><pubDate>Thu, 28 Jun 2018 00:00:00 +0000</pubDate><guid>/message/</guid><description>
&lt;figure id="figure-takao-onoyebrexecutive-vice-president-of-researchbrosaka-university">
&lt;a data-fancybox="" href="/message/onoye_hub8d5d29deaab3603ca73d7faf9d08aa8_105043_2000x2000_fit_lanczos_2.png" data-caption="Takao Onoye&amp;lt;br/&amp;gt;Executive Vice President of Research&amp;lt;br/&amp;gt;Osaka University">
&lt;img data-src="/message/onoye_hub8d5d29deaab3603ca73d7faf9d08aa8_105043_2000x2000_fit_lanczos_2.png" class="lazyload" alt="" width="245" height="280">
&lt;/a>
&lt;figcaption>
Takao Onoye&lt;br/>Executive Vice President of Research&lt;br/>Osaka University
&lt;/figcaption>
&lt;/figure>
&lt;p>With the dawn of the &amp;ldquo;big data&amp;rdquo; era, a massive amount of data is being generated every day, be it from our day-to-day lives, or from cutting-edge scientific fields. What sort of analysis is required to extract beneficial information from such enormous data sources? Furthermore, how can this be done in a sufficiently timely manner? These questions are critical to the future development of many scientific disciplines, and our society as a whole. In pursuing a better world for future generations, we believe the key is whether or not we can create, and subsequently utilize, new intellectual value from this data.
We at Osaka University have established the Institute for Datability Science (IDS), with the chief aim of utilizing the wealth of data we have at our disposal. One core goal at IDS is the pursuit of a new scientific paradigm through &amp;ldquo;Datability.&amp;rdquo; This refers to the sustainable and responsible use of large data resources. In pursuit of this goal, we shall pioneer new academic disciplines in the domains of Life Science, Biomedical Science, Science and Engineering, and the Humanities, through sophisticated technologies of information science such as artificial intelligence, with the final goal of creating new social and economic value through interdisciplinary research projects. At IDS, we seek to build a highly productive working environment of co-creation and harmony, cultivating the next generation of scientists and engineers with &amp;ldquo;datability&amp;rdquo;.&lt;/p>
&lt;p>Takao Onoye
Executive Vice President of Research
Osaka University&lt;/p></description></item><item><title>Organization</title><link>/organization/</link><pubDate>Thu, 28 Jun 2018 00:00:00 +0000</pubDate><guid>/organization/</guid><description>
&lt;figure >
&lt;a data-fancybox="" href="/organization/section01_img_hued806c2acd79cabe14faccd8b90130de_220986_2000x2000_fit_lanczos_2.png" >
&lt;img data-src="/organization/section01_img_hued806c2acd79cabe14faccd8b90130de_220986_2000x2000_fit_lanczos_2.png" class="lazyload" alt="" width="900" height="675">
&lt;/a>
&lt;/figure></description></item><item><title>iParaphrasing: Extracting visually grounded paraphrases via an image</title><link>/publication/chu-2018-a/</link><pubDate>Fri, 01 Jun 2018 00:00:00 +0000</pubDate><guid>/publication/chu-2018-a/</guid><description/></item><item><title>PCA-coded aperture for light field photography</title><link>/publication/yagi-2018-a/</link><pubDate>Thu, 01 Feb 2018 00:00:00 +0000</pubDate><guid>/publication/yagi-2018-a/</guid><description/></item><item><title>Coherent anti-stokes Raman scattering rigid endoscope toward robot-assisted surgery</title><link>/publication/hirose-2018-a/</link><pubDate>Mon, 01 Jan 2018 00:00:00 +0000</pubDate><guid>/publication/hirose-2018-a/</guid><description/></item><item><title>Complex word identification based on frequency in a learner corpus</title><link>/publication/kajiwara-2018/</link><pubDate>Mon, 01 Jan 2018 00:00:00 +0000</pubDate><guid>/publication/kajiwara-2018/</guid><description/></item><item><title>Graphical classification of DNA sequences of HLA alleles by deep learning</title><link>/publication/miyake-2018/</link><pubDate>Mon, 01 Jan 2018 00:00:00 +0000</pubDate><guid>/publication/miyake-2018/</guid><description/></item><item><title>Joint optimization for compressive video sensing and reconstruction under hardware constraints</title><link>/publication/yoshida-2018/</link><pubDate>Mon, 01 Jan 2018 00:00:00 +0000</pubDate><guid>/publication/yoshida-2018/</guid><description/></item><item><title>Learning to capture light fields through a coded aperture camera</title><link>/publication/inagaki-2018/</link><pubDate>Mon, 01 Jan 2018 00:00:00 +0000</pubDate><guid>/publication/inagaki-2018/</guid><description/></item><item><title>Metric for automatic machine translation evaluation based on universal sentence representations</title><link>/publication/shimanaka-2018-a/</link><pubDate>Mon, 01 Jan 2018 00:00:00 +0000</pubDate><guid>/publication/shimanaka-2018-a/</guid><description/></item><item><title>RUSE: Regressor using sentence embeddings for automatic machine translation evaluation</title><link>/publication/shimanaka-2018/</link><pubDate>Mon, 01 Jan 2018 00:00:00 +0000</pubDate><guid>/publication/shimanaka-2018/</guid><description/></item><item><title>The dynamic photometric stereo method using a multi-tap CMOS image sensor</title><link>/publication/yoda-2018/</link><pubDate>Mon, 01 Jan 2018 00:00:00 +0000</pubDate><guid>/publication/yoda-2018/</guid><description/></item><item><title>Visually grounded paraphrase extraction</title><link>/publication/chu-2018/</link><pubDate>Mon, 01 Jan 2018 00:00:00 +0000</pubDate><guid>/publication/chu-2018/</guid><description/></item><item><title>Adapting local features for face detection in thermal image</title><link>/publication/ma-2017-a/</link><pubDate>Fri, 01 Dec 2017 00:00:00 +0000</pubDate><guid>/publication/ma-2017-a/</guid><description/></item><item><title>Augmented reality marker hiding with texture deformation</title><link>/publication/kawai-2017-a/</link><pubDate>Sun, 01 Oct 2017 00:00:00 +0000</pubDate><guid>/publication/kawai-2017-a/</guid><description/></item><item><title>Adaptive background model registration for moving cameras</title><link>/publication/minematsu-2017/</link><pubDate>Fri, 01 Sep 2017 00:00:00 +0000</pubDate><guid>/publication/minematsu-2017/</guid><description/></item><item><title>Novel view synthesis with light-weight view-dependent texture mapping for a stereoscopic HMD</title><link>/publication/rongsirigul-2017/</link><pubDate>Tue, 01 Aug 2017 00:00:00 +0000</pubDate><guid>/publication/rongsirigul-2017/</guid><description/></item><item><title>Video summarization using textual descriptions for authoring video blogs</title><link>/publication/otani-2017-c/</link><pubDate>Mon, 01 May 2017 00:00:00 +0000</pubDate><guid>/publication/otani-2017-c/</guid><description/></item><item><title>Hyperspectral imaging using flickerless active LED illumination</title><link>/publication/ohsaki-2017/</link><pubDate>Wed, 01 Mar 2017 00:00:00 +0000</pubDate><guid>/publication/ohsaki-2017/</guid><description/></item><item><title>Classification of C2C12 cells at differentiation by convolutional neural network of deep learning using phase contrast images</title><link>/publication/niioka-2017/</link><pubDate>Sun, 01 Jan 2017 00:00:00 +0000</pubDate><guid>/publication/niioka-2017/</guid><description/></item><item><title>Fine-grained video retrieval for multi-clip video</title><link>/publication/otani-2017/</link><pubDate>Sun, 01 Jan 2017 00:00:00 +0000</pubDate><guid>/publication/otani-2017/</guid><description/></item><item><title>Increasing pose comprehension through augmented reality reenactment</title><link>/publication/dayrit-2017-a/</link><pubDate>Sun, 01 Jan 2017 00:00:00 +0000</pubDate><guid>/publication/dayrit-2017-a/</guid><description/></item><item><title>Incremental structural modeling on sparse visual SLAM</title><link>/publication/roberto-2017/</link><pubDate>Sun, 01 Jan 2017 00:00:00 +0000</pubDate><guid>/publication/roberto-2017/</guid><description/></item><item><title>Mixed features for face detection in thermal image</title><link>/publication/ma-2017/</link><pubDate>Sun, 01 Jan 2017 00:00:00 +0000</pubDate><guid>/publication/ma-2017/</guid><description/></item><item><title>Realtime novel view synthesis with eigen-texture regression</title><link>/publication/nakashima-2017/</link><pubDate>Sun, 01 Jan 2017 00:00:00 +0000</pubDate><guid>/publication/nakashima-2017/</guid><description/></item><item><title>ReMagicMirror: Action learning using human reenactment with the mirror metaphor</title><link>/publication/dayrit-2017/</link><pubDate>Sun, 01 Jan 2017 00:00:00 +0000</pubDate><guid>/publication/dayrit-2017/</guid><description/></item><item><title>Unsupervised Video Summarization using Deep Video Features</title><link>/publication/otani-2017-a/</link><pubDate>Sun, 01 Jan 2017 00:00:00 +0000</pubDate><guid>/publication/otani-2017-a/</guid><description/></item><item><title>Video question answering to find a desired video eegment</title><link>/publication/otani-2017-b/</link><pubDate>Sun, 01 Jan 2017 00:00:00 +0000</pubDate><guid>/publication/otani-2017-b/</guid><description/></item><item><title>High-speed imaging using CMOS image sensor with quasi pixel-wise exposure</title><link>/publication/nagahara-2016/</link><pubDate>Wed, 01 Jun 2016 00:00:00 +0000</pubDate><guid>/publication/nagahara-2016/</guid><description/></item><item><title>Dynamic photometric stereo method using multi-tap CMOS image sensor</title><link>/publication/yoda-2016/</link><pubDate>Fri, 01 Jan 2016 00:00:00 +0000</pubDate><guid>/publication/yoda-2016/</guid><description/></item></channel></rss>